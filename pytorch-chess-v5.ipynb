{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11198510,"sourceType":"datasetVersion","datasetId":6991749},{"sourceId":11315304,"sourceType":"datasetVersion","datasetId":7077673}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install Libraries\n!pip install python-chess","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:13:06.884656Z","iopub.execute_input":"2025-04-07T20:13:06.884929Z","iopub.status.idle":"2025-04-07T20:13:13.740390Z","shell.execute_reply.started":"2025-04-07T20:13:06.884906Z","shell.execute_reply":"2025-04-07T20:13:13.739345Z"}},"outputs":[{"name":"stdout","text":"Collecting python-chess\n  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\nCollecting chess<2,>=1 (from python-chess)\n  Downloading chess-1.11.2.tar.gz (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading python_chess-1.999-py3-none-any.whl (1.4 kB)\nBuilding wheels for collected packages: chess\n  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147776 sha256=795e0b043660a9de8b5b50af9867c02c54718abe826eac76b07253145cb85099\n  Stored in directory: /root/.cache/pip/wheels/7a/79/8e/0d6e404db9f1e82af2e40b49161d6acab485d75dfb0470ac08\nSuccessfully built chess\nInstalling collected packages: chess, python-chess\nSuccessfully installed chess-1.11.2 python-chess-1.999\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 3: Clean up previous output (Optional)\n!rm output.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:32:20.392813Z","iopub.execute_input":"2025-04-07T16:32:20.393205Z","iopub.status.idle":"2025-04-07T16:32:20.945486Z","shell.execute_reply.started":"2025-04-07T16:32:20.393173Z","shell.execute_reply":"2025-04-07T16:32:20.944398Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'output.zip': No such file or directory\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 2: Imports\n# VERSION 5: Added Dropout, Early Stopping, Default Augmentation, Improved Graphs.\n\n# %% [markdown]\n# # Advanced Chess AI V5 (Supervised Learning with Regularization & Early Stopping)\n#\n# Building on V4, let's make the AI even smarter by tackling overfitting!\n#\n# **New in V5:**\n# *   **Dropout Regularization:** Added Dropout layers to the model architecture to improve generalization (Cell 6).\n# *   **Early Stopping:** Training now stops automatically if validation loss doesn't improve for a set number of epochs (Cell 10).\n# *   **Data Augmentation Default:** Data augmentation (flipping) is now enabled by default during data processing (Cell 7). **Remember to reprocess data if loading V4 data!**\n# *   **Weight Decay Adjustment:** Slightly increased default weight decay (Cell 9).\n# *   **Training Accuracy Plot:** Added training accuracy to the performance graph for comparison (Cell 11).\n# *   **Clearer Configuration:** Key hyperparameters like Dropout Rate and Early Stopping Patience are easily configurable.\n#\n# Ready to train a more robust and potentially better-generalizing AI? Let's go! ❤️\n\n# %% {\"cell_type\": \"code\"}\nimport os\nimport numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F # Often useful for activation functions etc.\nfrom chess import Board, pgn, Move, flip_horizontal # Board has SVG capabilities!\nfrom tqdm.notebook import tqdm # Use notebook version for better display\nimport pickle\nimport gc # Garbage collector interface\nimport matplotlib.pyplot as plt # For plotting graphs\nimport random\nfrom sklearn.model_selection import train_test_split # For easy data splitting!\nfrom IPython.display import display, SVG # To display SVG output\n\nprint(\"Imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:13:13.741862Z","iopub.execute_input":"2025-04-07T20:13:13.742168Z","iopub.status.idle":"2025-04-07T20:13:17.408906Z","shell.execute_reply.started":"2025-04-07T20:13:13.742140Z","shell.execute_reply":"2025-04-07T20:13:17.408191Z"}},"outputs":[{"name":"stdout","text":"Imports successful!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 4: Auxiliary Functions (Board Rep, Data Gen, Move Encoding)\n# Contains helpers for board representation, data generation, move encoding, ELO parsing.\n# Data augmentation logic is included within process_games_generator_v4.\n\n# %% {\"cell_type\": \"code\"}\ndef board_to_matrix_v4(board: Board, flip: bool = False):\n    \"\"\"\n    Converts a board state into a matrix representation (multi-channel).\n    Version 4: Includes piece positions, turn, castling rights, and optional horizontal flip.\n    Input features are consistent with V3.\n\n    Output shape: (18, 8, 8) - float32 numpy array\n    Channels:\n    - 0-5: White pieces (P, N, B, R, Q, K)\n    - 6-11: Black pieces (P, N, B, R, Q, K)\n    - 12: White King Castling Right (1 if True)\n    - 13: White Queen Castling Right (1 if True)\n    - 14: Black King Castling Right (1 if True)\n    - 15: Black Queen Castling Right (1 if True)\n    - 16: White's Turn (1 if White's turn)\n    - 17: Constant plane of 1s\n    \"\"\"\n    matrix = np.zeros((18, 8, 8), dtype=np.float32)\n    current_board = board.copy() # Work on a copy to avoid modifying the original board\n\n    # Apply flip transformation if requested\n    if flip:\n        current_board = current_board.transform(flip_horizontal)\n\n    # Populate piece layers based on the (potentially flipped) board state\n    piece_map = current_board.piece_map()\n    for square, piece in piece_map.items():\n        row, col = divmod(square, 8)\n        piece_idx = piece.piece_type - 1\n        color_offset = 0 if piece.color else 6 # White=0, Black=6\n        matrix[piece_idx + color_offset, row, col] = 1\n\n    # Populate castling rights layers (use ORIGINAL board's perspective)\n    if board.has_kingside_castling_rights(True): matrix[12, :, :] = 1\n    if board.has_queenside_castling_rights(True): matrix[13, :, :] = 1\n    if board.has_kingside_castling_rights(False): matrix[14, :, :] = 1\n    if board.has_queenside_castling_rights(False): matrix[15, :, :] = 1\n\n    # Populate turn layer (based on the CURRENT board state after potential flip)\n    if current_board.turn: # True if White's turn in the current_board state\n        matrix[16, :, :] = 1\n\n    # Populate constant plane\n    matrix[17, :, :] = 1\n\n    return matrix\n\ndef get_value_target(game_result: str) -> float:\n    \"\"\" Assigns a numerical value based on the game result from White's perspective. \"\"\"\n    if game_result == '1-0': return 1.0\n    if game_result == '0-1': return -1.0\n    if game_result == '1/2-1/2': return 0.0\n    return 0.0 # Default for '*' or unexpected results\n\ndef parse_elo(elo_str: str) -> int:\n    \"\"\" Safely parses ELO string, returns 0 if invalid. \"\"\"\n    try:\n        return int(str(elo_str).replace('?', '').strip()) # Add str() for robustness\n    except (ValueError, AttributeError, TypeError):\n        return 0\n\ndef flip_move(move: Move) -> Move:\n    \"\"\" Flips a move horizontally. \"\"\"\n    from_sq_row, from_sq_col = divmod(move.from_square, 8)\n    to_sq_row, to_sq_col = divmod(move.to_square, 8)\n\n    from_square_flipped = from_sq_row * 8 + (7 - from_sq_col)\n    to_square_flipped = to_sq_row * 8 + (7 - to_sq_col)\n\n    promotion_flipped = move.promotion\n    return Move(from_square_flipped, to_square_flipped, promotion=promotion_flipped)\n\ndef process_games_generator_v4(pgn_filepath, min_elo=0, max_games_in_file=None, limit_total_samples=None, augment=True):\n    \"\"\"\n    Generator that processes a SINGLE PGN file iteratively with filters.\n    Yields tuples of (board_matrix, move_uci, value_target, avg_elo) for each position.\n    Handles augmentation internally if enabled.\n    \"\"\"\n    print(f\"\\nStarting V4 Generator for file: {os.path.basename(pgn_filepath)}\")\n    print(f\"  Settings: min_elo={min_elo}, max_games={max_games_in_file}, limit_samples={limit_total_samples}, augment={augment}\")\n    total_samples_yielded = 0\n    processed_games_count = 0\n    accepted_games_count = 0\n    elo_list_accepted = []\n\n    try:\n        with open(pgn_filepath, 'r', encoding='utf-8', errors='ignore') as pgn_file:\n            while True:\n                # Check total sample limit\n                if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                    print(f\"\\nGenerator hit total sample limit ({limit_total_samples}). Stopping.\")\n                    return elo_list_accepted # Return collected ELOs\n\n                # Check game limit for this file\n                if max_games_in_file is not None and processed_games_count >= max_games_in_file:\n                    print(f\"Generator hit game limit ({max_games_in_file}) for this file. Stopping.\")\n                    break # Stop processing this file\n\n                game_headers = None\n                current_game_start_pos = pgn_file.tell() # Remember position before reading game\n\n                try:\n                    game_headers = pgn.read_headers(pgn_file)\n                    if game_headers is None: break # Normal end of file\n                    processed_games_count += 1\n\n                    white_elo = parse_elo(game_headers.get(\"WhiteElo\", \"0\"))\n                    black_elo = parse_elo(game_headers.get(\"BlackElo\", \"0\"))\n                    avg_elo = (white_elo + black_elo) / 2 if white_elo > 0 and black_elo > 0 else 0\n\n                    if white_elo < min_elo and black_elo < min_elo:\n                        try: pgn.read_game(pgn_file) # Read and discard to advance pointer\n                        except Exception: pass\n                        continue\n\n                    pgn_file.seek(current_game_start_pos)\n                    game = pgn.read_game(pgn_file)\n                    if game is None:\n                         print(f\"Warning: Could not read game body after headers for game {processed_games_count}. Skipping.\")\n                         continue\n\n                except EOFError: break\n                except Exception as e:\n                    print(f\"Skipping game {processed_games_count} due to header/initial parsing error: {e}\")\n                    try: # Recovery attempt\n                        while True:\n                            line = pgn_file.readline()\n                            if not line: break\n                            if line.startswith('[Event '):\n                                pgn_file.seek(pgn_file.tell() - len(line.encode('utf-8')))\n                                break\n                    except Exception as re:\n                         print(f\"Recovery attempt failed: {re}. Stopping.\")\n                         break\n                    continue\n\n                accepted_games_count += 1\n                if avg_elo > 0: elo_list_accepted.append(avg_elo)\n\n                result = game.headers.get(\"Result\", \"*\")\n                value_target = get_value_target(result)\n                board = game.board()\n\n                try:\n                    move_count_in_game = 0\n                    for move in game.mainline_moves():\n                        move_count_in_game += 1\n                        if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                             print(f\"\\nGenerator hit total sample limit ({limit_total_samples}) mid-game. Stopping.\")\n                             return elo_list_accepted\n\n                        # Original Sample\n                        matrix = board_to_matrix_v4(board, flip=False)\n                        move_uci = move.uci()\n                        yield matrix, move_uci, value_target, avg_elo\n                        total_samples_yielded += 1\n\n                        # Augmented (Flipped) Sample\n                        if augment:\n                            if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                                 print(f\"\\nGenerator hit total sample limit ({limit_total_samples}) before augment. Stopping.\")\n                                 return elo_list_accepted\n\n                            matrix_flipped = board_to_matrix_v4(board, flip=True)\n                            move_flipped = flip_move(move)\n                            move_uci_flipped = move_flipped.uci()\n                            yield matrix_flipped, move_uci_flipped, value_target, avg_elo\n                            total_samples_yielded += 1\n\n                        board.push(move)\n\n                except (ValueError, AssertionError, AttributeError) as e:\n                    print(f\"Warning: Skipping moves in game {accepted_games_count} due to error: {e}. Game had {move_count_in_game} moves.\")\n                    break # Stop processing this game's moves\n\n    except FileNotFoundError: print(f\"Error: PGN file not found at {pgn_filepath}\")\n    except Exception as e: print(f\"An unexpected error occurred while processing {os.path.basename(pgn_filepath)}: {e}\")\n\n    print(f\"\\nFinished generator for file {os.path.basename(pgn_filepath)}.\")\n    print(f\"  Games Scanned: {processed_games_count}, Accepted: {accepted_games_count}\")\n    return elo_list_accepted\n\ndef encode_moves_v4(moves_uci_list):\n    \"\"\" Encodes a list of UCI move strings into integer labels. Returns all mappings. \"\"\"\n    print(f\"\\nEncoding {len(moves_uci_list)} policy targets...\")\n    unique_moves = sorted(list(set(moves_uci_list))) # Sort for consistent mapping\n    move_to_int = {move: idx for idx, move in enumerate(unique_moves)}\n    int_to_move = {idx: move for move, idx in move_to_int.items()} # Inverse mapping\n    num_classes = len(unique_moves)\n    encoded_y = np.array([move_to_int[move] for move in moves_uci_list], dtype=np.int64) # Use int64 for PyTorch CrossEntropyLoss\n    print(f\"Found {num_classes} unique moves across all samples.\")\n    return encoded_y, move_to_int, int_to_move, num_classes\n\nprint(\"Auxiliary functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:13:17.410173Z","iopub.execute_input":"2025-04-07T20:13:17.410531Z","iopub.status.idle":"2025-04-07T20:13:17.429279Z","shell.execute_reply.started":"2025-04-07T20:13:17.410510Z","shell.execute_reply":"2025-04-07T20:13:17.428413Z"}},"outputs":[{"name":"stdout","text":"Auxiliary functions defined.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 5: Dataset Class (V5)\n\n# %% {\"cell_type\": \"code\"}\nclass ChessDatasetV5(Dataset):\n    \"\"\" Basic PyTorch Dataset for Chess Data \"\"\"\n    def __init__(self, X_matrices, y_policy_encoded, y_value_targets, dataset_name=\"\"):\n        self.X = X_matrices\n        self.y_policy = y_policy_encoded\n        self.y_value = y_value_targets\n        self.name = dataset_name\n        print(f\"\\nChessDatasetV5 ({self.name}) initialized:\")\n        if len(self.X) > 0:\n            print(f\"  Number of samples: {len(self.X)}\")\n            print(f\"  Sample X type: {type(self.X[0])}, shape: {self.X[0].shape if isinstance(self.X[0], np.ndarray) else 'N/A'}\")\n            print(f\"  Sample y_policy type: {type(self.y_policy[0])}, value: {self.y_policy[0] if len(self.y_policy) > 0 else 'N/A'}\")\n            print(f\"  Sample y_value type: {type(self.y_value[0])}, value: {self.y_value[0] if len(self.y_value) > 0 else 'N/A'}\")\n        else:\n            print(f\"  Dataset ({self.name}) is empty.\")\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Returns data as tensors for the DataLoader\n        return (torch.tensor(self.X[idx], dtype=torch.float32),\n                torch.tensor(self.y_policy[idx], dtype=torch.long), # Policy target must be torch.long\n                torch.tensor(self.y_value[idx], dtype=torch.float32))\n\nprint(\"ChessDatasetV5 class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:17:32.115125Z","iopub.execute_input":"2025-04-07T16:17:32.115447Z","iopub.status.idle":"2025-04-07T16:17:32.122325Z","shell.execute_reply.started":"2025-04-07T16:17:32.115423Z","shell.execute_reply":"2025-04-07T16:17:32.121301Z"}},"outputs":[{"name":"stdout","text":"ChessDatasetV5 class defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 6: Model Definition (V5 - ResNet with Dropout)\n\n# %% {\"cell_type\": \"code\"}\nclass ResidualBlock(nn.Module):\n    \"\"\" Standard Residual Block with Conv -> BN -> ReLU -> Conv -> BN -> Add -> ReLU \"\"\"\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ChessModelV5(nn.Module):\n    \"\"\"\n    V5: Dual-headed ResNet-style model with Dropout for regularization.\n    Takes (N, 18, 8, 8) input tensor.\n    Outputs policy logits (N, num_moves) and value prediction (N, 1).\n    \"\"\"\n    def __init__(self, num_policy_classes, num_res_blocks=9, num_channels=128, dropout_rate=0.3): ### V5 Change: Added dropout_rate\n        super(ChessModelV5, self).__init__()\n        input_channels = 18 # From board_to_matrix_v4\n        self.dropout_rate = dropout_rate ### V5 Change: Store dropout rate\n\n        # --- Input Convolution Block ---\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        # --- Residual Blocks Tower ---\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_channels) for _ in range(num_res_blocks)]\n        )\n\n        # --- Policy Head ---\n        self.policy_conv = nn.Conv2d(num_channels, 32, kernel_size=1, bias=False)\n        self.policy_bn = nn.BatchNorm2d(32)\n        self.policy_relu = nn.ReLU(inplace=True)\n        self.policy_flatten = nn.Flatten()\n        policy_flat_size = self._get_flat_size(lambda x: self.policy_relu(self.policy_bn(self.policy_conv(x))), input_channels, num_channels)\n        ### V5 Change: Added Dropout before final policy layer\n        self.policy_dropout = nn.Dropout(p=self.dropout_rate)\n        self.policy_fc = nn.Linear(policy_flat_size, num_policy_classes)\n\n        # --- Value Head ---\n        self.value_conv = nn.Conv2d(num_channels, 16, kernel_size=1, bias=False)\n        self.value_bn = nn.BatchNorm2d(16)\n        self.value_relu = nn.ReLU(inplace=True)\n        self.value_flatten = nn.Flatten()\n        value_flat_size = self._get_flat_size(lambda x: self.value_relu(self.value_bn(self.value_conv(x))), input_channels, num_channels)\n        self.value_fc1 = nn.Linear(value_flat_size, 64)\n        self.value_relu2 = nn.ReLU(inplace=True)\n        ### V5 Change: Added Dropout layers in value head\n        self.value_dropout1 = nn.Dropout(p=self.dropout_rate)\n        self.value_fc2 = nn.Linear(64, 1)\n        self.value_tanh = nn.Tanh() # Squash output to [-1, 1]\n\n        print(f\"\\nChessModelV5 initialized:\")\n        print(f\"  Input Channels: {input_channels}\")\n        print(f\"  Residual Blocks: {num_res_blocks} x {num_channels} channels\")\n        print(f\"  Dropout Rate: {self.dropout_rate}\") ### V5 Change: Print dropout rate\n        print(f\"  Policy Head Flattened Size: {policy_flat_size}\")\n        print(f\"  Value Head Flattened Size: {value_flat_size}\")\n        print(f\"  Output Policy Classes: {num_policy_classes}\")\n\n        self._initialize_weights()\n\n    def _get_flat_size(self, head_conv_lambda, input_channels, num_body_channels):\n        \"\"\" Helper function to calculate flattened size after conv blocks. \"\"\"\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, input_channels, 8, 8)\n            dummy_body_features = self.conv_in(dummy_input)\n            dummy_res_features = self.res_blocks(dummy_body_features)\n            dummy_head_features = head_conv_lambda(dummy_res_features)\n            flat_size = dummy_head_features.view(1, -1).size(1)\n        return flat_size\n\n    def _initialize_weights(self):\n        \"\"\" Initializes weights using common practices (Kaiming for Conv, Xavier for Linear). \"\"\"\n        print(\"Initializing model weights...\")\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                 nn.init.xavier_uniform_(m.weight)\n                 if m.bias is not None:\n                     nn.init.constant_(m.bias, 0)\n        print(\"Weight initialization complete.\")\n\n    def forward(self, x):\n        features = self.conv_in(x)\n        features = self.res_blocks(features)\n\n        # Policy Head Forward\n        policy = self.policy_conv(features)\n        policy = self.policy_bn(policy)\n        policy = self.policy_relu(policy)\n        policy = self.policy_flatten(policy)\n        policy = self.policy_dropout(policy) ### V5 Change: Apply dropout\n        policy_logits = self.policy_fc(policy)\n\n        # Value Head Forward\n        value = self.value_conv(features)\n        value = self.value_bn(value)\n        value = self.value_relu(value)\n        value = self.value_flatten(value)\n        value = self.value_fc1(value)\n        value = self.value_relu2(value)\n        value = self.value_dropout1(value) ### V5 Change: Apply dropout\n        value = self.value_fc2(value)\n        value_output = self.value_tanh(value)\n\n        return policy_logits, value_output\n\nprint(\"ChessModelV5 (ResNet style with Dropout) class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:13:17.430259Z","iopub.execute_input":"2025-04-07T20:13:17.430564Z","iopub.status.idle":"2025-04-07T20:13:17.447211Z","shell.execute_reply.started":"2025-04-07T20:13:17.430534Z","shell.execute_reply":"2025-04-07T20:13:17.446631Z"}},"outputs":[{"name":"stdout","text":"ChessModelV5 (ResNet style with Dropout) class defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 7: Data Loading, Processing, and Splitting (V5)\n\n# %% {\"cell_type\": \"code\"}\n# --- Configuration ---\nLOAD_PREPROCESSED_DATA = True  # <<< SET TO False for the first run, True for subsequent runs\nPREPROCESSED_DATA_DIR = \"/kaggle/input/aguemntation-files/processed_data/\" # Directory containing saved data (if LOAD_PREPROCESSED_DATA=True)\n#PREPROCESSED_DATA_DIR = \"/kaggle/working/processed_data/\" # <<< USE THIS if generating data for the first time\nPREPROCESSED_NPZ_PATH = os.path.join(PREPROCESSED_DATA_DIR, \"chess_data_split.npz\")\nPREPROCESSED_MAPPINGS_PATH = os.path.join(PREPROCESSED_DATA_DIR, \"chess_mappings_processed.pkl\")\n\n# PGN Source (Only used if LOAD_PREPROCESSED_DATA = False)\nPGN_FILE_PATH = None\nPGN_DIR = \"/kaggle/input/lichess-high-quality-games/Lichess Elite Database\" # Example path\n\n# Output paths for trained model and mappings\nMODEL_SAVE_PATH = \"/kaggle/working/chess_model_v5_resnet_dropout.pth\" ### V5 Change: Path name\nMAPPING_SAVE_PATH = \"/kaggle/working/chess_mappings_v5.pkl\" ### V5 Change: Path name\n\n# --- Data Filtering & Sampling (Only used if LOAD_PREPROCESSED_DATA = False) ---\nMIN_ELO = 2400\nMAX_GAMES_PER_FILE = 1000 # Limit games read *per file* if processing a directory\nTOTAL_SAMPLE_LIMIT = 3_000_000 # Max board positions (samples) to load *before* splitting\n\n### V5 Change: Augmentation is highly recommended to reduce overfitting. Defaulting to True.\n### ** IMPORTANT: If LOAD_PREPROCESSED_DATA=True, ensure the loaded data was created with the desired augmentation setting! **\n### ** If you change USE_AUGMENTATION, you MUST set LOAD_PREPROCESSED_DATA = False and re-process.**\nUSE_AUGMENTATION = True # <<< Set to True unless you have a specific reason not to\n\nVALIDATION_SET_SIZE = 0.15 # Use 15% of the loaded data for validation\n\n# --- Initialize variables ---\ndata_ready = False\nX_train_np, y_policy_train_np, y_value_train_np = None, None, None\nX_val_np, y_policy_val_np, y_value_val_np = None, None, None\nmove_to_int, int_to_move, num_classes = None, None, None\nelo_all_list = []\n\n# --- Create directory if it doesn't exist (Needed if saving preprocessed data) ---\nif not LOAD_PREPROCESSED_DATA:\n     os.makedirs(PREPROCESSED_DATA_DIR, exist_ok=True)\n\n# --- Attempt to Load Preprocessed Data ---\nif LOAD_PREPROCESSED_DATA and os.path.exists(PREPROCESSED_NPZ_PATH) and os.path.exists(PREPROCESSED_MAPPINGS_PATH):\n    print(f\"--- Loading Preprocessed Data ---\")\n    print(f\"  Source Directory: {PREPROCESSED_DATA_DIR}\")\n    print(f\"  *** Ensure this data was created with USE_AUGMENTATION={USE_AUGMENTATION} ***\") # V5 Warning\n    try:\n        print(f\"  Loading arrays from: {PREPROCESSED_NPZ_PATH}\")\n        with np.load(PREPROCESSED_NPZ_PATH) as data:\n            X_train_np = data['X_train']\n            y_policy_train_np = data['y_policy_train']\n            y_value_train_np = data['y_value_train']\n            X_val_np = data['X_val']\n            y_policy_val_np = data['y_policy_val']\n            y_value_val_np = data['y_value_val']\n        print(\"    NumPy arrays loaded successfully.\")\n\n        print(f\"  Loading mappings from: {PREPROCESSED_MAPPINGS_PATH}\")\n        with open(PREPROCESSED_MAPPINGS_PATH, \"rb\") as f:\n            loaded_mappings = pickle.load(f)\n        move_to_int = loaded_mappings['move_to_int']\n        int_to_move = loaded_mappings['int_to_move']\n        num_classes = loaded_mappings['num_classes']\n        print(\"    Mappings loaded successfully.\")\n        print(f\"    Train samples: {len(X_train_np)}, Val samples: {len(X_val_np)}, Num Classes: {num_classes}\")\n\n        data_ready = True # Flag that data is ready\n\n    except Exception as e:\n        print(f\"  Error loading preprocessed data: {e}. Will reprocess from PGNs.\")\n        X_train_np, y_policy_train_np, y_value_train_np = None, None, None\n        X_val_np, y_policy_val_np, y_value_val_np = None, None, None\n        move_to_int, int_to_move, num_classes = None, None, None\n        data_ready = False\n\n# --- Reprocess Data from PGNs if Loading Failed or Disabled ---\nif not data_ready:\n    print(\"\\n--- Processing Data from PGN Files ---\")\n    print(f\"  Augmentation Enabled: {USE_AUGMENTATION}\")\n    print(f\"  Preprocessed data will be saved to: {PREPROCESSED_DATA_DIR}\")\n\n    pgn_files_to_process = []\n    if PGN_FILE_PATH and os.path.isfile(PGN_FILE_PATH):\n        pgn_files_to_process = [PGN_FILE_PATH]\n        print(f\"Processing single PGN file: {PGN_FILE_PATH}\")\n    elif PGN_DIR and os.path.isdir(PGN_DIR):\n        files_in_dir = sorted([os.path.join(PGN_DIR, f) for f in os.listdir(PGN_DIR) if f.endswith(\".pgn\")])\n        pgn_files_to_process = files_in_dir\n        print(f\"Found {len(pgn_files_to_process)} PGN files in directory: {PGN_DIR}\")\n    else:\n        print(f\"Error: No valid PGN file or directory specified. Check PGN_FILE_PATH or PGN_DIR.\")\n\n    X_all_list = []\n    y_policy_uci_all_list = []\n    y_value_all_list = []\n    elo_all_list = [] # Reset elo list\n\n    if pgn_files_to_process:\n        print(f\"\\n--- Starting Data Generation Phase ---\")\n        global_samples_yielded = 0\n        generator_needs_break = False\n        for pgn_path in pgn_files_to_process:\n            if generator_needs_break: break\n            remaining_samples = TOTAL_SAMPLE_LIMIT - global_samples_yielded if TOTAL_SAMPLE_LIMIT else None\n            data_generator = process_games_generator_v4(\n                pgn_path, min_elo=MIN_ELO, max_games_in_file=MAX_GAMES_PER_FILE,\n                limit_total_samples=remaining_samples, augment=USE_AUGMENTATION ### V5 Change: Pass augment flag\n            )\n            try:\n                file_samples = 0\n                for matrix, move_uci, value, avg_elo in tqdm(data_generator, desc=f\"Processing {os.path.basename(pgn_path)}\", leave=False):\n                     X_all_list.append(matrix)\n                     y_policy_uci_all_list.append(move_uci)\n                     y_value_all_list.append(value)\n                     if avg_elo > 0: elo_all_list.append(avg_elo)\n                     global_samples_yielded += 1\n                     file_samples += 1\n                     if TOTAL_SAMPLE_LIMIT is not None and global_samples_yielded >= TOTAL_SAMPLE_LIMIT:\n                          print(f\"\\nGlobal sample limit {TOTAL_SAMPLE_LIMIT} reached.\")\n                          generator_needs_break = True\n                          break\n                print(f\"  Finished {os.path.basename(pgn_path)}, added {file_samples} samples.\")\n            except Exception as e:\n                print(f\"Error during generation for {os.path.basename(pgn_path)}: {e}\")\n\n        print(f\"\\n--- Data Generation Complete ---\")\n        print(f\"Collected {len(X_all_list)} total samples (incl. augmentation={USE_AUGMENTATION}).\")\n\n        if not X_all_list:\n            print(\"Error: No samples collected.\")\n            can_proceed = False\n        else:\n            y_policy_encoded_all, move_to_int, int_to_move, num_classes = encode_moves_v4(y_policy_uci_all_list)\n            can_proceed = True\n\n        if can_proceed:\n            print(f\"\\nSplitting data (Train: {1-VALIDATION_SET_SIZE:.0%}, Val: {VALIDATION_SET_SIZE:.0%})...\")\n            indices = list(range(len(X_all_list)))\n            train_indices, val_indices = train_test_split(\n                indices, test_size=VALIDATION_SET_SIZE, random_state=42, shuffle=True\n            )\n            print(f\"  Train set size: {len(train_indices)}\")\n            print(f\"  Validation set size: {len(val_indices)}\")\n\n            print(\"\\nCreating NumPy arrays...\")\n            # Use list comprehensions for potentially better memory efficiency during creation\n            X_train_np = np.array([X_all_list[i] for i in train_indices], dtype=np.float32)\n            y_policy_train_np = np.array([y_policy_encoded_all[i] for i in train_indices], dtype=np.int64)\n            y_value_train_np = np.array([y_value_all_list[i] for i in train_indices], dtype=np.float32)\n            X_val_np = np.array([X_all_list[i] for i in val_indices], dtype=np.float32)\n            y_policy_val_np = np.array([y_policy_encoded_all[i] for i in val_indices], dtype=np.int64)\n            y_value_val_np = np.array([y_value_all_list[i] for i in val_indices], dtype=np.float32)\n            print(\"  Arrays created.\")\n\n            print(f\"--- Saving Preprocessed Data ---\")\n            try:\n                print(f\"  Saving arrays to: {PREPROCESSED_NPZ_PATH}\")\n                np.savez_compressed(PREPROCESSED_NPZ_PATH,\n                                    X_train=X_train_np, y_policy_train=y_policy_train_np, y_value_train=y_value_train_np,\n                                    X_val=X_val_np, y_policy_val=y_policy_val_np, y_value_val=y_value_val_np)\n                print(\"    NumPy arrays saved successfully.\")\n\n                mappings_to_save = {\n                    'move_to_int': move_to_int,\n                    'int_to_move': int_to_move,\n                    'num_classes': num_classes\n                    # Consider saving config flags like USE_AUGMENTATION here too\n                }\n                print(f\"  Saving mappings to: {PREPROCESSED_MAPPINGS_PATH}\")\n                with open(PREPROCESSED_MAPPINGS_PATH, \"wb\") as f:\n                    pickle.dump(mappings_to_save, f)\n                print(\"    Mappings saved successfully.\")\n                print(f\"    --> Set LOAD_PREPROCESSED_DATA = True at the top of this cell to load next time <--\")\n\n            except Exception as e:\n                print(f\"  Error saving preprocessed data: {e}\")\n\n            print(\"\\nCleaning up original large lists and encoded array...\")\n            del X_all_list, y_policy_uci_all_list, y_value_all_list, y_policy_encoded_all\n            del train_indices, val_indices, indices\n            gc.collect()\n            print(\"Cleanup complete.\")\n            data_ready = True # Data is ready after processing\n\n        else: # if not can_proceed\n             data_ready = False\n    else: # if not pgn_files_to_process\n        print(\"Skipping data generation as no valid PGN source was found.\")\n        data_ready = False\n\n# --- Final Check ---\nif not data_ready:\n    print(\"\\nERROR: Data preparation failed. Cannot proceed to training setup.\")\nelse:\n    print(\"\\n--- Data is ready for Training Setup (Next Cell) ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:14:27.638953Z","iopub.execute_input":"2025-04-07T20:14:27.639230Z","iopub.status.idle":"2025-04-07T20:15:00.551975Z","shell.execute_reply.started":"2025-04-07T20:14:27.639206Z","shell.execute_reply":"2025-04-07T20:15:00.551268Z"}},"outputs":[{"name":"stdout","text":"--- Loading Preprocessed Data ---\n  Source Directory: /kaggle/input/aguemntation-files/processed_data/\n  *** Ensure this data was created with USE_AUGMENTATION=True ***\n  Loading arrays from: /kaggle/input/aguemntation-files/processed_data/chess_data_split.npz\n    NumPy arrays loaded successfully.\n  Loading mappings from: /kaggle/input/aguemntation-files/processed_data/chess_mappings_processed.pkl\n    Mappings loaded successfully.\n    Train samples: 2550000, Val samples: 450000, Num Classes: 1888\n\n--- Data is ready for Training Setup (Next Cell) ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 8: Basic EDA - ELO Distribution\n\n# %% {\"cell_type\": \"code\"}\nif 'elo_all_list' in locals() and elo_all_list:\n    print(f\"\\n--- Basic EDA: ELO Distribution of Accepted Games ({len(elo_all_list)} positions/samples) ---\")\n    plt.figure(figsize=(10, 5))\n    plt.hist(elo_all_list, bins=60, color='lightcoral', edgecolor='black')\n    plt.title(f'Distribution of Average ELOs in Collected Samples (Min Player ELO Filter: {MIN_ELO})')\n    plt.xlabel('Average ELO of Game')\n    plt.ylabel('Number of Samples (Board Positions)')\n    plt.grid(axis='y', alpha=0.75)\n    plt.show()\n\n    avg_elo_loaded = np.mean(elo_all_list)\n    median_elo_loaded = np.median(elo_all_list)\n    print(f\"  Average ELO of samples: {avg_elo_loaded:.0f}\")\n    print(f\"  Median ELO of samples: {median_elo_loaded:.0f}\")\nelse:\n    print(\"\\nNo ELO data collected for EDA (possibly due to loading preprocessed data or filtering).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:15:43.891627Z","iopub.execute_input":"2025-04-07T20:15:43.891933Z","iopub.status.idle":"2025-04-07T20:15:43.897632Z","shell.execute_reply.started":"2025-04-07T20:15:43.891908Z","shell.execute_reply":"2025-04-07T20:15:43.896681Z"}},"outputs":[{"name":"stdout","text":"\nNo ELO data collected for EDA (possibly due to loading preprocessed data or filtering).\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 9: Training Setup (V5 - Device, Datasets, Model, Optimizer)\n\n# %% {\"cell_type\": \"code\"}\n# --- Hyperparameters & Configuration ---\nBATCH_SIZE = 4096  # Adjust based on GPU VRAM\nLEARNING_RATE = 0.001 # Initial learning rate for Adam\nNUM_EPOCHS = 25 # Max number of epochs (might stop early)\nPOLICY_LOSS_WEIGHT = 1.0\nVALUE_LOSS_WEIGHT = 0.8 # Weight for the value loss component\n\n# Model Configuration (MUST match saved model if loading weights)\nNUM_RES_BLOCKS = 9\nNUM_CHANNELS = 128\nDROPOUT_RATE = 0.3 # ### V5 Change: Dropout rate for regularization\n\n# Regularization & Optimization\nWEIGHT_DECAY = 2e-5 # ### V5 Change: Slightly increased L2 Regularization for Adam\nSCHEDULER_PATIENCE = 3 # ReduceLROnPlateau patience\nSCHEDULER_FACTOR = 0.2 # Factor to reduce LR by\n\n# ### V5 Change: Early Stopping Configuration\nEARLY_STOPPING_PATIENCE = 5 # Stop after N epochs with no validation loss improvement\n                            # Should generally be >= SCHEDULER_PATIENCE\n\n# --- Setup Device ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"Using {torch.cuda.device_count()} GPU(s):\")\n    for i in range(torch.cuda.device_count()):\n         print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n    torch.cuda.empty_cache()\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU (Training will be significantly slower!)\")\n    # Mock device count if no CUDA for DataParallel checks\n    if not hasattr(torch.cuda, 'device_count'): torch.cuda.device_count = lambda: 0\n\n# --- Create Datasets and DataLoaders ---\ntrain_dataset = None\nval_dataset = None\ntrain_dataloader = None\nval_dataloader = None\nmodel = None # Initialize to None\nsetup_ok = False\n\nif data_ready: # Check if data was loaded/processed successfully\n    print(\"\\nCreating Datasets and DataLoaders...\")\n    try:\n        train_dataset = ChessDatasetV5(X_train_np, y_policy_train_np, y_value_train_np, dataset_name=\"Train\")\n        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=True)\n        print(f\"  Train DataLoader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n\n        val_dataset = ChessDatasetV5(X_val_np, y_policy_val_np, y_value_val_np, dataset_name=\"Validation\")\n        val_batch_size = BATCH_SIZE * 2 # Can often use larger batch for validation\n        val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=False)\n        print(f\"  Validation DataLoader: {len(val_dataloader)} batches of size {val_batch_size}\")\n\n        print(\"\\nCleaning up split NumPy arrays...\")\n        del X_train_np, y_policy_train_np, y_value_train_np\n        del X_val_np, y_policy_val_np, y_value_val_np\n        gc.collect()\n        print(\"NumPy arrays deleted.\")\n        setup_ok = True\n\n    except NameError as e:\n         print(f\"Error creating Datasets/DataLoaders: {e}\")\n         print(\"This likely means data arrays (e.g., X_train_np) were not loaded/created correctly in Cell 7.\")\n    except Exception as e:\n        print(f\"Error creating Datasets/DataLoaders: {e}\")\n\nelse:\n    print(\"\\nSkipping Dataset/DataLoader creation as data preparation failed (data_ready=False).\")\n\n\n# --- Model, Loss, Optimizer Initialization ---\nif setup_ok and 'num_classes' in locals() and num_classes is not None:\n    print(\"\\nInitializing Model, Loss Functions, and Optimizer...\")\n    # Initialize Model structure (V5 with Dropout)\n    model = ChessModelV5(\n        num_policy_classes=num_classes,\n        num_res_blocks=NUM_RES_BLOCKS,\n        num_channels=NUM_CHANNELS,\n        dropout_rate=DROPOUT_RATE ### V5 Change: Pass dropout rate\n    )\n\n    # DataParallel for multi-GPU\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"--- Wrapping model with nn.DataParallel for {torch.cuda.device_count()} GPUs ---\")\n        model = nn.DataParallel(model)\n\n    model.to(device) # Move model (or wrapped model) to device\n\n    print(f\"Model created/wrapped with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n    if device.type == 'cuda':\n         print(f\"  CUDA Memory Allocated after model load: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n         print(f\"  CUDA Memory Reserved after model load:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n\n    # Loss Functions\n    criterion_policy = nn.CrossEntropyLoss()\n    criterion_value = nn.MSELoss()\n    print(\"Loss functions defined (CrossEntropy for policy, MSE for value).\")\n\n    # Optimizer (AdamW might be slightly better, but Adam is fine)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    print(f\"Optimizer defined: Adam (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\")\n\n    # Learning Rate Scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE, verbose=True)\n    print(f\"Learning rate scheduler defined: ReduceLROnPlateau on validation loss (Patience={SCHEDULER_PATIENCE}, Factor={SCHEDULER_FACTOR}).\")\n\n    print(f\"Early stopping enabled with patience: {EARLY_STOPPING_PATIENCE}\") ### V5 Change: Print early stopping info\n\n    print(\"\\n--- Training setup complete! ---\")\n\nelse:\n    print(\"\\nSkipping Model/Optimizer setup due to previous errors or missing 'num_classes'.\")\n    setup_ok = False # Ensure setup_ok is False if we skip this","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:15:46.797289Z","iopub.execute_input":"2025-04-07T20:15:46.797634Z","iopub.status.idle":"2025-04-07T20:15:47.014927Z","shell.execute_reply.started":"2025-04-07T20:15:46.797606Z","shell.execute_reply":"2025-04-07T20:15:47.014020Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPU(s):\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\n\nCreating Datasets and DataLoaders...\nError creating Datasets/DataLoaders: name 'ChessDatasetV5' is not defined\nThis likely means data arrays (e.g., X_train_np) were not loaded/created correctly in Cell 7.\n\nSkipping Model/Optimizer setup due to previous errors or missing 'num_classes'.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 10: Training Loop with Validation and Early Stopping (V5)\n\n# %% {\"cell_type\": \"code\"}\n# Dictionary to store training and validation history\nhistory = {\n    'epoch': [],\n    'train_loss': [], 'train_policy_loss': [], 'train_value_loss': [], 'train_policy_accuracy': [], ### V5 Change: Added Train Accuracy\n    'val_loss': [], 'val_policy_loss': [], 'val_value_loss': [], 'val_policy_accuracy': [],\n    'lr': []\n}\n\nif setup_ok: # Check if model, dataloaders etc. were initialized\n    print(f\"\\n--- Starting Training for max {NUM_EPOCHS} Epochs ---\")\n    training_start_time = time.time()\n\n    # Variables for Early Stopping ### V5 Change: Early stopping vars\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n\n        # --- Training Phase ---\n        model.train() # Set model to training mode (enables dropout)\n        running_train_loss = 0.0\n        running_train_policy_loss = 0.0\n        running_train_value_loss = 0.0\n        correct_train_policy_predictions = 0 ### V5 Change: Train accuracy tracking\n        total_train_samples = 0 ### V5 Change: Train accuracy tracking\n\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n        for batch_idx, (inputs, policy_labels, value_labels) in enumerate(train_progress_bar):\n            inputs = inputs.to(device, non_blocking=True)\n            policy_labels = policy_labels.to(device, non_blocking=True)\n            value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n            optimizer.zero_grad()\n            policy_logits, value_output = model(inputs)\n\n            loss_policy = criterion_policy(policy_logits, policy_labels)\n            loss_value = criterion_value(value_output, value_labels)\n            total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5) # Optional: Gradient clipping\n            optimizer.step()\n\n            running_train_loss += total_loss.item()\n            running_train_policy_loss += loss_policy.item()\n            running_train_value_loss += loss_value.item()\n\n            # V5 Change: Calculate training accuracy\n            predicted_train_indices = torch.argmax(policy_logits, dim=1)\n            correct_train_policy_predictions += (predicted_train_indices == policy_labels).sum().item()\n            total_train_samples += policy_labels.size(0)\n\n            if (batch_idx + 1) % 100 == 0:\n                 train_progress_bar.set_postfix({\n                     'Loss': f'{running_train_loss / (batch_idx + 1):.4f}',\n                     'P_Loss': f'{running_train_policy_loss / (batch_idx + 1):.4f}',\n                     'V_Loss': f'{running_train_value_loss / (batch_idx + 1):.4f}'\n                 })\n\n        avg_train_loss = running_train_loss / len(train_dataloader)\n        avg_train_policy_loss = running_train_policy_loss / len(train_dataloader)\n        avg_train_value_loss = running_train_value_loss / len(train_dataloader)\n        train_policy_accuracy = correct_train_policy_predictions / total_train_samples if total_train_samples > 0 else 0.0 ### V5 Change: Final train accuracy\n\n        # --- Validation Phase ---\n        model.eval() # Set model to evaluation mode (disables dropout)\n        running_val_loss = 0.0\n        running_val_policy_loss = 0.0\n        running_val_value_loss = 0.0\n        correct_val_policy_predictions = 0\n        total_val_samples = 0\n\n        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Validate]\", leave=False)\n\n        with torch.no_grad():\n            for inputs, policy_labels, value_labels in val_progress_bar:\n                inputs = inputs.to(device, non_blocking=True)\n                policy_labels = policy_labels.to(device, non_blocking=True)\n                value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n                policy_logits, value_output = model(inputs)\n\n                loss_policy = criterion_policy(policy_logits, policy_labels)\n                loss_value = criterion_value(value_output, value_labels)\n                total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n                running_val_loss += total_loss.item()\n                running_val_policy_loss += loss_policy.item()\n                running_val_value_loss += loss_value.item()\n\n                predicted_val_indices = torch.argmax(policy_logits, dim=1)\n                correct_val_policy_predictions += (predicted_val_indices == policy_labels).sum().item()\n                total_val_samples += policy_labels.size(0)\n\n        avg_val_loss = running_val_loss / len(val_dataloader)\n        avg_val_policy_loss = running_val_policy_loss / len(val_dataloader)\n        avg_val_value_loss = running_val_value_loss / len(val_dataloader)\n        val_policy_accuracy = correct_val_policy_predictions / total_val_samples if total_val_samples > 0 else 0.0\n\n        # --- End of Epoch Summary ---\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} Summary ---\")\n        print(f\"  Time: {epoch_duration:.2f}s | LR: {current_lr:.6f}\")\n        print(f\"  Train Loss: {avg_train_loss:.4f} (Policy: {avg_train_policy_loss:.4f}, Value: {avg_train_value_loss:.4f}) | Train Acc: {train_policy_accuracy:.4f}\") ### V5 Change: Added Train Acc print\n        print(f\"  Valid Loss: {avg_val_loss:.4f} (Policy: {avg_val_policy_loss:.4f}, Value: {avg_val_value_loss:.4f}) | Valid Acc: {val_policy_accuracy:.4f}\")\n        if device.type == 'cuda':\n             print(f\"  CUDA Mem Used (Peak): {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB\")\n             torch.cuda.reset_peak_memory_stats(device)\n\n        # Store history\n        history['epoch'].append(epoch + 1)\n        history['train_loss'].append(avg_train_loss)\n        history['train_policy_loss'].append(avg_train_policy_loss)\n        history['train_value_loss'].append(avg_train_value_loss)\n        history['train_policy_accuracy'].append(train_policy_accuracy) ### V5 Change: Store train accuracy\n        history['val_loss'].append(avg_val_loss)\n        history['val_policy_loss'].append(avg_val_policy_loss)\n        history['val_value_loss'].append(avg_val_value_loss)\n        history['val_policy_accuracy'].append(val_policy_accuracy)\n        history['lr'].append(current_lr)\n\n        # Step the scheduler\n        scheduler.step(avg_val_loss)\n\n        # --- Save Best Model & Early Stopping Check --- ### V5 Change: Early Stopping Logic\n        if avg_val_loss < best_val_loss:\n            print(f\"  Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving best model...\")\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0 # Reset counter\n            try:\n                if isinstance(model, nn.DataParallel): state_dict_to_save = model.module.state_dict()\n                else: state_dict_to_save = model.state_dict()\n                best_model_save_path = MODEL_SAVE_PATH.replace('.pth', '_best.pth')\n                torch.save(state_dict_to_save, best_model_save_path)\n                print(f\"    Best model checkpoint saved to {best_model_save_path}\")\n            except Exception as e: print(f\"    Error saving best model checkpoint: {e}\")\n        else:\n            epochs_no_improve += 1\n            print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s). Best: {best_val_loss:.4f}\")\n\n        # Early stopping condition\n        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\n--- Early stopping triggered after {epoch + 1} epochs ---\")\n            break\n\n        # Optional: Save periodic checkpoints\n        if (epoch + 1) % 10 == 0:\n           chkpt_path = f\"/kaggle/working/checkpoint_epoch_{epoch+1}.pth\"\n           try:\n                if isinstance(model, nn.DataParallel): state_dict_to_save = model.module.state_dict()\n                else: state_dict_to_save = model.state_dict()\n                torch.save(state_dict_to_save, chkpt_path)\n                print(f\"  Periodic checkpoint saved to {chkpt_path}\")\n           except Exception as e: print(f\"    Error saving periodic checkpoint: {e}\")\n\n        gc.collect() # Collect garbage\n\n    # --- Training Finished ---\n    training_end_time = time.time()\n    total_training_time = training_end_time - training_start_time\n    print(f\"\\n--- Training finished in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes) ---\")\n    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\nelse:\n    print(\"Skipping training loop as setup was not successful.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:37:08.422664Z","iopub.execute_input":"2025-04-07T16:37:08.423157Z","iopub.status.idle":"2025-04-07T19:00:45.845825Z","shell.execute_reply.started":"2025-04-07T16:37:08.423122Z","shell.execute_reply":"2025-04-07T19:00:45.845097Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Training for max 25 Epochs ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 1/25 Summary ---\n  Time: 294.10s | LR: 0.001000\n  Train Loss: 4.1698 (Policy: 3.4845, Value: 0.8566) | Train Acc: 0.2130\n  Valid Loss: 3.2447 (Policy: 2.5839, Value: 0.8261) | Valid Acc: 0.3048\n  CUDA Mem Used (Peak): 2941.53 MB\n  Validation loss improved (inf --> 3.2447). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 2/25 Summary ---\n  Time: 344.86s | LR: 0.001000\n  Train Loss: 3.1500 (Policy: 2.4963, Value: 0.8172) | Train Acc: 0.3211\n  Valid Loss: 3.0494 (Policy: 2.3550, Value: 0.8680) | Valid Acc: 0.3455\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (3.2447 --> 3.0494). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 3/25 Summary ---\n  Time: 348.53s | LR: 0.001000\n  Train Loss: 2.9263 (Policy: 2.2872, Value: 0.7989) | Train Acc: 0.3583\n  Valid Loss: 2.9002 (Policy: 2.2554, Value: 0.8060) | Valid Acc: 0.3611\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (3.0494 --> 2.9002). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 4/25 Summary ---\n  Time: 347.84s | LR: 0.001000\n  Train Loss: 2.7866 (Policy: 2.1612, Value: 0.7818) | Train Acc: 0.3828\n  Valid Loss: 2.8277 (Policy: 2.1883, Value: 0.7993) | Valid Acc: 0.3747\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.9002 --> 2.8277). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 5/25 Summary ---\n  Time: 347.70s | LR: 0.001000\n  Train Loss: 2.6796 (Policy: 2.0693, Value: 0.7629) | Train Acc: 0.4019\n  Valid Loss: 2.7748 (Policy: 2.1412, Value: 0.7920) | Valid Acc: 0.3844\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.8277 --> 2.7748). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 6/25 Summary ---\n  Time: 348.01s | LR: 0.001000\n  Train Loss: 2.5892 (Policy: 1.9981, Value: 0.7388) | Train Acc: 0.4169\n  Valid Loss: 2.7649 (Policy: 2.1183, Value: 0.8083) | Valid Acc: 0.3900\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.7748 --> 2.7649). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 7/25 Summary ---\n  Time: 347.70s | LR: 0.001000\n  Train Loss: 2.5084 (Policy: 1.9388, Value: 0.7121) | Train Acc: 0.4302\n  Valid Loss: 2.7738 (Policy: 2.1033, Value: 0.8382) | Valid Acc: 0.3921\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.7649\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 8/25 Summary ---\n  Time: 348.37s | LR: 0.001000\n  Train Loss: 2.4372 (Policy: 1.8923, Value: 0.6811) | Train Acc: 0.4406\n  Valid Loss: 2.6812 (Policy: 2.0761, Value: 0.7563) | Valid Acc: 0.3992\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.7649 --> 2.6812). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 9/25 Summary ---\n  Time: 348.03s | LR: 0.001000\n  Train Loss: 2.3704 (Policy: 1.8503, Value: 0.6501) | Train Acc: 0.4509\n  Valid Loss: 2.7558 (Policy: 2.0811, Value: 0.8433) | Valid Acc: 0.4008\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.6812\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 10/25 Summary ---\n  Time: 347.13s | LR: 0.001000\n  Train Loss: 2.3114 (Policy: 1.8140, Value: 0.6217) | Train Acc: 0.4589\n  Valid Loss: 2.6316 (Policy: 2.0716, Value: 0.7000) | Valid Acc: 0.4010\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.6812 --> 2.6316). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n  Periodic checkpoint saved to /kaggle/working/checkpoint_epoch_10.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 11/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 11/25 Summary ---\n  Time: 347.32s | LR: 0.001000\n  Train Loss: 2.2573 (Policy: 1.7827, Value: 0.5933) | Train Acc: 0.4662\n  Valid Loss: 2.6323 (Policy: 2.0729, Value: 0.6992) | Valid Acc: 0.4031\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.6316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 12/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 12/25 Summary ---\n  Time: 347.32s | LR: 0.001000\n  Train Loss: 2.2086 (Policy: 1.7542, Value: 0.5681) | Train Acc: 0.4736\n  Valid Loss: 2.5996 (Policy: 2.0721, Value: 0.6594) | Valid Acc: 0.4081\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.6316 --> 2.5996). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 13/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 13/25 Summary ---\n  Time: 346.68s | LR: 0.001000\n  Train Loss: 2.1653 (Policy: 1.7283, Value: 0.5463) | Train Acc: 0.4797\n  Valid Loss: 2.5989 (Policy: 2.0655, Value: 0.6668) | Valid Acc: 0.4077\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.5996 --> 2.5989). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 14/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 14/25 Summary ---\n  Time: 347.45s | LR: 0.001000\n  Train Loss: 2.1277 (Policy: 1.7050, Value: 0.5284) | Train Acc: 0.4854\n  Valid Loss: 2.6298 (Policy: 2.0607, Value: 0.7113) | Valid Acc: 0.4070\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.5989\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 15/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 15/25 Summary ---\n  Time: 347.03s | LR: 0.001000\n  Train Loss: 2.0930 (Policy: 1.6843, Value: 0.5108) | Train Acc: 0.4904\n  Valid Loss: 2.6269 (Policy: 2.0664, Value: 0.7006) | Valid Acc: 0.4069\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 2 epoch(s). Best: 2.5989\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 16/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 16/25 Summary ---\n  Time: 346.35s | LR: 0.001000\n  Train Loss: 2.0613 (Policy: 1.6643, Value: 0.4962) | Train Acc: 0.4958\n  Valid Loss: 2.6008 (Policy: 2.0744, Value: 0.6579) | Valid Acc: 0.4084\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 3 epoch(s). Best: 2.5989\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 17/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc0479404f7410e94e0e7af08a865be"}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 17/25 Summary ---\n  Time: 344.58s | LR: 0.001000\n  Train Loss: 2.0325 (Policy: 1.6462, Value: 0.4828) | Train Acc: 0.4998\n  Valid Loss: 2.5956 (Policy: 2.0902, Value: 0.6317) | Valid Acc: 0.4075\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.5989 --> 2.5956). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 18/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 18/25 Summary ---\n  Time: 344.66s | LR: 0.001000\n  Train Loss: 2.0074 (Policy: 1.6300, Value: 0.4717) | Train Acc: 0.5042\n  Valid Loss: 2.5683 (Policy: 2.0748, Value: 0.6168) | Valid Acc: 0.4091\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.5956 --> 2.5683). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 19/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 19/25 Summary ---\n  Time: 346.55s | LR: 0.001000\n  Train Loss: 1.9834 (Policy: 1.6139, Value: 0.4619) | Train Acc: 0.5081\n  Valid Loss: 2.6050 (Policy: 2.0776, Value: 0.6593) | Valid Acc: 0.4096\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.5683\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 20/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 20/25 Summary ---\n  Time: 345.53s | LR: 0.001000\n  Train Loss: 1.9601 (Policy: 1.5988, Value: 0.4516) | Train Acc: 0.5122\n  Valid Loss: 2.5602 (Policy: 2.0772, Value: 0.6038) | Valid Acc: 0.4107\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.5683 --> 2.5602). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n  Periodic checkpoint saved to /kaggle/working/checkpoint_epoch_20.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 21/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 21/25 Summary ---\n  Time: 345.28s | LR: 0.001000\n  Train Loss: 1.9400 (Policy: 1.5847, Value: 0.4441) | Train Acc: 0.5156\n  Valid Loss: 2.6049 (Policy: 2.0997, Value: 0.6315) | Valid Acc: 0.4117\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 1 epoch(s). Best: 2.5602\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 22/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 22/25 Summary ---\n  Time: 344.18s | LR: 0.001000\n  Train Loss: 1.9195 (Policy: 1.5698, Value: 0.4371) | Train Acc: 0.5194\n  Valid Loss: 2.5925 (Policy: 2.0905, Value: 0.6275) | Valid Acc: 0.4091\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 2 epoch(s). Best: 2.5602\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 23/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 23/25 Summary ---\n  Time: 344.41s | LR: 0.001000\n  Train Loss: 1.9028 (Policy: 1.5582, Value: 0.4308) | Train Acc: 0.5225\n  Valid Loss: 2.5817 (Policy: 2.0931, Value: 0.6107) | Valid Acc: 0.4108\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 3 epoch(s). Best: 2.5602\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 24/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 24/25 Summary ---\n  Time: 345.37s | LR: 0.001000\n  Train Loss: 1.8860 (Policy: 1.5465, Value: 0.4243) | Train Acc: 0.5255\n  Valid Loss: 2.6015 (Policy: 2.1001, Value: 0.6267) | Valid Acc: 0.4138\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss did not improve for 4 epoch(s). Best: 2.5602\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/25 [Train]:   0%|          | 0/622 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 25/25 [Validate]:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- Epoch 25/25 Summary ---\n  Time: 346.84s | LR: 0.000200\n  Train Loss: 1.5873 (Policy: 1.3170, Value: 0.3378) | Train Acc: 0.5867\n  Valid Loss: 2.5456 (Policy: 2.0806, Value: 0.5812) | Valid Acc: 0.4224\n  CUDA Mem Used (Peak): 2941.59 MB\n  Validation loss improved (2.5602 --> 2.5456). Saving best model...\n    Best model checkpoint saved to /kaggle/working/chess_model_v5_resnet_dropout_best.pth\n\n--- Training finished in 8617.40 seconds (143.62 minutes) ---\nBest validation loss achieved: 2.5456\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 11: Plot Training Graphs (Loss & Accuracy) (V5)\n\n# %% {\"cell_type\": \"code\"}\nif history['epoch']: # Check if training ran and history exists\n    print(\"\\n--- Plotting Training and Validation Progress ---\")\n    epochs_range = history['epoch']\n\n    plt.style.use('seaborn-v0_8-darkgrid') # Use a nice style\n    plt.figure(figsize=(15, 12)) # Slightly larger figure\n\n    # --- Plot Total Loss ---\n    plt.subplot(2, 2, 1)\n    plt.plot(epochs_range, history['train_loss'], label='Train Total Loss', color='royalblue', marker='.')\n    plt.plot(epochs_range, history['val_loss'], label='Validation Total Loss', color='darkorange', marker='.')\n    plt.title('Total Loss vs. Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # --- Plot Policy Loss ---\n    plt.subplot(2, 2, 2)\n    plt.plot(epochs_range, history['train_policy_loss'], label='Train Policy Loss', color='dodgerblue', linestyle='--', marker='x')\n    plt.plot(epochs_range, history['val_policy_loss'], label='Validation Policy Loss', color='sandybrown', linestyle='--', marker='x')\n    plt.title('Policy Loss vs. Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Policy Loss (CrossEntropy)')\n    plt.legend()\n\n    # --- Plot Value Loss ---\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs_range, history['train_value_loss'], label='Train Value Loss', color='mediumseagreen', linestyle=':', marker='s')\n    plt.plot(epochs_range, history['val_value_loss'], label='Validation Value Loss', color='lightcoral', linestyle=':', marker='s')\n    plt.title('Value Loss vs. Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Value Loss (MSE)')\n    plt.legend()\n\n    # --- Plot Policy Accuracy & Learning Rate --- ### V5 Change: Plot train & val accuracy\n    ax1 = plt.subplot(2, 2, 4)\n    color1 = 'tab:red'\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Policy Accuracy', color=color1)\n    ax1.plot(epochs_range, history['train_policy_accuracy'], label='Train Accuracy', color='lightcoral', marker='*') # Train accuracy\n    ax1.plot(epochs_range, history['val_policy_accuracy'], label='Validation Accuracy', color='firebrick', marker='*') # Validation accuracy\n    ax1.tick_params(axis='y', labelcolor=color1)\n    ax1.legend(loc='upper left') # Legend for accuracy\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n    color2 = 'tab:blue'\n    ax2.set_ylabel('Learning Rate', color=color2)\n    ax2.plot(epochs_range, history['lr'], label='Learning Rate', color=color2, linestyle='-.', marker='+')\n    ax2.tick_params(axis='y', labelcolor=color2)\n    ax2.legend(loc='upper right') # Legend for LR\n\n    plt.title('Policy Accuracy & Learning Rate vs. Epoch')\n    ax1.grid(True, axis='y', linestyle=':', alpha=0.6) # Add light grid for primary axis\n\n    plt.tight_layout()\n    plt.show()\n\nelse:\n    print(\"\\nNo training history found to plot.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Save Final Model and Mappings (V5)\n\n# %% {\"cell_type\": \"code\"}\nif 'model' in locals() and model is not None and 'move_to_int' in locals():\n    # Save the final model state\n    print(f\"\\nSaving final model state dict to: {MODEL_SAVE_PATH}\")\n    try:\n        if isinstance(model, nn.DataParallel):\n            state_dict_to_save = model.module.state_dict()\n            print(\"  Saving final state_dict from model.module (DataParallel active)\")\n        else:\n            state_dict_to_save = model.state_dict()\n            print(\"  Saving final state_dict from model (DataParallel not active)\")\n        torch.save(state_dict_to_save, MODEL_SAVE_PATH)\n        print(\"  Final model saved successfully.\")\n    except Exception as e:\n        print(f\"  Error saving final model: {e}\")\n\n    # Save mappings\n    print(f\"Saving mappings to: {MAPPING_SAVE_PATH}\")\n    try:\n        mappings = {\n            'move_to_int': loaded_move_to_int if 'loaded_move_to_int' in locals() and loaded_move_to_int else move_to_int,\n            'int_to_move': loaded_int_to_move if 'loaded_int_to_move' in locals() and loaded_int_to_move else int_to_move,\n            'num_classes': loaded_num_classes if 'loaded_num_classes' in locals() and loaded_num_classes else num_classes\n        }\n        with open(MAPPING_SAVE_PATH, \"wb\") as f:\n            pickle.dump(mappings, f)\n        print(\"  Mappings saved successfully!\")\n    except Exception as e:\n        print(f\"  Error saving mappings: {e}\")\n\n    print(\"\\n--- Saving Complete ---\")\n    # Flag to indicate whether to use the best checkpoint during prediction\n    LOAD_BEST_MODEL_FOR_PREDICTION = True if 'best_val_loss' in locals() and best_val_loss != float('inf') else False\n\nelse:\n    print(\"\\nSkipping final saving as model or mappings are not available.\")\n    LOAD_BEST_MODEL_FOR_PREDICTION = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Prediction Setup (V5)\n\n# %% {\"cell_type\": \"code\"}\n# --- Load Mappings ---\nloaded_mappings = None\nloaded_move_to_int = None\nloaded_int_to_move = None\nloaded_num_classes = None\nprediction_model = None\nmodel_loaded_successfully = False\n\ntry:\n    print(f\"\\n--- Prediction Setup ---\")\n    print(f\"Loading mappings from: {MAPPING_SAVE_PATH}\")\n    with open(MAPPING_SAVE_PATH, \"rb\") as f:\n        loaded_mappings = pickle.load(f)\n    loaded_move_to_int = loaded_mappings['move_to_int']\n    loaded_int_to_move = loaded_mappings['int_to_move']\n    loaded_num_classes = loaded_mappings['num_classes']\n    print(\"Mappings loaded successfully.\")\n    print(f\"  Number of move classes: {loaded_num_classes}\")\nexcept Exception as e:\n    print(f\"Error: Could not load mapping file from {MAPPING_SAVE_PATH}: {e}\")\n    loaded_mappings = None\n\n# --- Setup Device ---\nif torch.cuda.is_available():\n    prediction_device = torch.device(\"cuda\")\n    print(f\"Using GPU for prediction.\")\nelse:\n    prediction_device = torch.device(\"cpu\")\n    print(\"Using CPU for prediction.\")\n\n# --- Load Model ---\nmodel_path_to_load = MODEL_SAVE_PATH # Default to final model\n\n# Choose whether to load the best model (if it was saved)\nif LOAD_BEST_MODEL_FOR_PREDICTION:\n     best_model_path = MODEL_SAVE_PATH.replace('.pth', '_best.pth')\n     if os.path.exists(best_model_path):\n         model_path_to_load = best_model_path\n         print(f\"Attempting to load BEST model from: {model_path_to_load}\")\n     else:\n         print(f\"Warning: Best model file '{best_model_path}' not found. Loading FINAL model instead: {model_path_to_load}\")\nelse:\n    print(f\"Attempting to load FINAL model from: {model_path_to_load}\")\n\n\nif loaded_mappings and os.path.exists(model_path_to_load):\n    print(f\"\\nLoading model structure and weights...\")\n    try:\n        # Re-initialize model structure (V5) - ensure parameters match saved model!\n        # Retrieve necessary parameters from the training setup cell (or save/load them)\n        # Using globals from Cell 9 here for simplicity, but saving/loading config is safer\n        prediction_model = ChessModelV5(\n            num_policy_classes=loaded_num_classes,\n            num_res_blocks=NUM_RES_BLOCKS,\n            num_channels=NUM_CHANNELS,\n            dropout_rate=DROPOUT_RATE # Pass dropout rate used during training\n        )\n        # Load the saved weights\n        # Need to load onto CPU first if the model was saved from DataParallel\n        state_dict = torch.load(model_path_to_load, map_location='cpu')\n        # If it was saved directly from DataParallel, it might have 'module.' prefix\n        # Create a new state dict without the prefix if necessary\n        if all(key.startswith('module.') for key in state_dict.keys()):\n             print(\"  Removing 'module.' prefix from state dict keys (saved from DataParallel).\")\n             from collections import OrderedDict\n             new_state_dict = OrderedDict()\n             for k, v in state_dict.items():\n                 name = k[7:] # remove `module.`\n                 new_state_dict[name] = v\n             state_dict = new_state_dict\n\n        prediction_model.load_state_dict(state_dict)\n        prediction_model.to(prediction_device)\n        prediction_model.eval() # Set to evaluation mode!\n        print(\"Model loaded successfully and set to evaluation mode.\")\n        model_loaded_successfully = True\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {model_path_to_load}.\")\n    except Exception as e:\n        print(f\"Error loading model state dict: {e}\")\n        print(\"Ensure the model definition (ResBlocks, Channels, Dropout etc.) in Cell 6 matches the saved model!\")\n        prediction_model = None\nelif not loaded_mappings:\n    print(\"\\nSkipping model loading as mappings were not loaded.\")\nelse:\n    print(f\"\\nSkipping model loading as model file not found: {model_path_to_load}\")\n\n\nprint(\"\\n--- Prediction Setup Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Prediction Function (V5)\n\n# %% {\"cell_type\": \"code\"}\ndef prepare_input_v4(board: Board):\n    \"\"\" Prepares a single board state for the V4/V5 model \"\"\" # Renaming not needed\n    matrix = board_to_matrix_v4(board, flip=False)\n    X_tensor = torch.tensor(matrix, dtype=torch.float32).unsqueeze(0) # Add batch dim [1, C, H, W]\n    return X_tensor\n\ndef predict_move_v5(board: Board, model_to_use, device_to_use, int_to_move_map): # Renamed to V5\n    \"\"\"\n    Predicts the best legal move using the V5 model.\n    Returns: (best_legal_move_object, predicted_value)\n    \"\"\"\n    if not model_loaded_successfully or not model_to_use or not int_to_move_map:\n        print(\"Error: Model or mappings not ready in predict_move_v5.\")\n        return None, 0.0\n\n    X_tensor = prepare_input_v4(board).to(device_to_use) # Use same prep function\n    predicted_value = 0.0\n    best_legal_move_obj = None\n    best_prob = -1.0\n\n    model_to_use.eval() # Ensure model is in eval mode\n    with torch.no_grad():\n        try:\n            policy_logits, value_output = model_to_use(X_tensor)\n            predicted_value = value_output.item()\n            policy_logits = policy_logits.squeeze(0)\n            probabilities = torch.softmax(policy_logits, dim=0).cpu().numpy()\n\n            legal_moves = list(board.legal_moves)\n            if not legal_moves: return None, predicted_value\n            legal_moves_uci_set = {move.uci() for move in legal_moves}\n\n            candidate_moves = {}\n            for i, prob in enumerate(probabilities):\n                 move_uci = int_to_move_map.get(i)\n                 if move_uci in legal_moves_uci_set:\n                     candidate_moves[move_uci] = prob\n\n            if candidate_moves:\n                 best_legal_move_uci = max(candidate_moves, key=candidate_moves.get)\n                 best_prob = candidate_moves[best_legal_move_uci]\n                 for move in legal_moves:\n                      if move.uci() == best_legal_move_uci:\n                           best_legal_move_obj = move\n                           break\n            else:\n                 print(\"Warning: Model assigned zero probability to all legal moves! Picking random.\")\n                 best_legal_move_obj = random.choice(legal_moves)\n\n        except Exception as e:\n            print(f\"Error during model prediction: {e}\")\n            legal_moves = list(board.legal_moves)\n            if legal_moves:\n                print(\"Picking random legal move due to prediction error.\")\n                best_legal_move_obj = random.choice(legal_moves)\n\n    # Final checks\n    if not best_legal_move_obj and board.is_game_over():\n        return None, predicted_value\n    elif not best_legal_move_obj:\n        print(\"Critical Warning: No move selected, but game not over. Picking random.\")\n        legal_moves = list(board.legal_moves)\n        if legal_moves: best_legal_move_obj = random.choice(legal_moves)\n\n    return best_legal_move_obj, predicted_value\n\nprint(\"Prediction function predict_move_v5 defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15: Example Prediction with SVG Output (V5)\n\n# %% {\"cell_type\": \"code\"}\nif model_loaded_successfully:\n    board = Board()\n    print(\"\\n--- Starting Prediction Example (V5) with SVG Output ---\")\n    MAX_PREDICTION_MOVES = 50 # Play N moves\n    move_counter = 0\n\n    try:\n        print(\"Initial Board:\")\n        display(SVG(board._repr_svg_()))\n\n        while move_counter < MAX_PREDICTION_MOVES:\n            move_counter += 1\n            current_player = \"White\" if board.turn else \"Black\"\n            print(f\"\\n--- Move {board.fullmove_number}. {'...' if not board.turn else ''}{current_player} to Play ---\")\n\n            if board.is_game_over(claim_draw=True):\n                print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break\n\n            ai_move, predicted_val = predict_move_v5(board, prediction_model, prediction_device, loaded_int_to_move) ### V5 Change: Call V5 function\n\n            if ai_move:\n                move_uci = ai_move.uci()\n                print(f\"AI suggests move: {move_uci} (Predicted Value: {predicted_val:.3f})\")\n                board.push(ai_move)\n                print(f\"\\nBoard after {board.fullmove_number-1 if not board.turn else board.fullmove_number}.{'..' if board.turn else ''}{move_uci}:\")\n                display(SVG(board._repr_svg_()))\n            else:\n                print(f\"AI could not suggest a valid move for {current_player}.\")\n                if board.is_game_over(claim_draw=True):\n                     print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break\n\n    except Exception as e:\n         print(f\"\\nAn unexpected error occurred during the prediction simulation: {e}\")\n         import traceback\n         traceback.print_exc()\n\n    print(\"\\n--- Prediction Example Finished ---\")\n    print(\"\\nFinal Board State:\")\n    display(SVG(board._repr_svg_()))\n    if board.is_game_over(claim_draw=True):\n         print(f\"Final Result: {board.result(claim_draw=True)}\")\n\nelse:\n    print(\"\\nPrediction model or mappings not loaded/ready. Skipping prediction example.\")\n\n\n# %% [markdown]\n# ---\n# Excellent! This V5 notebook incorporates **Dropout** and **Early Stopping** to combat overfitting, sets **Data Augmentation** as the default, and includes **Training Accuracy** in the plots.\n#\n# **Important Reminders:**\n# *   **Data Augmentation:** If you load preprocessed data created with V4 (where augmentation might have been off), you **must** set `LOAD_PREPROCESSED_DATA = False` in Cell 7 and re-run it *at least once* with `USE_AUGMENTATION = True` to benefit from the augmentation during training.\n# *   **Hyperparameters:** Remember to adjust settings in Cells 7 and 9 (paths, ELO, sample limits, batch size, model layers, epochs, dropout rate, weight decay, patience values) for your specific needs and resources.\n# *   **Tuning:** Finding the *optimal* dropout rate, weight decay, and model complexity often requires experimentation. This version provides a good starting point with regularization.\n#\n# Let me know if you run into any issues or want further refinements! Good luck with training V5! 😊","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}