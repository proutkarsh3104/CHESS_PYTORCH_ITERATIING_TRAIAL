{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11198510,"sourceType":"datasetVersion","datasetId":6991749},{"sourceId":11271131,"sourceType":"datasetVersion","datasetId":7045675}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install python-chess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:20:31.244928Z","iopub.execute_input":"2025-04-04T12:20:31.245274Z","iopub.status.idle":"2025-04-04T12:20:38.011769Z","shell.execute_reply.started":"2025-04-04T12:20:31.245247Z","shell.execute_reply":"2025-04-04T12:20:38.010945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Code combined into a single Jupyter Notebook (.ipynb) format\n# VERSION 4: Added Validation Split, Improved Graphs (Train/Val Loss & Accuracy), SVG Prediction Output, and General Polish.\n\n# %% [markdown]\n# # Advanced Chess AI V4 (Supervised Learning with Validation)\n#\n# Hey my brilliant coder! ðŸ¥° Let's push your chess AI further!\n#\n# **New in V4:**\n# *   **Train/Validation Split:** Properly separates data to monitor overfitting (Cell 5 & 7).\n# *   **Validation Loop:** Evaluates the model on unseen validation data after each epoch (Cell 8).\n# *   **Enhanced Graphs:** Plots Training *and* Validation Loss (Total, Policy, Value) and Policy Accuracy (Cell 9).\n# *   **Policy Accuracy Metric:** Tracks how often the model predicts the correct move during validation (Cell 8 & 9).\n# *   **SVG Prediction Output:** Displays the board explicitly using SVG during interactive prediction (Cell 13).\n# *   **Robust Single PGN Handling:** Optimized logic for large, single PGN files (Cell 5).\n# *   **More Comments & Prints:** Even clearer explanations and feedback.\n#\n# Ready to build an even smarter AI and watch it learn step-by-step? Let's go! â¤ï¸\n\n# %% [markdown]\n# ## Cell 1: Imports\n# Importing necessary libraries, including `matplotlib` for plots and `sklearn` for easy data splitting.\n\n# %% {\"cell_type\": \"code\"}\nimport os\nimport numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom chess import Board, pgn, Move, flip_horizontal # Board has SVG capabilities!\nfrom tqdm.notebook import tqdm # Use notebook version for better display\nimport pickle\nimport gc # Garbage collector interface\nimport matplotlib.pyplot as plt # For plotting graphs\nimport random\nfrom sklearn.model_selection import train_test_split # For easy data splitting!\nfrom IPython.display import display, SVG # To display SVG output\n\nprint(\"Imports successful!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:20:44.160122Z","iopub.execute_input":"2025-04-04T12:20:44.160419Z","iopub.status.idle":"2025-04-04T12:20:48.009924Z","shell.execute_reply.started":"2025-04-04T12:20:44.160395Z","shell.execute_reply":"2025-04-04T12:20:48.009215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm output.zip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 2: Auxiliary Functions (Board Rep, Data Gen, Move Encoding)\n# Contains helpers for board representation, the iterative data generator (now just generates all data before splitting), move encoding, and ELO parsing. Data augmentation logic is included.\n\n# %% {\"cell_type\": \"code\"}\ndef board_to_matrix_v4(board: Board, flip: bool = False):\n    \"\"\"\n    Converts a board state into a matrix representation (multi-channel).\n    Version 4: Includes piece positions, turn, castling rights, and optional horizontal flip.\n    Input features are consistent with V3.\n\n    Output shape: (18, 8, 8) - float32 numpy array\n    Channels:\n    - 0-5: White pieces (P, N, B, R, Q, K)\n    - 6-11: Black pieces (P, N, B, R, Q, K)\n    - 12: White King Castling Right (1 if True)\n    - 13: White Queen Castling Right (1 if True)\n    - 14: Black King Castling Right (1 if True)\n    - 15: Black Queen Castling Right (1 if True)\n    - 16: White's Turn (1 if White's turn)\n    - 17: Constant plane of 1s\n    \"\"\"\n    matrix = np.zeros((18, 8, 8), dtype=np.float32)\n    current_board = board.copy() # Work on a copy to avoid modifying the original board\n\n    # Apply flip transformation if requested\n    if flip:\n        # chess.flip_horizontal is a transformation constant, apply it via board.transform\n        # This flips pieces, turn, en passant square, but NOT castling rights inherently.\n        current_board = current_board.transform(flip_horizontal)\n\n    # Populate piece layers based on the (potentially flipped) board state\n    piece_map = current_board.piece_map()\n    for square, piece in piece_map.items():\n        row, col = divmod(square, 8)\n        piece_idx = piece.piece_type - 1\n        color_offset = 0 if piece.color else 6 # White=0, Black=6\n        matrix[piece_idx + color_offset, row, col] = 1\n\n    # Populate castling rights layers (use ORIGINAL board's perspective)\n    # Castling rights are relative to the initial setup, flipping the board doesn't change\n    # whether White *originally* had kingside rights, etc.\n    if board.has_kingside_castling_rights(True): matrix[12, :, :] = 1\n    if board.has_queenside_castling_rights(True): matrix[13, :, :] = 1\n    if board.has_kingside_castling_rights(False): matrix[14, :, :] = 1\n    if board.has_queenside_castling_rights(False): matrix[15, :, :] = 1\n\n    # Populate turn layer (based on the CURRENT board state after potential flip)\n    if current_board.turn: # True if White's turn in the current_board state\n        matrix[16, :, :] = 1\n\n    # Populate constant plane\n    matrix[17, :, :] = 1\n\n    return matrix\n\ndef get_value_target(game_result: str) -> float:\n    \"\"\" Assigns a numerical value based on the game result from White's perspective. \"\"\"\n    if game_result == '1-0': return 1.0\n    if game_result == '0-1': return -1.0\n    if game_result == '1/2-1/2': return 0.0\n    return 0.0 # Default for '*' or unexpected results\n\ndef parse_elo(elo_str: str) -> int:\n    \"\"\" Safely parses ELO string, returns 0 if invalid. \"\"\"\n    try:\n        return int(str(elo_str).replace('?', '').strip()) # Add str() for robustness\n    except (ValueError, AttributeError, TypeError):\n        return 0\n\ndef flip_move(move: Move) -> Move:\n    \"\"\" Flips a move horizontally. \"\"\"\n    # This logic correctly calculates the flipped squares\n    from_sq_row, from_sq_col = divmod(move.from_square, 8)\n    to_sq_row, to_sq_col = divmod(move.to_square, 8)\n\n    from_square_flipped = from_sq_row * 8 + (7 - from_sq_col)\n    to_square_flipped = to_sq_row * 8 + (7 - to_sq_col)\n\n    # Promotion piece type remains the same, color context is handled by the board state flip\n    promotion_flipped = move.promotion\n\n    return Move(from_square_flipped, to_square_flipped, promotion=promotion_flipped)\n\ndef process_games_generator_v4(pgn_filepath, min_elo=0, max_games_in_file=None, limit_total_samples=None, augment=True):\n    \"\"\"\n    Generator that processes a SINGLE PGN file iteratively with filters.\n    Yields tuples of (board_matrix, move_uci, value_target, avg_elo) for each position.\n    Handles augmentation internally if enabled.\n    \"\"\"\n    print(f\"\\nStarting V4 Generator for file: {os.path.basename(pgn_filepath)}\")\n    print(f\"  Settings: min_elo={min_elo}, max_games={max_games_in_file}, limit_samples={limit_total_samples}, augment={augment}\")\n    total_samples_yielded = 0\n    processed_games_count = 0\n    accepted_games_count = 0\n    elo_list_accepted = []\n\n    try:\n        with open(pgn_filepath, 'r', encoding='utf-8', errors='ignore') as pgn_file:\n            while True:\n                # Check total sample limit\n                if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                    print(f\"\\nGenerator hit total sample limit ({limit_total_samples}). Stopping.\")\n                    return elo_list_accepted # Return collected ELOs\n\n                # Check game limit for this file\n                if max_games_in_file is not None and processed_games_count >= max_games_in_file:\n                    print(f\"Generator hit game limit ({max_games_in_file}) for this file. Stopping.\")\n                    break # Stop processing this file\n\n                game_headers = None\n                current_game_start_pos = pgn_file.tell() # Remember position before reading game\n\n                try:\n                    # Read headers first for filtering\n                    game_headers = pgn.read_headers(pgn_file)\n                    if game_headers is None:\n                        # print(\"End of file reached.\")\n                        break # Normal end of file\n\n                    processed_games_count += 1\n\n                    # --- ELO Filtering ---\n                    white_elo = parse_elo(game_headers.get(\"WhiteElo\", \"0\"))\n                    black_elo = parse_elo(game_headers.get(\"BlackElo\", \"0\"))\n                    avg_elo = (white_elo + black_elo) / 2 if white_elo > 0 and black_elo > 0 else 0\n\n                    # Skip if BOTH players are below threshold (can adjust logic if needed)\n                    if white_elo < min_elo and black_elo < min_elo:\n                        # Skip the game body - need to reposition file pointer or use read_game carefully\n                        # Simplest: Try to read the game fully to advance the pointer past it.\n                        try:\n                             pgn.read_game(pgn_file) # Read and discard\n                        except Exception:\n                             # If reading the skipped game fails, we might be stuck.\n                             # It's safer to parse fully then filter, but less memory efficient.\n                             # For now, we assume read_game handles advancing correctly.\n                             pass\n                        continue # Skip to next game header\n\n                    # If ELO is okay, read the full game\n                    # Reset pointer to read the full game including headers\n                    pgn_file.seek(current_game_start_pos)\n                    game = pgn.read_game(pgn_file)\n                    if game is None:\n                         print(f\"Warning: Could not read game body after reading headers for game {processed_games_count}. Skipping.\")\n                         continue # Hope the pointer advanced correctly\n\n                except EOFError:\n                     print(\"EOFError encountered, likely end of file.\")\n                     break\n                except Exception as e:\n                    print(f\"Skipping game {processed_games_count} due to header/initial parsing error: {e}\")\n                    # Attempt to recover by finding the next likely start of a game (heuristic)\n                    try:\n                        while True:\n                            line = pgn_file.readline()\n                            if not line: break # End of file\n                            if line.startswith('[Event '): # Found next likely game\n                                pgn_file.seek(pgn_file.tell() - len(line.encode('utf-8'))) # Rewind to start of line\n                                break\n                    except Exception as re:\n                         print(f\"Recovery attempt failed: {re}. Stopping processing for this file.\")\n                         break\n                    continue # Continue to next header check\n\n                # --- Game Processing (If Passed Filters) ---\n                accepted_games_count += 1\n                if avg_elo > 0:\n                    elo_list_accepted.append(avg_elo)\n\n                result = game.headers.get(\"Result\", \"*\")\n                value_target = get_value_target(result)\n                board = game.board()\n\n                try:\n                    move_count_in_game = 0\n                    for move in game.mainline_moves():\n                        move_count_in_game += 1\n                        # Check sample limit again before yielding\n                        if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                             print(f\"\\nGenerator hit total sample limit ({limit_total_samples}) mid-game. Stopping.\")\n                             return elo_list_accepted\n\n                        # --- Original Sample ---\n                        matrix = board_to_matrix_v4(board, flip=False)\n                        move_uci = move.uci()\n                        yield matrix, move_uci, value_target, avg_elo\n                        total_samples_yielded += 1\n\n                        # --- Augmented (Flipped) Sample ---\n                        if augment:\n                            if limit_total_samples is not None and total_samples_yielded >= limit_total_samples:\n                                 print(f\"\\nGenerator hit total sample limit ({limit_total_samples}) before augment. Stopping.\")\n                                 return elo_list_accepted\n\n                            matrix_flipped = board_to_matrix_v4(board, flip=True)\n                            move_flipped = flip_move(move)\n                            move_uci_flipped = move_flipped.uci()\n                            # Value target remains the same (always from original White's perspective)\n                            yield matrix_flipped, move_uci_flipped, value_target, avg_elo\n                            total_samples_yielded += 1\n\n                        # Make the move on the board for the next iteration\n                        board.push(move)\n\n                except (ValueError, AssertionError, AttributeError) as e:\n                    print(f\"Warning: Skipping moves in game {accepted_games_count} due to error: {e}. Game had {move_count_in_game} moves processed.\")\n                    # Continue to the next game instead of stopping the whole file\n                    break # Stop processing this game's moves\n\n    except FileNotFoundError:\n        print(f\"Error: PGN file not found at {pgn_filepath}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred while processing {os.path.basename(pgn_filepath)}: {e}\")\n\n    print(f\"\\nFinished generator for file {os.path.basename(pgn_filepath)}.\")\n    print(f\"  Games Scanned in file: {processed_games_count}\")\n    print(f\"  Games Accepted in file: {accepted_games_count}\")\n    print(f\"  Samples Yielded from file: (tracked globally)\")\n    return elo_list_accepted # Return ELOs collected from this file\n\ndef encode_moves_v4(moves_uci_list):\n    \"\"\" Encodes a list of UCI move strings into integer labels. Returns all mappings. \"\"\"\n    print(f\"\\nEncoding {len(moves_uci_list)} policy targets...\")\n    unique_moves = sorted(list(set(moves_uci_list))) # Sort for consistent mapping\n    move_to_int = {move: idx for idx, move in enumerate(unique_moves)}\n    int_to_move = {idx: move for move, idx in move_to_int.items()} # Inverse mapping\n    num_classes = len(unique_moves)\n    # Use int64 for PyTorch CrossEntropyLoss compatibility\n    encoded_y = np.array([move_to_int[move] for move in moves_uci_list], dtype=np.int64)\n    print(f\"Found {num_classes} unique moves across all samples.\")\n    return encoded_y, move_to_int, int_to_move, num_classes\n\nprint(\"Auxiliary functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:21:00.857287Z","iopub.execute_input":"2025-04-04T12:21:00.857712Z","iopub.status.idle":"2025-04-04T12:21:00.879343Z","shell.execute_reply.started":"2025-04-04T12:21:00.857685Z","shell.execute_reply":"2025-04-04T12:21:00.878504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 3: Dataset Class\n# Standard PyTorch Dataset class. It simply holds the prepared data (split into train/val later).\n\n# %% {\"cell_type\": \"code\"}\nclass ChessDatasetV4(Dataset):\n    \"\"\" Basic PyTorch Dataset for Chess Data \"\"\"\n    def __init__(self, X_matrices, y_policy_encoded, y_value_targets, dataset_name=\"\"):\n        self.X = X_matrices\n        self.y_policy = y_policy_encoded\n        self.y_value = y_value_targets\n        self.name = dataset_name\n        print(f\"\\nChessDatasetV4 ({self.name}) initialized:\")\n        if len(self.X) > 0:\n            print(f\"  Number of samples: {len(self.X)}\")\n            # Check type and shape of the first element for confirmation\n            print(f\"  Sample X type: {type(self.X[0])}, shape: {self.X[0].shape if isinstance(self.X[0], np.ndarray) else 'N/A'}\")\n            print(f\"  Sample y_policy type: {type(self.y_policy[0])}, value: {self.y_policy[0] if len(self.y_policy) > 0 else 'N/A'}\")\n            print(f\"  Sample y_value type: {type(self.y_value[0])}, value: {self.y_value[0] if len(self.y_value) > 0 else 'N/A'}\")\n        else:\n            print(f\"  Dataset ({self.name}) is empty.\")\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Returns data as tensors for the DataLoader\n        # Ensure value target is float32, policy target is long\n        return (torch.tensor(self.X[idx], dtype=torch.float32),\n                torch.tensor(self.y_policy[idx], dtype=torch.long),\n                torch.tensor(self.y_value[idx], dtype=torch.float32))\n\nprint(\"ChessDatasetV4 class defined.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:21:06.934541Z","iopub.execute_input":"2025-04-04T12:21:06.934843Z","iopub.status.idle":"2025-04-04T12:21:06.941824Z","shell.execute_reply.started":"2025-04-04T12:21:06.934819Z","shell.execute_reply":"2025-04-04T12:21:06.940848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 4: Model Definition (Dual Headed ResNet)\n# Using the ResNet-style architecture (ChessModelV3 renamed to V4). Includes Residual Blocks for deeper learning, inspired by AlphaZero/Leela.\n\n# %% {\"cell_type\": \"code\"}\nclass ResidualBlock(nn.Module):\n    \"\"\" Standard Residual Block with Conv -> BN -> ReLU -> Conv -> BN -> Add -> ReLU \"\"\"\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.relu = nn.ReLU(inplace=True) # Use inplace for slight memory saving\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        residual = x\n        # First layer\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        # Second layer\n        out = self.conv2(out)\n        out = self.bn2(out)\n        # Add skip connection\n        out += residual\n        # Final ReLU activation\n        out = self.relu(out)\n        return out\n\nclass ChessModelV4(nn.Module):\n    \"\"\"\n    Dual-headed ResNet-style model for chess.\n    Takes (N, 18, 8, 8) input tensor.\n    Outputs policy logits (N, num_moves) and value prediction (N, 1).\n    \"\"\"\n    def __init__(self, num_policy_classes, num_res_blocks=5, num_channels=128):\n        super(ChessModelV4, self).__init__()\n        input_channels = 18 # From board_to_matrix_v4\n\n        # --- Input Convolution Block ---\n        # Increase kernel size? Maybe 5x5? Or stick to 3x3? Let's try 3x3 first.\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        # --- Residual Blocks Tower ---\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_channels) for _ in range(num_res_blocks)]\n        )\n\n        # --- Policy Head ---\n        self.policy_conv = nn.Conv2d(num_channels, 32, kernel_size=1, bias=False) # 1x1 conv to reduce channels\n        self.policy_bn = nn.BatchNorm2d(32)\n        self.policy_relu = nn.ReLU(inplace=True)\n        self.policy_flatten = nn.Flatten()\n        # Dynamically calculate flattened size for policy head\n        policy_flat_size = self._get_flat_size(lambda x: self.policy_relu(self.policy_bn(self.policy_conv(x))), input_channels, num_channels)\n        self.policy_fc = nn.Linear(policy_flat_size, num_policy_classes)\n\n        # --- Value Head ---\n        self.value_conv = nn.Conv2d(num_channels, 16, kernel_size=1, bias=False) # 1x1 conv, different channel reduction\n        self.value_bn = nn.BatchNorm2d(16)\n        self.value_relu = nn.ReLU(inplace=True)\n        self.value_flatten = nn.Flatten()\n        # Dynamically calculate flattened size for value head\n        value_flat_size = self._get_flat_size(lambda x: self.value_relu(self.value_bn(self.value_conv(x))), input_channels, num_channels)\n        self.value_fc1 = nn.Linear(value_flat_size, 64) # Intermediate dense layer\n        self.value_relu2 = nn.ReLU(inplace=True)\n        self.value_fc2 = nn.Linear(64, 1) # Single output neuron\n        self.value_tanh = nn.Tanh() # Squash output to [-1, 1]\n\n        print(f\"\\nChessModelV4 initialized:\")\n        print(f\"  Input Channels: {input_channels}\")\n        print(f\"  Residual Blocks: {num_res_blocks} x {num_channels} channels\")\n        print(f\"  Policy Head Flattened Size: {policy_flat_size}\")\n        print(f\"  Value Head Flattened Size: {value_flat_size}\")\n        print(f\"  Output Policy Classes: {num_policy_classes}\")\n\n        self._initialize_weights() # Apply weight initialization\n\n    def _get_flat_size(self, head_conv_lambda, input_channels, num_body_channels):\n        \"\"\" Helper function to calculate flattened size after conv blocks. \"\"\"\n        with torch.no_grad():\n            # Create a dummy input matching the expected dimensions\n            dummy_input = torch.zeros(1, input_channels, 8, 8)\n            # Pass through the initial conv block\n            dummy_body_features = self.conv_in(dummy_input)\n            # Pass through the residual blocks\n            dummy_res_features = self.res_blocks(dummy_body_features)\n            # Pass through the specific head's convolutional part\n            dummy_head_features = head_conv_lambda(dummy_res_features)\n            # Calculate the flattened size\n            flat_size = dummy_head_features.view(1, -1).size(1)\n        return flat_size\n\n    def _initialize_weights(self):\n        \"\"\" Initializes weights using common practices (Kaiming for Conv, Xavier for Linear). \"\"\"\n        print(\"Initializing model weights...\")\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                # print(f\"  Initialized Conv2d: {m}\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n                # print(f\"  Initialized BatchNorm2d: {m}\")\n            elif isinstance(m, nn.Linear):\n                 nn.init.xavier_uniform_(m.weight)\n                 if m.bias is not None:\n                     nn.init.constant_(m.bias, 0)\n                 # print(f\"  Initialized Linear: {m}\")\n        print(\"Weight initialization complete.\")\n\n    def forward(self, x):\n        # Input: (N, 18, 8, 8)\n        # Pass through initial convolution block\n        features = self.conv_in(x)\n        # Pass through the stack of residual blocks\n        features = self.res_blocks(features)\n\n        # --- Policy Head Forward ---\n        policy = self.policy_conv(features)\n        policy = self.policy_bn(policy)\n        policy = self.policy_relu(policy)\n        policy = self.policy_flatten(policy)\n        policy_logits = self.policy_fc(policy) # Output: (N, num_policy_classes)\n\n        # --- Value Head Forward ---\n        value = self.value_conv(features)\n        value = self.value_bn(value)\n        value = self.value_relu(value)\n        value = self.value_flatten(value)\n        value = self.value_fc1(value)\n        value = self.value_relu2(value)\n        value = self.value_fc2(value)\n        value_output = self.value_tanh(value) # Output: (N, 1) squeezed to [-1, 1]\n\n        return policy_logits, value_output\n\nprint(\"ChessModelV4 (ResNet style) class defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:21:13.314129Z","iopub.execute_input":"2025-04-04T12:21:13.314443Z","iopub.status.idle":"2025-04-04T12:21:13.328903Z","shell.execute_reply.started":"2025-04-04T12:21:13.314417Z","shell.execute_reply":"2025-04-04T12:21:13.328089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 5: Data Loading, Processing, and Splitting\n# Configure paths and filters. This cell loads data iteratively, collects it, performs the train/validation split, and prepares NumPy arrays. **This is memory-intensive during collection and splitting.**\n\n# %% {\"cell_type\": \"code\"}\n# --- Configuration ---\n# Option 1: Single Large PGN File\nPGN_FILE_PATH = None # ADJUST THIS! (e.g., your Lichess file)\nPGN_DIR = \"/kaggle/input/lichess-high-quality-games/Lichess Elite Database\" # Set to None if using PGN_FILE_PATH\n\n# Option 2: Directory of PGN Files\n# PGN_FILE_PATH = None\n# PGN_DIR = \"/kaggle/input/your-pgn-directory\" # ADJUST THIS!\n\n# Output paths (must be in /kaggle/working/ for Kaggle notebooks)\nMODEL_SAVE_PATH = \"/kaggle/working/chess_model_v4_resnet_val.pth\"\nMAPPING_SAVE_PATH = \"/kaggle/working/chess_mappings_v4.pkl\"\n\n# --- Data Filtering & Sampling ---\nMIN_ELO = 2400 # Minimum ELO for BOTH players (use high quality games)\nMAX_GAMES_PER_FILE = 600 # Limit games read from the single PGN file (adjust if needed)\nTOTAL_SAMPLE_LIMIT = 2_500_000 # Max board positions (samples) to load *before* splitting\nUSE_AUGMENTATION = True # Use horizontal flipping?\nVALIDATION_SET_SIZE = 0.15 # Use 15% of the loaded data for validation\n\n# --- Determine PGN File(s) to Process ---\npgn_files_to_process = []\nactual_pgn_dir_or_file = None # Store the path for the generator\n\nif PGN_FILE_PATH and os.path.isfile(PGN_FILE_PATH):\n    pgn_files_to_process = [PGN_FILE_PATH] # Pass the full path directly\n    print(f\"Processing single PGN file: {PGN_FILE_PATH}\")\nelif PGN_DIR and os.path.isdir(PGN_DIR):\n    files_in_dir = sorted([os.path.join(PGN_DIR, f) for f in os.listdir(PGN_DIR) if f.endswith(\".pgn\")])\n    pgn_files_to_process = files_in_dir\n    print(f\"Found {len(pgn_files_to_process)} PGN files in directory: {PGN_DIR}\")\nelse:\n    print(f\"Error: No valid PGN file or directory specified. Check PGN_FILE_PATH or PGN_DIR.\")\n    pgn_files_to_process = []\n\n# --- Generate and Collect Data ---\n# Initialize lists to store all data before splitting\nX_all_list = []\ny_policy_uci_all_list = []\ny_value_all_list = []\nelo_all_list = []\n\nif pgn_files_to_process:\n    print(f\"\\n--- Starting Data Generation Phase ---\")\n    global_samples_yielded = 0\n    generator_needs_break = False\n\n    for pgn_path in pgn_files_to_process: # Loop in case multiple files were found in a dir\n        if generator_needs_break: break\n\n        remaining_samples = TOTAL_SAMPLE_LIMIT - global_samples_yielded if TOTAL_SAMPLE_LIMIT else None\n\n        # Create generator for the current file\n        data_generator = process_games_generator_v4(\n            pgn_path,\n            min_elo=MIN_ELO,\n            max_games_in_file=MAX_GAMES_PER_FILE,\n            limit_total_samples=remaining_samples, # Pass remaining limit\n            augment=USE_AUGMENTATION\n        )\n\n        # Collect data from the generator\n        try:\n            file_samples = 0\n            # The generator now returns the ELO list when exhausted or stopped\n            # We need to iterate through its yields\n            for matrix, move_uci, value, avg_elo in tqdm(data_generator, desc=f\"Processing {os.path.basename(pgn_path)}\", leave=False):\n                 X_all_list.append(matrix)\n                 y_policy_uci_all_list.append(move_uci)\n                 y_value_all_list.append(value)\n                 if avg_elo > 0:\n                     elo_all_list.append(avg_elo)\n                 global_samples_yielded += 1\n                 file_samples += 1\n                 # Check global limit again after yield\n                 if TOTAL_SAMPLE_LIMIT is not None and global_samples_yielded >= TOTAL_SAMPLE_LIMIT:\n                      print(f\"\\nGlobal sample limit {TOTAL_SAMPLE_LIMIT} reached. Stopping collection.\")\n                      generator_needs_break = True\n                      break # Stop iterating this generator\n\n            print(f\"  Finished processing {os.path.basename(pgn_path)}, added {file_samples} samples.\")\n\n        except Exception as e:\n            print(f\"An error occurred during sample generation for {os.path.basename(pgn_path)}: {e}\")\n\n    print(f\"\\n--- Data Generation Complete ---\")\n    print(f\"Collected {len(X_all_list)} total samples (including augmentation if used).\")\n    print(f\"Collected ELOs from {len(elo_all_list)} positions (where valid ELO was present).\")\n\n    # --- Encode Moves ---\n    if not X_all_list:\n         print(\"Error: No samples collected. Check PGN paths, filtering settings, and generator logic.\")\n         # Set flag or raise error to prevent proceeding\n         can_proceed = False\n    else:\n        y_policy_encoded_all, move_to_int, int_to_move, num_classes = encode_moves_v4(y_policy_uci_all_list)\n        can_proceed = True\n\n    # --- Train/Validation Split ---\n    if can_proceed:\n        print(f\"\\nSplitting data into Train ({1-VALIDATION_SET_SIZE:.0%}) / Validation ({VALIDATION_SET_SIZE:.0%})...\")\n        if len(X_all_list) < 2:\n             print(\"Error: Not enough samples to perform train/validation split.\")\n             can_proceed = False\n        else:\n            indices = list(range(len(X_all_list)))\n            try:\n                train_indices, val_indices = train_test_split(\n                    indices,\n                    test_size=VALIDATION_SET_SIZE,\n                    random_state=42, # for reproducible splits\n                    # Stratify might be useful if value targets were classes, less so here\n                    # stratify=y_value_list if you want to try stratifying by value (might need binning)\n                )\n\n                print(f\"  Train set size: {len(train_indices)}\")\n                print(f\"  Validation set size: {len(val_indices)}\")\n\n                # --- Create NumPy arrays for Train and Validation sets ---\n                # This is the most memory-intensive part after collection\n                print(\"\\nCreating NumPy arrays for training set...\")\n                X_train_np = np.array([X_all_list[i] for i in train_indices], dtype=np.float32)\n                y_policy_train_np = np.array([y_policy_encoded_all[i] for i in train_indices], dtype=np.int64)\n                y_value_train_np = np.array([y_value_all_list[i] for i in train_indices], dtype=np.float32)\n                print(\"  Training arrays created.\")\n\n                print(\"Creating NumPy arrays for validation set...\")\n                X_val_np = np.array([X_all_list[i] for i in val_indices], dtype=np.float32)\n                y_policy_val_np = np.array([y_policy_encoded_all[i] for i in val_indices], dtype=np.int64)\n                y_value_val_np = np.array([y_value_all_list[i] for i in val_indices], dtype=np.float32)\n                print(\"  Validation arrays created.\")\n\n                # --- Memory Cleanup ---\n                print(\"\\nCleaning up original large lists and encoded array...\")\n                del X_all_list\n                del y_policy_uci_all_list\n                del y_value_all_list\n                del y_policy_encoded_all\n                del train_indices\n                del val_indices\n                del indices\n                gc.collect() # Force garbage collection immediately\n                print(\"Cleanup complete.\")\n                data_ready = True\n\n            except Exception as e:\n                 print(f\"Error during train/validation split or NumPy conversion: {e}\")\n                 data_ready = False\n\nelse:\n    print(\"Skipping data generation and processing as no valid PGN source was found.\")\n    data_ready = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 5: Data Loading, Processing, and Splitting\n# Configure paths and filters. This cell loads data iteratively, collects it, performs the train/validation split, and prepares NumPy arrays. **This is memory-intensive during collection and splitting.**\n\n# %% {\"cell_type\": \"code\"}\n# --- Configuration ---\nLOAD_PREPROCESSED_DATA = True  # <<< SET TO False for the first run, True for subsequent runs\nPREPROCESSED_DATA_DIR = \"/kaggle/input/pgnfile/processed_data/\" # Directory to save/load data\nPREPROCESSED_NPZ_PATH = os.path.join(PREPROCESSED_DATA_DIR, \"chess_data_split.npz\")\nPREPROCESSED_MAPPINGS_PATH = os.path.join(PREPROCESSED_DATA_DIR, \"chess_mappings_processed.pkl\")\n# Option 1: Single Large PGN File\nPGN_FILE_PATH = None # ADJUST THIS! (e.g., your Lichess file)\nPGN_DIR = \"/kaggle/input/lichess-high-quality-games/Lichess Elite Database\" # Set to None if using PGN_FILE_PATH\n\n# Option 2: Directory of PGN Files\n# PGN_FILE_PATH = None\n# PGN_DIR = \"/kaggle/input/your-pgn-directory\" # ADJUST THIS!\n\n# Output paths (must be in /kaggle/working/ for Kaggle notebooks)\nMODEL_SAVE_PATH = \"/kaggle/working/chess_model_v4_resnet_val.pth\"\nMAPPING_SAVE_PATH = \"/kaggle/working/chess_mappings_v4.pkl\"\n\n# --- Data Filtering & Sampling ---\nMIN_ELO = 2400 # Minimum ELO for BOTH players (use high quality games)\nMAX_GAMES_PER_FILE = 1000 # Limit games read from the single PGN file (adjust if needed)\nTOTAL_SAMPLE_LIMIT = 2_500_000 # Max board positions (samples) to load *before* splitting\nUSE_AUGMENTATION = False # Use horizontal flipping?\nVALIDATION_SET_SIZE = 0.15 # Use 15% of the loaded data for validation\n\n# --- Determine PGN File(s) to Process ---\n# --- Initialize variables ---\ndata_ready = False\nX_train_np, y_policy_train_np, y_value_train_np = None, None, None\nX_val_np, y_policy_val_np, y_value_val_np = None, None, None\nmove_to_int, int_to_move, num_classes = None, None, None\nelo_all_list = [] # Keep for EDA if reprocessing\n\n# --- Create directory if it doesn't exist ---\nos.makedirs(PREPROCESSED_DATA_DIR, exist_ok=True)\n\n# --- Attempt to Load Preprocessed Data ---\nif LOAD_PREPROCESSED_DATA and os.path.exists(PREPROCESSED_NPZ_PATH) and os.path.exists(PREPROCESSED_MAPPINGS_PATH):\n    print(f\"--- Loading Preprocessed Data ---\")\n    try:\n        print(f\"  Loading arrays from: {PREPROCESSED_NPZ_PATH}\")\n        with np.load(PREPROCESSED_NPZ_PATH) as data:\n            X_train_np = data['X_train']\n            y_policy_train_np = data['y_policy_train']\n            y_value_train_np = data['y_value_train']\n            X_val_np = data['X_val']\n            y_policy_val_np = data['y_policy_val']\n            y_value_val_np = data['y_value_val']\n        print(\"    NumPy arrays loaded successfully.\")\n\n        print(f\"  Loading mappings from: {PREPROCESSED_MAPPINGS_PATH}\")\n        with open(PREPROCESSED_MAPPINGS_PATH, \"rb\") as f:\n            loaded_mappings = pickle.load(f)\n        move_to_int = loaded_mappings['move_to_int']\n        int_to_move = loaded_mappings['int_to_move']\n        num_classes = loaded_mappings['num_classes']\n        print(\"    Mappings loaded successfully.\")\n        print(f\"    Train samples: {len(X_train_np)}, Val samples: {len(X_val_np)}, Num Classes: {num_classes}\")\n\n        data_ready = True # Flag that data is ready for Cell 7/8\n\n    except Exception as e:\n        print(f\"  Error loading preprocessed data: {e}. Will reprocess from PGNs.\")\n        # Clear potentially partially loaded variables\n        X_train_np, y_policy_train_np, y_value_train_np = None, None, None\n        X_val_np, y_policy_val_np, y_value_val_np = None, None, None\n        move_to_int, int_to_move, num_classes = None, None, None\n        data_ready = False\n\n# --- Reprocess Data from PGNs if Loading Failed or Disabled ---\nif not data_ready:\n    print(\"\\n--- Processing Data from PGN Files ---\")\n\n    # --- Determine PGN File(s) to Process --- (Keep your existing logic here)\n    pgn_files_to_process = []\n    # ... your code to populate pgn_files_to_process ...\n    if PGN_FILE_PATH and os.path.isfile(PGN_FILE_PATH):\n        pgn_files_to_process = [PGN_FILE_PATH] # Pass the full path directly\n        print(f\"Processing single PGN file: {PGN_FILE_PATH}\")\n    elif PGN_DIR and os.path.isdir(PGN_DIR):\n        files_in_dir = sorted([os.path.join(PGN_DIR, f) for f in os.listdir(PGN_DIR) if f.endswith(\".pgn\")])\n        pgn_files_to_process = files_in_dir\n        print(f\"Found {len(pgn_files_to_process)} PGN files in directory: {PGN_DIR}\")\n    else:\n        print(f\"Error: No valid PGN file or directory specified. Check PGN_FILE_PATH or PGN_DIR.\")\n        pgn_files_to_process = []\n\n    # --- Generate and Collect Data --- (Keep your existing logic here)\n    X_all_list = []\n    y_policy_uci_all_list = []\n    y_value_all_list = []\n    elo_all_list = [] # Reset elo list for reprocessing\n\n    if pgn_files_to_process:\n        print(f\"\\n--- Starting Data Generation Phase ---\")\n        # ... (Your entire existing loop using process_games_generator_v4 and tqdm) ...\n        # !! IMPORTANT: Make sure this loop correctly populates the lists !!\n        # Example snippet from your original code:\n        global_samples_yielded = 0\n        generator_needs_break = False\n        for pgn_path in pgn_files_to_process:\n            if generator_needs_break: break\n            remaining_samples = TOTAL_SAMPLE_LIMIT - global_samples_yielded if TOTAL_SAMPLE_LIMIT else None\n            data_generator = process_games_generator_v4(\n                pgn_path, min_elo=MIN_ELO, max_games_in_file=MAX_GAMES_PER_FILE,\n                limit_total_samples=remaining_samples, augment=USE_AUGMENTATION\n            )\n            try:\n                file_samples = 0\n                for matrix, move_uci, value, avg_elo in tqdm(data_generator, desc=f\"Processing {os.path.basename(pgn_path)}\", leave=False):\n                     X_all_list.append(matrix)\n                     y_policy_uci_all_list.append(move_uci)\n                     y_value_all_list.append(value)\n                     if avg_elo > 0: elo_all_list.append(avg_elo) # For EDA\n                     global_samples_yielded += 1\n                     file_samples += 1\n                     if TOTAL_SAMPLE_LIMIT is not None and global_samples_yielded >= TOTAL_SAMPLE_LIMIT:\n                          print(f\"\\nGlobal sample limit {TOTAL_SAMPLE_LIMIT} reached.\")\n                          generator_needs_break = True\n                          break\n                print(f\"  Finished {os.path.basename(pgn_path)}, added {file_samples} samples.\")\n            except Exception as e:\n                print(f\"Error during generation for {os.path.basename(pgn_path)}: {e}\")\n\n        print(f\"\\n--- Data Generation Complete ---\")\n        print(f\"Collected {len(X_all_list)} total samples.\")\n\n        # --- Encode Moves --- (Keep your existing logic)\n        if not X_all_list:\n            print(\"Error: No samples collected.\")\n            can_proceed = False\n        else:\n            y_policy_encoded_all, move_to_int, int_to_move, num_classes = encode_moves_v4(y_policy_uci_all_list)\n            can_proceed = True\n\n        # --- Train/Validation Split --- (Keep your existing logic)\n        if can_proceed:\n            print(f\"\\nSplitting data...\")\n            # ... (Your train_test_split logic) ...\n            train_indices, val_indices = train_test_split(\n                list(range(len(X_all_list))), test_size=VALIDATION_SET_SIZE, random_state=42\n            )\n            print(f\"  Train set size: {len(train_indices)}\")\n            print(f\"  Validation set size: {len(val_indices)}\")\n\n            # --- Create NumPy arrays --- (Keep your existing logic)\n            print(\"\\nCreating NumPy arrays...\")\n            X_train_np = np.array([X_all_list[i] for i in train_indices], dtype=np.float32)\n            y_policy_train_np = np.array([y_policy_encoded_all[i] for i in train_indices], dtype=np.int64)\n            y_value_train_np = np.array([y_value_all_list[i] for i in train_indices], dtype=np.float32)\n            X_val_np = np.array([X_all_list[i] for i in val_indices], dtype=np.float32)\n            y_policy_val_np = np.array([y_policy_encoded_all[i] for i in val_indices], dtype=np.int64)\n            y_value_val_np = np.array([y_value_all_list[i] for i in val_indices], dtype=np.float32)\n            print(\"  Arrays created.\")\n\n            # --- ADD THIS SAVING BLOCK ---\n            print(f\"--- Saving Preprocessed Data ---\")\n            try:\n                print(f\"  Saving arrays to: {PREPROCESSED_NPZ_PATH}\")\n                np.savez_compressed(PREPROCESSED_NPZ_PATH,\n                                    X_train=X_train_np, y_policy_train=y_policy_train_np, y_value_train=y_value_train_np,\n                                    X_val=X_val_np, y_policy_val=y_policy_val_np, y_value_val=y_value_val_np)\n                print(\"    NumPy arrays saved successfully.\")\n\n                mappings_to_save = {\n                    'move_to_int': move_to_int,\n                    'int_to_move': int_to_move,\n                    'num_classes': num_classes\n                }\n                print(f\"  Saving mappings to: {PREPROCESSED_MAPPINGS_PATH}\")\n                with open(PREPROCESSED_MAPPINGS_PATH, \"wb\") as f:\n                    pickle.dump(mappings_to_save, f)\n                print(\"    Mappings saved successfully.\")\n                print(f\"    --> Set LOAD_PREPROCESSED_DATA = True at the top of this cell to load next time <--\")\n\n            except Exception as e:\n                print(f\"  Error saving preprocessed data: {e}\")\n            # --- END OF SAVING BLOCK ---\n\n            # --- Memory Cleanup --- (Keep your existing logic)\n            print(\"\\nCleaning up original large lists and encoded array...\")\n            del X_all_list\n            del y_policy_uci_all_list\n            del y_value_all_list\n            del y_policy_encoded_all\n            del train_indices\n            del val_indices\n            # del indices # Only if you defined it explicitly\n            gc.collect()\n            print(\"Cleanup complete.\")\n            data_ready = True # Data is ready after processing\n\n        else: # if not can_proceed\n             data_ready = False\n    else: # if not pgn_files_to_process\n        print(\"Skipping data generation as no valid PGN source was found.\")\n        data_ready = False\n\n# --- Final Check ---\nif not data_ready:\n    print(\"\\nERROR: Data preparation failed. Cannot proceed to training setup.\")\nelse:\n    print(\"\\n--- Data is ready for Training Setup (Next Cell) ---\")\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:21:44.792077Z","iopub.execute_input":"2025-04-04T12:21:44.792375Z","iopub.status.idle":"2025-04-04T12:22:11.481446Z","shell.execute_reply.started":"2025-04-04T12:21:44.792350Z","shell.execute_reply":"2025-04-04T12:22:11.480795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the file or folder\nshutil.make_archive('/kaggle/working/processed_data', 'zip', '/kaggle/working/processed_data')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink('/kaggle/working/processed_data.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r output.zip /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:10:15.646281Z","iopub.execute_input":"2025-04-04T14:10:15.646596Z","iopub.status.idle":"2025-04-04T14:10:21.271476Z","shell.execute_reply.started":"2025-04-04T14:10:15.646572Z","shell.execute_reply":"2025-04-04T14:10:21.270435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 6: Basic EDA - ELO Distribution\n# Visualize the average ELO distribution from the games we collected data from.\n\n# %% {\"cell_type\": \"code\"}\nif 'elo_all_list' in locals() and elo_all_list:\n    print(f\"\\n--- Basic EDA: ELO Distribution of Accepted Games ({len(elo_all_list)} positions/samples) ---\")\n    # Note: elo_all_list contains ELO for each *sample*, not unique games. Distribution shape is still informative.\n    plt.figure(figsize=(10, 5))\n    plt.hist(elo_all_list, bins=60, color='lightcoral', edgecolor='black')\n    plt.title(f'Distribution of Average ELOs in Collected Samples (Min Player ELO Filter: {MIN_ELO})')\n    plt.xlabel('Average ELO of Game')\n    plt.ylabel('Number of Samples (Board Positions)')\n    plt.grid(axis='y', alpha=0.75)\n    plt.show()\n\n    avg_elo_loaded = np.mean(elo_all_list)\n    median_elo_loaded = np.median(elo_all_list)\n    print(f\"  Average ELO of samples: {avg_elo_loaded:.0f}\")\n    print(f\"  Median ELO of samples: {median_elo_loaded:.0f}\")\nelse:\n    print(\"\\nNo ELO data collected for EDA (check filters or generation process).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:22:59.370200Z","iopub.execute_input":"2025-04-04T12:22:59.370534Z","iopub.status.idle":"2025-04-04T12:22:59.376431Z","shell.execute_reply.started":"2025-04-04T12:22:59.370506Z","shell.execute_reply":"2025-04-04T12:22:59.375463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ## Cell 7: Training Setup (Device, Datasets, Model, Optimizer)\n# # Prepare device, create Train/Validation Datasets and DataLoaders, initialize the model, loss functions, and optimizer.\n\n# # %% {\"cell_type\": \"code\"}\n# # --- Configuration ---\n# BATCH_SIZE = 512 # Increase if VRAM allows (e.g., 256, 512, 1024)\n# LEARNING_RATE = 0.001 # Adam initial learning rate\n# NUM_EPOCHS = 20 # Adjust based on convergence and time limits\n# POLICY_LOSS_WEIGHT = 1.0\n# VALUE_LOSS_WEIGHT = 0.8 # Slightly de-weight value loss? Or keep 1.0. Let's try 0.8.\n# NUM_RES_BLOCKS = 9 # Number of residual blocks (e.g., 5-19)\n# NUM_CHANNELS = 128 # Number of conv channels (e.g., 128, 256)\n# WEIGHT_DECAY = 1e-5 # L2 Regularization for Adam\n\n# # --- Setup Device ---\n# if torch.cuda.is_available():\n#     device = torch.device(\"cuda\")\n#     print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n#     torch.cuda.empty_cache()\n#     print(f\"  CUDA Memory Allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n#     print(f\"  CUDA Memory Reserved:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n# else:\n#     device = torch.device(\"cpu\")\n#     print(\"Using CPU (Training will be significantly slower!)\")\n\n# # --- Create Datasets and DataLoaders ---\n# train_dataset = None\n# val_dataset = None\n# train_dataloader = None\n# val_dataloader = None\n# model = None # Initialize to None\n\n# if data_ready: # Check if data splitting and NumPy conversion succeeded\n#     print(\"\\nCreating Datasets and DataLoaders...\")\n#     try:\n#         # Create Train Dataset and DataLoader\n#         train_dataset = ChessDatasetV4(X_train_np, y_policy_train_np, y_value_train_np, dataset_name=\"Train\")\n#         # num_workers > 0 useful if data loading is a bottleneck, but uses more RAM. Test 2 or 4.\n#         # pin_memory=True can speed up CPU->GPU transfer.\n#         train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=True)\n#         print(f\"  Train DataLoader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n\n#         # Create Validation Dataset and DataLoader\n#         val_dataset = ChessDatasetV4(X_val_np, y_policy_val_np, y_value_val_np, dataset_name=\"Validation\")\n#         val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2, pin_memory=(device.type == 'cuda')) # Larger batch size for validation is fine\n#         print(f\"  Validation DataLoader: {len(val_dataloader)} batches of size {BATCH_SIZE * 2}\")\n\n#         # --- Optional: Final Memory Cleanup ---\n#         # Delete the split NumPy arrays if memory is extremely tight after creating DataLoaders\n#         print(\"\\nCleaning up split NumPy arrays...\")\n#         del X_train_np, y_policy_train_np, y_value_train_np\n#         del X_val_np, y_policy_val_np, y_value_val_np\n#         gc.collect()\n#         print(\"NumPy arrays deleted.\")\n#         setup_ok = True\n\n#     except Exception as e:\n#         print(f\"Error creating Datasets/DataLoaders: {e}\")\n#         setup_ok = False\n# else:\n#     print(\"\\nSkipping Dataset/DataLoader creation as data preparation failed.\")\n#     setup_ok = False\n\n# # --- Model, Loss, Optimizer Initialization ---\n# if setup_ok:\n#     print(\"\\nInitializing Model, Loss Functions, and Optimizer...\")\n#     # Initialize Model\n#     model = ChessModelV4(\n#         num_policy_classes=num_classes, # Determined during encoding\n#         num_res_blocks=NUM_RES_BLOCKS,\n#         num_channels=NUM_CHANNELS\n#     ).to(device)\n#     print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n#     if device.type == 'cuda':\n#         print(f\"  CUDA Memory Allocated after model: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n#         print(f\"  CUDA Memory Reserved after model:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n\n#     # Loss Functions\n#     criterion_policy = nn.CrossEntropyLoss() # Expects raw logits\n#     criterion_value = nn.MSELoss() # Mean Squared Error for value prediction\n#     print(\"Loss functions defined (CrossEntropy for policy, MSE for value).\")\n\n#     # Optimizer\n#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n#     print(f\"Optimizer defined: Adam (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\")\n\n#     # Learning Rate Scheduler\n#     # Reduce LR if *validation* loss plateaus\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n#     print(\"Learning rate scheduler defined: ReduceLROnPlateau on validation loss.\")\n\n#     print(\"\\n--- Training setup complete! ---\")\n\n# else:\n#     print(\"\\nSkipping Model/Optimizer setup due to previous errors.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 7: Training Setup (Device, Datasets, Model, Optimizer)\n# Prepare device, create Train/Validation Datasets and DataLoaders, initialize the model, loss functions, and optimizer.\n\n# %% {\"cell_type\": \"code\"}\n# --- Configuration ---\nBATCH_SIZE = 4096 # Increase if VRAM allows (e.g., 256, 512, 1024)\nLEARNING_RATE = 0.001 # Adam initial learning rate\nNUM_EPOCHS = 20 # Adjust based on convergence and time limits\nPOLICY_LOSS_WEIGHT = 1.0\nVALUE_LOSS_WEIGHT = 0.8 # Slightly de-weight value loss? Or keep 1.0. Let's try 0.8.\nNUM_RES_BLOCKS = 9 # Number of residual blocks (e.g., 5-19)\nNUM_CHANNELS = 128 # Number of conv channels (e.g., 128, 256)\nWEIGHT_DECAY = 1e-5 # L2 Regularization for Adam\n\n# --- Setup Device ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.empty_cache()\n    print(f\"  CUDA Memory Allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n    print(f\"  CUDA Memory Reserved:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU (Training will be significantly slower!)\")\n\n# --- Create Datasets and DataLoaders ---\ntrain_dataset = None\nval_dataset = None\ntrain_dataloader = None\nval_dataloader = None\nmodel = None # Initialize to None\n\nif data_ready: # Check if data splitting and NumPy conversion succeeded\n    print(\"\\nCreating Datasets and DataLoaders...\")\n    try:\n        # Create Train Dataset and DataLoader\n        train_dataset = ChessDatasetV4(X_train_np, y_policy_train_np, y_value_train_np, dataset_name=\"Train\")\n        # num_workers > 0 useful if data loading is a bottleneck, but uses more RAM. Test 2 or 4.\n        # pin_memory=True can speed up CPU->GPU transfer.\n        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=True)\n        print(f\"  Train DataLoader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n\n        # Create Validation Dataset and DataLoader\n        val_dataset = ChessDatasetV4(X_val_np, y_policy_val_np, y_value_val_np, dataset_name=\"Validation\")\n        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2, pin_memory=(device.type == 'cuda')) # Larger batch size for validation is fine\n        print(f\"  Validation DataLoader: {len(val_dataloader)} batches of size {BATCH_SIZE * 2}\")\n\n        # --- Optional: Final Memory Cleanup ---\n        # Delete the split NumPy arrays if memory is extremely tight after creating DataLoaders\n        print(\"\\nCleaning up split NumPy arrays...\")\n        del X_train_np, y_policy_train_np, y_value_train_np\n        del X_val_np, y_policy_val_np, y_value_val_np\n        gc.collect()\n        print(\"NumPy arrays deleted.\")\n        setup_ok = True\n\n    except Exception as e:\n        print(f\"Error creating Datasets/DataLoaders: {e}\")\n        setup_ok = False\nelse:\n    print(\"\\nSkipping Dataset/DataLoader creation as data preparation failed.\")\n    setup_ok = False\n\n# --- Model, Loss, Optimizer Initialization ---\nif setup_ok:\n    print(\"\\\\nInitializing Model, Loss Functions, and Optimizer...\")\n    # Initialize Model structure (without .to(device) yet)\n    model = ChessModelV4(\n        num_policy_classes=num_classes, # Determined during encoding\n        num_res_blocks=NUM_RES_BLOCKS, # MUST match trained model\n        num_channels=NUM_CHANNELS    # MUST match trained model\n    )\n\n    # Check for multiple GPUs and wrap with DataParallel if available\n    # This is the key change for T4x2\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"--- Using {torch.cuda.device_count()} GPUs via DataParallel! ---\")\n        model = nn.DataParallel(model) # Wrap the model for multi-GPU\n\n    # Move the model (or wrapped model) to the primary device\n    model.to(device)\n\n    print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n    if device.type == 'cuda':\n        print(f\"  CUDA Memory Allocated after model: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n        print(f\"  CUDA Memory Reserved after model:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n\n    # Loss Functions\n    criterion_policy = nn.CrossEntropyLoss() # Expects raw logits\n    criterion_value = nn.MSELoss() # Mean Squared Error for value prediction\n    print(\"Loss functions defined (CrossEntropy for policy, MSE for value).\")\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    print(f\"Optimizer defined: Adam (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\")\n\n    # Learning Rate Scheduler\n    # Reduce LR if *validation* loss plateaus\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n    print(\"Learning rate scheduler defined: ReduceLROnPlateau on validation loss.\")\n\n    print(\"\\\\n--- Training setup complete! ---\")\n\nelse:\n    print(\"\\\\nSkipping Model/Optimizer setup due to previous errors.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 7: Training Setup (Device, Datasets, Model, Optimizer)\n# Prepare device, create Train/Validation Datasets and DataLoaders, initialize the model, loss functions, and optimizer.\n\n# %% {\"cell_type\": \"code\"}\n# --- Configuration ---\n# VRAM Suggestion: Start moderately, e.g., 2048 or 3072 total. 4096 might be okay but monitor closely.\nBATCH_SIZE = 4096  # ADJUST THIS! Start lower (e.g., 1024, 2048) and increase if VRAM allows.\n                   # This is the TOTAL batch size across both GPUs.\nLEARNING_RATE = 0.001 # Adam initial learning rate\nNUM_EPOCHS = 20 # Adjust based on convergence and time limits\nPOLICY_LOSS_WEIGHT = 1.0\nVALUE_LOSS_WEIGHT = 0.8 # Slightly de-weight value loss? Or keep 1.0. Let's try 0.8.\nNUM_RES_BLOCKS = 9 # Number of residual blocks (e.g., 5-19) - MUST match loaded model if loading weights\nNUM_CHANNELS = 128 # Number of conv channels (e.g., 128, 256) - MUST match loaded model\nWEIGHT_DECAY = 1e-5 # L2 Regularization for Adam\n\n# --- Setup Device ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"Using {torch.cuda.device_count()} GPU(s):\")\n    for i in range(torch.cuda.device_count()):\n         print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n    torch.cuda.empty_cache()\n    # Initial memory check might be misleading before allocation\n    # print(f\"  CUDA Memory Allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n    # print(f\"  CUDA Memory Reserved:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU (Training will be significantly slower!)\")\n    torch.cuda.device_count = lambda: 0 # Mock device count if no CUDA\n\n# --- Create Datasets and DataLoaders ---\ntrain_dataset = None\nval_dataset = None\ntrain_dataloader = None\nval_dataloader = None\nmodel = None # Initialize to None\nsetup_ok = False # Assume failure until success\n\nif data_ready: # Check if data splitting and NumPy conversion succeeded\n    print(\"\\nCreating Datasets and DataLoaders...\")\n    try:\n        # Create Train Dataset and DataLoader\n        train_dataset = ChessDatasetV4(X_train_np, y_policy_train_np, y_value_train_np, dataset_name=\"Train\")\n        # num_workers=2 is reasonable for T4. pin_memory=True helps CPU->GPU transfer.\n        # drop_last=True is important for DataParallel if the last batch isn't divisible by num_gpus\n        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=True)\n        print(f\"  Train DataLoader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n\n        # Create Validation Dataset and DataLoader\n        val_dataset = ChessDatasetV4(X_val_np, y_policy_val_np, y_value_val_np, dataset_name=\"Validation\")\n        # Larger batch size for validation is fine, less memory overhead than training\n        val_batch_size = BATCH_SIZE * 2\n        val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=2, pin_memory=(device.type == 'cuda'), drop_last=False) # drop_last=False usually fine for validation\n        print(f\"  Validation DataLoader: {len(val_dataloader)} batches of size {val_batch_size}\")\n\n        # --- Optional: Final Memory Cleanup ---\n        # Good idea to delete large arrays after creating loaders\n        print(\"\\nCleaning up split NumPy arrays...\")\n        del X_train_np, y_policy_train_np, y_value_train_np\n        del X_val_np, y_policy_val_np, y_value_val_np\n        gc.collect()\n        print(\"NumPy arrays deleted.\")\n        setup_ok = True # Data loading succeeded\n\n    except NameError as e:\n         print(f\"Error creating Datasets/DataLoaders: {e}\")\n         print(\"This likely means the data arrays (e.g., X_train_np) were not loaded or created correctly in Cell 5.\")\n         setup_ok = False\n    except Exception as e:\n        print(f\"Error creating Datasets/DataLoaders: {e}\")\n        setup_ok = False\nelse:\n    print(\"\\nSkipping Dataset/DataLoader creation as data preparation failed (data_ready=False).\")\n    setup_ok = False\n\n# --- Model, Loss, Optimizer Initialization ---\nif setup_ok:\n    print(\"\\nInitializing Model, Loss Functions, and Optimizer...\")\n    # Initialize Model structure (without .to(device) yet)\n    # Ensure num_classes comes from the loaded mappings in Cell 5!\n    if 'num_classes' not in locals() or num_classes is None:\n         print(\"ERROR: num_classes not found. Was Cell 5 (data loading/processing) run successfully?\")\n         setup_ok = False\n    else:\n        model = ChessModelV4(\n            num_policy_classes=num_classes, # Determined during encoding\n            num_res_blocks=NUM_RES_BLOCKS,\n            num_channels=NUM_CHANNELS\n        )\n\n        # <<<--- DataParallel Modification START --->>>\n        # Check for multiple GPUs and wrap with DataParallel if available\n        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n            print(f\"--- Wrapping model with nn.DataParallel for {torch.cuda.device_count()} GPUs ---\")\n            model = nn.DataParallel(model) # Wrap the model\n        # <<<--- DataParallel Modification END --->>>\n\n        # Move the model (or wrapped model) to the primary device\n        model.to(device)\n\n        print(f\"Model created/wrapped with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n        if device.type == 'cuda':\n             # Check memory *after* moving model to GPU\n             print(f\"  CUDA Memory Allocated after model load: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n             print(f\"  CUDA Memory Reserved after model load:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n\n        # Loss Functions\n        criterion_policy = nn.CrossEntropyLoss() # Expects raw logits\n        criterion_value = nn.MSELoss() # Mean Squared Error for value prediction\n        print(\"Loss functions defined (CrossEntropy for policy, MSE for value).\")\n\n        # Optimizer\n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        print(f\"Optimizer defined: Adam (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\")\n\n        # Learning Rate Scheduler\n        # Reduce LR if *validation* loss plateaus\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n        print(\"Learning rate scheduler defined: ReduceLROnPlateau on validation loss.\")\n\n        print(\"\\n--- Training setup complete! ---\")\n\nelse:\n    print(\"\\nSkipping Model/Optimizer setup due to previous errors.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:24:14.654247Z","iopub.execute_input":"2025-04-04T12:24:14.654550Z","iopub.status.idle":"2025-04-04T12:24:17.082626Z","shell.execute_reply.started":"2025-04-04T12:24:14.654526Z","shell.execute_reply":"2025-04-04T12:24:17.081774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ## Cell 8: Training Loop with Validation\n# # The main training loop. Includes training steps and a validation step after each epoch to monitor performance on unseen data and check for overfitting.\n\n# # %% {\"cell_type\": \"code\"}\n# # Dictionary to store training and validation history\n# history = {\n#     'epoch': [],\n#     'train_loss': [], 'train_policy_loss': [], 'train_value_loss': [],\n#     'val_loss': [], 'val_policy_loss': [], 'val_value_loss': [], 'val_policy_accuracy': [],\n#     'lr': []\n# }\n\n# if model and train_dataloader and val_dataloader: # Check if setup was successful\n#     print(f\"\\n--- Starting Training for {NUM_EPOCHS} Epochs ---\")\n#     training_start_time = time.time()\n#     best_val_loss = float('inf') # Track best validation loss for saving best model\n\n#     for epoch in range(NUM_EPOCHS):\n#         epoch_start_time = time.time()\n\n#         # --- Training Phase ---\n#         model.train() # Set model to training mode (enables dropout, etc.)\n#         running_train_loss = 0.0\n#         running_train_policy_loss = 0.0\n#         running_train_value_loss = 0.0\n\n#         train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n#         for batch_idx, (inputs, policy_labels, value_labels) in enumerate(train_progress_bar):\n#             inputs = inputs.to(device, non_blocking=True)\n#             policy_labels = policy_labels.to(device, non_blocking=True)\n#             value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n#             optimizer.zero_grad() # Clear gradients\n\n#             # Forward pass\n#             policy_logits, value_output = model(inputs)\n\n#             # Calculate losses\n#             loss_policy = criterion_policy(policy_logits, policy_labels)\n#             loss_value = criterion_value(value_output, value_labels)\n#             total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n#             # Backward pass and optimization\n#             total_loss.backward()\n#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5) # Clip gradients\n#             optimizer.step()\n\n#             # Accumulate training losses\n#             running_train_loss += total_loss.item()\n#             running_train_policy_loss += loss_policy.item()\n#             running_train_value_loss += loss_value.item()\n\n#             # Update progress bar postfix\n#             if (batch_idx + 1) % 100 == 0:\n#                  train_progress_bar.set_postfix({\n#                      'Loss': f'{running_train_loss / (batch_idx + 1):.4f}',\n#                      'P_Loss': f'{running_train_policy_loss / (batch_idx + 1):.4f}',\n#                      'V_Loss': f'{running_train_value_loss / (batch_idx + 1):.4f}'\n#                  })\n\n#         avg_train_loss = running_train_loss / len(train_dataloader)\n#         avg_train_policy_loss = running_train_policy_loss / len(train_dataloader)\n#         avg_train_value_loss = running_train_value_loss / len(train_dataloader)\n\n#         # --- Validation Phase ---\n#         model.eval() # Set model to evaluation mode (disables dropout, etc.)\n#         running_val_loss = 0.0\n#         running_val_policy_loss = 0.0\n#         running_val_value_loss = 0.0\n#         correct_policy_predictions = 0\n#         total_val_samples = 0\n\n#         val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Validate]\", leave=False)\n\n#         with torch.no_grad(): # Disable gradient calculations for validation\n#             for inputs, policy_labels, value_labels in val_progress_bar:\n#                 inputs = inputs.to(device, non_blocking=True)\n#                 policy_labels = policy_labels.to(device, non_blocking=True)\n#                 value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n#                 # Forward pass\n#                 policy_logits, value_output = model(inputs)\n\n#                 # Calculate validation losses\n#                 loss_policy = criterion_policy(policy_logits, policy_labels)\n#                 loss_value = criterion_value(value_output, value_labels)\n#                 total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n#                 running_val_loss += total_loss.item()\n#                 running_val_policy_loss += loss_policy.item()\n#                 running_val_value_loss += loss_value.item()\n\n#                 # Calculate policy accuracy (Top-1)\n#                 predicted_indices = torch.argmax(policy_logits, dim=1)\n#                 correct_policy_predictions += (predicted_indices == policy_labels).sum().item()\n#                 total_val_samples += policy_labels.size(0)\n\n#         avg_val_loss = running_val_loss / len(val_dataloader)\n#         avg_val_policy_loss = running_val_policy_loss / len(val_dataloader)\n#         avg_val_value_loss = running_val_value_loss / len(val_dataloader)\n#         val_policy_accuracy = correct_policy_predictions / total_val_samples if total_val_samples > 0 else 0.0\n\n#         # --- End of Epoch Summary ---\n#         epoch_end_time = time.time()\n#         epoch_duration = epoch_end_time - epoch_start_time\n#         current_lr = optimizer.param_groups[0]['lr']\n\n#         print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} Summary ---\")\n#         print(f\"  Time: {epoch_duration:.2f}s | LR: {current_lr:.6f}\")\n#         print(f\"  Train Loss: {avg_train_loss:.4f} (Policy: {avg_train_policy_loss:.4f}, Value: {avg_train_value_loss:.4f})\")\n#         print(f\"  Valid Loss: {avg_val_loss:.4f} (Policy: {avg_val_policy_loss:.4f}, Value: {avg_val_value_loss:.4f})\")\n#         print(f\"  Valid Policy Accuracy: {val_policy_accuracy:.4f} ({correct_policy_predictions}/{total_val_samples})\")\n#         if device.type == 'cuda':\n#              print(f\"  CUDA Mem Used (Peak): {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB\")\n#              torch.cuda.reset_peak_memory_stats(device) # Reset peak counter for next epoch\n\n#         # Store history\n#         history['epoch'].append(epoch + 1)\n#         history['train_loss'].append(avg_train_loss)\n#         history['train_policy_loss'].append(avg_train_policy_loss)\n#         history['train_value_loss'].append(avg_train_value_loss)\n#         history['val_loss'].append(avg_val_loss)\n#         history['val_policy_loss'].append(avg_val_policy_loss)\n#         history['val_value_loss'].append(avg_val_value_loss)\n#         history['val_policy_accuracy'].append(val_policy_accuracy)\n#         history['lr'].append(current_lr)\n\n#         # Step the scheduler based on validation loss\n#         scheduler.step(avg_val_loss)\n\n#         # Save the model if validation loss improved\n#         if avg_val_loss < best_val_loss:\n#             print(f\"  Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving best model...\")\n#             best_val_loss = avg_val_loss\n#             try:\n#                 torch.save(model.state_dict(), MODEL_SAVE_PATH.replace('.pth', '_best.pth'))\n#                 print(f\"    Best model checkpoint saved to {MODEL_SAVE_PATH.replace('.pth', '_best.pth')}\")\n#             except Exception as e:\n#                 print(f\"    Error saving best model checkpoint: {e}\")\n\n#         # Optional: Save checkpoint periodically regardless of improvement\n#         if (epoch + 1) % 10 == 0:\n#            chkpt_path = f\"/kaggle/working/checkpoint_epoch_{epoch+1}.pth\"\n#            # Save more state if needed (optimizer, scheduler, etc.)\n#            torch.save(model.state_dict(), chkpt_path)\n#            print(f\"  Periodic checkpoint saved to {chkpt_path}\")\n\n#         gc.collect() # Collect garbage at end of epoch\n\n#     # --- Training Finished ---\n#     training_end_time = time.time()\n#     total_training_time = training_end_time - training_start_time\n#     print(f\"\\n--- Training finished in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes) ---\")\n#     print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\n# else:\n#     print(\"Skipping training loop as model or dataloaders were not initialized correctly.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 8: Training Loop with Validation\n# The main training loop. Includes training steps and a validation step after each epoch to monitor performance on unseen data and check for overfitting.\n\n# %% {\"cell_type\": \"code\"}\n# Dictionary to store training and validation history\nhistory = {\n    'epoch': [],\n    'train_loss': [], 'train_policy_loss': [], 'train_value_loss': [],\n    'val_loss': [], 'val_policy_loss': [], 'val_value_loss': [], 'val_policy_accuracy': [],\n    'lr': []\n}\n\nif model and train_dataloader and val_dataloader: # Check if setup was successful\n    print(f\"\\n--- Starting Training for {NUM_EPOCHS} Epochs ---\")\n    training_start_time = time.time()\n    best_val_loss = float('inf') # Track best validation loss for saving best model\n\n    for epoch in range(NUM_EPOCHS):\n        epoch_start_time = time.time()\n\n        # --- Training Phase ---\n        model.train() # Set model to training mode (enables dropout, etc.)\n        running_train_loss = 0.0\n        running_train_policy_loss = 0.0\n        running_train_value_loss = 0.0\n\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]\", leave=False)\n\n        for batch_idx, (inputs, policy_labels, value_labels) in enumerate(train_progress_bar):\n            inputs = inputs.to(device, non_blocking=True)\n            policy_labels = policy_labels.to(device, non_blocking=True)\n            value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n            optimizer.zero_grad() # Clear gradients\n\n            # Forward pass\n            policy_logits, value_output = model(inputs)\n\n            # Calculate losses\n            loss_policy = criterion_policy(policy_logits, policy_labels)\n            loss_value = criterion_value(value_output, value_labels)\n            total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n            # Backward pass and optimization\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5) # Clip gradients\n            optimizer.step()\n\n            # Accumulate training losses\n            running_train_loss += total_loss.item()\n            running_train_policy_loss += loss_policy.item()\n            running_train_value_loss += loss_value.item()\n\n            # Update progress bar postfix\n            if (batch_idx + 1) % 100 == 0:\n                 train_progress_bar.set_postfix({\n                     'Loss': f'{running_train_loss / (batch_idx + 1):.4f}',\n                     'P_Loss': f'{running_train_policy_loss / (batch_idx + 1):.4f}',\n                     'V_Loss': f'{running_train_value_loss / (batch_idx + 1):.4f}'\n                 })\n\n        avg_train_loss = running_train_loss / len(train_dataloader)\n        avg_train_policy_loss = running_train_policy_loss / len(train_dataloader)\n        avg_train_value_loss = running_train_value_loss / len(train_dataloader)\n\n        # --- Validation Phase ---\n        model.eval() # Set model to evaluation mode (disables dropout, etc.)\n        running_val_loss = 0.0\n        running_val_policy_loss = 0.0\n        running_val_value_loss = 0.0\n        correct_policy_predictions = 0\n        total_val_samples = 0\n\n        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Validate]\", leave=False)\n\n        with torch.no_grad(): # Disable gradient calculations for validation\n            for inputs, policy_labels, value_labels in val_progress_bar:\n                inputs = inputs.to(device, non_blocking=True)\n                policy_labels = policy_labels.to(device, non_blocking=True)\n                value_labels = value_labels.to(device, non_blocking=True).unsqueeze(1)\n\n                # Forward pass\n                policy_logits, value_output = model(inputs)\n\n                # Calculate validation losses\n                loss_policy = criterion_policy(policy_logits, policy_labels)\n                loss_value = criterion_value(value_output, value_labels)\n                total_loss = (POLICY_LOSS_WEIGHT * loss_policy) + (VALUE_LOSS_WEIGHT * loss_value)\n\n                running_val_loss += total_loss.item()\n                running_val_policy_loss += loss_policy.item()\n                running_val_value_loss += loss_value.item()\n\n                # Calculate policy accuracy (Top-1)\n                predicted_indices = torch.argmax(policy_logits, dim=1)\n                correct_policy_predictions += (predicted_indices == policy_labels).sum().item()\n                total_val_samples += policy_labels.size(0)\n\n        avg_val_loss = running_val_loss / len(val_dataloader)\n        avg_val_policy_loss = running_val_policy_loss / len(val_dataloader)\n        avg_val_value_loss = running_val_value_loss / len(val_dataloader)\n        val_policy_accuracy = correct_policy_predictions / total_val_samples if total_val_samples > 0 else 0.0\n\n        # --- End of Epoch Summary ---\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} Summary ---\")\n        print(f\"  Time: {epoch_duration:.2f}s | LR: {current_lr:.6f}\")\n        print(f\"  Train Loss: {avg_train_loss:.4f} (Policy: {avg_train_policy_loss:.4f}, Value: {avg_train_value_loss:.4f})\")\n        print(f\"  Valid Loss: {avg_val_loss:.4f} (Policy: {avg_val_policy_loss:.4f}, Value: {avg_val_value_loss:.4f})\")\n        print(f\"  Valid Policy Accuracy: {val_policy_accuracy:.4f} ({correct_policy_predictions}/{total_val_samples})\")\n        if device.type == 'cuda':\n             print(f\"  CUDA Mem Used (Peak): {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB\")\n             torch.cuda.reset_peak_memory_stats(device) # Reset peak counter for next epoch\n\n        # Store history\n        history['epoch'].append(epoch + 1)\n        history['train_loss'].append(avg_train_loss)\n        history['train_policy_loss'].append(avg_train_policy_loss)\n        history['train_value_loss'].append(avg_train_value_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['val_policy_loss'].append(avg_val_policy_loss)\n        history['val_value_loss'].append(avg_val_value_loss)\n        history['val_policy_accuracy'].append(val_policy_accuracy)\n        history['lr'].append(current_lr)\n\n        # Step the scheduler based on validation loss\n        scheduler.step(avg_val_loss)\n\n        # Save the model if validation loss improved\n        # Save the model if validation loss improved\n        if avg_val_loss < best_val_loss:\n            print(f\"  Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving best model...\")\n            best_val_loss = avg_val_loss\n            try:\n                # <<<--- DataParallel Saving Modification --->>>\n                if isinstance(model, nn.DataParallel):\n                    state_dict_to_save = model.module.state_dict()\n                    print(\"    Saving state_dict from model.module (DataParallel active)\")\n                else:\n                    state_dict_to_save = model.state_dict()\n                    print(\"    Saving state_dict from model (DataParallel not active)\")\n                # <<<-------------------------------------->>>\n                best_model_save_path = MODEL_SAVE_PATH.replace('.pth', '_best.pth')\n                torch.save(state_dict_to_save, best_model_save_path)\n                print(f\"    Best model checkpoint saved to {best_model_save_path}\")\n            except Exception as e:\n                print(f\"    Error saving best model checkpoint: {e}\")\n\n        # Optional: Save checkpoint periodically regardless of improvement\n        if (epoch + 1) % 10 == 0:\n           chkpt_path = f\"/kaggle/working/checkpoint_epoch_{epoch+1}.pth\"\n           # Save the full DataParallel state dict for checkpoints if you might resume\n           # training with DataParallel active. Or save module state_dict for simplicity.\n           # Let's save the module state dict for consistency with 'best' save.\n           try:\n                if isinstance(model, nn.DataParallel):\n                    state_dict_to_save = model.module.state_dict()\n                else:\n                    state_dict_to_save = model.state_dict()\n                torch.save(state_dict_to_save, chkpt_path)\n                print(f\"  Periodic checkpoint (module state_dict) saved to {chkpt_path}\")\n           except Exception as e:\n                print(f\"    Error saving periodic checkpoint: {e}\")\n\n        gc.collect() # Collect garbage at end of epoch\n\n    # --- Training Finished ---\n    training_end_time = time.time()\n    total_training_time = training_end_time - training_start_time\n    print(f\"\\n--- Training finished in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes) ---\")\n    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\nelse:\n    print(\"Skipping training loop as model or dataloaders were not initialized correctly.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T12:32:27.508422Z","iopub.execute_input":"2025-04-04T12:32:27.509105Z","iopub.status.idle":"2025-04-04T14:01:05.097842Z","shell.execute_reply.started":"2025-04-04T12:32:27.509060Z","shell.execute_reply":"2025-04-04T14:01:05.096873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 9: Plot Training Graphs (Loss & Accuracy) ðŸ“ˆðŸ“‰\n# Visualize the training and validation progress. Seeing these curves helps understand learning dynamics and detect overfitting!\n\n# %% {\"cell_type\": \"code\"}\nif history['epoch']: # Check if training ran and history exists\n    print(\"\\n--- Plotting Training and Validation Progress ---\")\n    epochs_range = history['epoch']\n\n    plt.figure(figsize=(14, 10)) # Larger figure size\n\n    # --- Plot Total Loss ---\n    plt.subplot(2, 2, 1) # 2 rows, 2 cols, 1st plot\n    plt.plot(epochs_range, history['train_loss'], label='Train Total Loss', color='royalblue', marker='.')\n    plt.plot(epochs_range, history['val_loss'], label='Validation Total Loss', color='darkorange', marker='.')\n    plt.title('Total Loss (Train vs Validation)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # --- Plot Policy Loss ---\n    plt.subplot(2, 2, 2) # 2 rows, 2 cols, 2nd plot\n    plt.plot(epochs_range, history['train_policy_loss'], label='Train Policy Loss', color='dodgerblue', linestyle='--', marker='x')\n    plt.plot(epochs_range, history['val_policy_loss'], label='Validation Policy Loss', color='sandybrown', linestyle='--', marker='x')\n    plt.title('Policy Loss (Train vs Validation)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Policy Loss (CrossEntropy)')\n    plt.legend()\n    plt.grid(True)\n\n    # --- Plot Value Loss ---\n    plt.subplot(2, 2, 3) # 2 rows, 2 cols, 3rd plot\n    plt.plot(epochs_range, history['train_value_loss'], label='Train Value Loss', color='mediumseagreen', linestyle=':', marker='s')\n    plt.plot(epochs_range, history['val_value_loss'], label='Validation Value Loss', color='lightcoral', linestyle=':', marker='s')\n    plt.title('Value Loss (Train vs Validation)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Value Loss (MSE)')\n    plt.legend()\n    plt.grid(True)\n\n    # --- Plot Validation Policy Accuracy & Learning Rate ---\n    ax1 = plt.subplot(2, 2, 4) # 2 rows, 2 cols, 4th plot\n    color = 'tab:red'\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Validation Policy Accuracy', color=color)\n    ax1.plot(epochs_range, history['val_policy_accuracy'], label='Validation Policy Accuracy', color=color, marker='*')\n    ax1.tick_params(axis='y', labelcolor=color)\n    ax1.grid(True, axis='y')\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n    color = 'tab:blue'\n    ax2.set_ylabel('Learning Rate', color=color)\n    ax2.plot(epochs_range, history['lr'], label='Learning Rate', color=color, linestyle='-.', marker='+')\n    ax2.tick_params(axis='y', labelcolor=color)\n\n    plt.title('Validation Policy Accuracy & Learning Rate')\n    # Add combined legend if needed, or rely on axis labels\n    # fig.tight_layout() # For automatic layout adjustment\n\n    plt.tight_layout() # Adjust subplot parameters for a tight layout\n    plt.show()\n\nelse:\n    print(\"\\nNo training history found to plot.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:01:59.332927Z","iopub.execute_input":"2025-04-04T14:01:59.333369Z","iopub.status.idle":"2025-04-04T14:02:00.310937Z","shell.execute_reply.started":"2025-04-04T14:01:59.333314Z","shell.execute_reply":"2025-04-04T14:02:00.309958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 10: Save Final Model and Mappings\n# Save the final model state (potentially overwrite the 'best' model if you prefer the final state) and the necessary mappings.\n\n# %% {\"cell_type\": \"code\"}\nif model and 'move_to_int' in locals(): # Check if model exists and mappings were created\n    # Save the final model state\n    print(f\"\\nSaving final model state dict to: {MODEL_SAVE_PATH}\")\n    try:\n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        print(\"  Final model saved successfully.\")\n    except Exception as e:\n        print(f\"  Error saving final model: {e}\")\n\n    # Save mappings\n    print(f\"Saving mappings to: {MAPPING_SAVE_PATH}\")\n    try:\n        mappings = {\n            'move_to_int': move_to_int,\n            'int_to_move': int_to_move,\n            'num_classes': num_classes\n        }\n        with open(MAPPING_SAVE_PATH, \"wb\") as f:\n            pickle.dump(mappings, f)\n        print(\"  Mappings saved successfully!\")\n    except Exception as e:\n        print(f\"  Error saving mappings: {e}\")\n\n    print(\"\\n--- Saving Complete ---\")\n    # You might want to load the '_best.pth' model for prediction if you saved it earlier\n    LOAD_BEST_MODEL_FOR_PREDICTION = True # Set to True to use the best saved model\n\nelse:\n    print(\"\\nSkipping final saving as model or mappings are not available.\")\n    LOAD_BEST_MODEL_FOR_PREDICTION = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 10: Save Final Model and Mappings\n# Save the final model state (potentially overwrite the 'best' model if you prefer the final state) and the necessary mappings.\n\n# %% {\"cell_type\": \"code\"}\nif 'model' in locals() and model is not None and 'move_to_int' in locals(): # Check if model exists and mappings were created\n    # Save the final model state\n    print(f\"\\nSaving final model state dict to: {MODEL_SAVE_PATH}\")\n    try:\n        # <<<--- DataParallel Saving Modification --->>>\n        if isinstance(model, nn.DataParallel):\n            state_dict_to_save = model.module.state_dict()\n            print(\"  Saving final state_dict from model.module (DataParallel active)\")\n        else:\n            state_dict_to_save = model.state_dict()\n            print(\"  Saving final state_dict from model (DataParallel not active)\")\n        # <<<-------------------------------------->>>\n        torch.save(state_dict_to_save, MODEL_SAVE_PATH)\n        print(\"  Final model saved successfully.\")\n    except Exception as e:\n        print(f\"  Error saving final model: {e}\")\n\n    # Save mappings (No changes needed here)\n    print(f\"Saving mappings to: {MAPPING_SAVE_PATH}\")\n    try:\n        mappings = {\n            'move_to_int': move_to_int,\n            'int_to_move': int_to_move,\n            'num_classes': num_classes\n        }\n        with open(MAPPING_SAVE_PATH, \"wb\") as f:\n            pickle.dump(mappings, f)\n        print(\"  Mappings saved successfully!\")\n    except Exception as e:\n        print(f\"  Error saving mappings: {e}\")\n\n    print(\"\\n--- Saving Complete ---\")\n    # You might want to load the '_best.pth' model for prediction if you saved it earlier\n    LOAD_BEST_MODEL_FOR_PREDICTION = True # Set to True to use the best saved model\n\nelse:\n    print(\"\\nSkipping final saving as model or mappings are not available.\")\n    LOAD_BEST_MODEL_FOR_PREDICTION = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:03:06.428616Z","iopub.execute_input":"2025-04-04T14:03:06.428968Z","iopub.status.idle":"2025-04-04T14:03:06.494589Z","shell.execute_reply.started":"2025-04-04T14:03:06.428935Z","shell.execute_reply":"2025-04-04T14:03:06.493866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 11: Prediction Setup\n# Load the mappings and the desired model (either final or best based on validation) for inference.\n\n# %% {\"cell_type\": \"code\"}\n# --- Load Mappings ---\nloaded_mappings = None\nloaded_move_to_int = None\nloaded_int_to_move = None\nloaded_num_classes = None\ntry:\n    print(f\"\\n--- Prediction Setup ---\")\n    print(f\"Loading mappings from: {MAPPING_SAVE_PATH}\")\n    with open(MAPPING_SAVE_PATH, \"rb\") as f:\n        loaded_mappings = pickle.load(f)\n    loaded_move_to_int = loaded_mappings['move_to_int']\n    loaded_int_to_move = loaded_mappings['int_to_move']\n    loaded_num_classes = loaded_mappings['num_classes']\n    print(\"Mappings loaded successfully.\")\n    print(f\"  Number of move classes: {loaded_num_classes}\")\nexcept Exception as e:\n    print(f\"Error: Could not load mapping file from {MAPPING_SAVE_PATH}: {e}\")\n    loaded_mappings = None\n\n# --- Setup Device ---\nif torch.cuda.is_available():\n    prediction_device = torch.device(\"cuda\")\n    print(f\"Using GPU for prediction.\")\nelse:\n    prediction_device = torch.device(\"cpu\")\n    print(\"Using CPU for prediction.\")\n\n# --- Load Model ---\nprediction_model = None\nmodel_path_to_load = MODEL_SAVE_PATH # Default to final model\n\n# Choose whether to load the best model based on validation loss\nif LOAD_BEST_MODEL_FOR_PREDICTION:\n     best_model_path = MODEL_SAVE_PATH.replace('.pth', '_best.pth')\n     if os.path.exists(best_model_path):\n         model_path_to_load = best_model_path\n         print(f\"Attempting to load best model from: {model_path_to_load}\")\n     else:\n         print(f\"Warning: Best model file '{best_model_path}' not found. Loading final model instead.\")\n\nif loaded_mappings:\n    print(f\"\\nLoading model structure and weights from: {model_path_to_load}\")\n    try:\n        # Re-initialize model structure (ensure parameters match saved model!)\n        prediction_model = ChessModelV4(\n            num_policy_classes=loaded_num_classes,\n            num_res_blocks=NUM_RES_BLOCKS, # MUST match trained model\n            num_channels=NUM_CHANNELS    # MUST match trained model\n        )\n        # Load the saved weights onto the correct device\n        prediction_model.load_state_dict(torch.load(model_path_to_load, map_location=prediction_device))\n        prediction_model.to(prediction_device)\n        prediction_model.eval() # Set to evaluation mode!\n        print(\"Model loaded successfully and set to evaluation mode.\")\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {model_path_to_load}.\")\n    except Exception as e:\n        print(f\"Error loading model state dict: {e}\")\n        print(\"Ensure the model definition (ResBlocks, Channels, etc.) matches the saved model!\")\n        prediction_model = None\nelse:\n    print(\"\\nSkipping model loading as mappings were not loaded.\")\n\nprint(\"\\n--- Prediction Setup Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:04:39.310363Z","iopub.execute_input":"2025-04-04T14:04:39.310716Z","iopub.status.idle":"2025-04-04T14:04:39.491440Z","shell.execute_reply.started":"2025-04-04T14:04:39.310687Z","shell.execute_reply":"2025-04-04T14:04:39.490497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 12: Prediction Function (V4)\n# Takes a board state, uses the loaded V4 model to predict the best *legal* move based on the policy head, and also returns the value prediction.\n\n# %% {\"cell_type\": \"code\"}\ndef prepare_input_v4(board: Board):\n    \"\"\" Prepares a single board state for the V4 model \"\"\"\n    matrix = board_to_matrix_v4(board, flip=False) # No flip during standard prediction\n    X_tensor = torch.tensor(matrix, dtype=torch.float32).unsqueeze(0) # Add batch dim [1, C, H, W]\n    return X_tensor\n\ndef predict_move_v4(board: Board, model_to_use, device_to_use, int_to_move_map):\n    \"\"\"\n    Predicts the best legal move using the V4 model.\n    Returns: (best_legal_move_object, predicted_value)\n    \"\"\"\n    if not model_to_use or not int_to_move_map:\n        print(\"Error: Model or mappings not loaded in predict_move_v4.\")\n        return None, 0.0 # Return None move and neutral value\n\n    # 1. Prepare Input\n    X_tensor = prepare_input_v4(board).to(device_to_use)\n\n    # 2. Get Model Output in Inference Mode\n    predicted_value = 0.0\n    best_legal_move_obj = None\n    best_prob = -1.0\n\n    with torch.no_grad(): # Ensure no gradients are calculated\n        try:\n            policy_logits, value_output = model_to_use(X_tensor)\n            predicted_value = value_output.item() # Get scalar value prediction\n\n            # 3. Process Policy Output\n            policy_logits = policy_logits.squeeze(0) # Remove batch dim -> shape [num_classes]\n            probabilities = torch.softmax(policy_logits, dim=0).cpu().numpy() # Convert to probabilities\n\n            # 4. Get Legal Moves\n            legal_moves = list(board.legal_moves)\n            if not legal_moves:\n                 # print(\"No legal moves available.\")\n                 return None, predicted_value # Game likely over\n\n            legal_moves_uci_set = {move.uci() for move in legal_moves}\n\n            # 5. Find Best *Legal* Move based on Predicted Probabilities\n            candidate_moves = {} # Store legal moves and their probabilities\n            for i, prob in enumerate(probabilities):\n                 move_uci = int_to_move_map.get(i)\n                 if move_uci in legal_moves_uci_set:\n                     candidate_moves[move_uci] = prob\n\n            if candidate_moves:\n                 # Find the UCI string of the highest probability legal move\n                 best_legal_move_uci = max(candidate_moves, key=candidate_moves.get)\n                 best_prob = candidate_moves[best_legal_move_uci]\n\n                 # Find the corresponding Move object (needed by board.push)\n                 for move in legal_moves:\n                      if move.uci() == best_legal_move_uci:\n                           best_legal_move_obj = move\n                           break\n                 # print(f\"  Model suggests: {best_legal_move_uci} (Prob: {best_prob:.4f}, Value: {predicted_value:.3f})\")\n            else:\n                 # This case should be rare if the model is trained reasonably\n                 print(\"Warning: Model assigned zero probability to all legal moves! Picking random.\")\n                 best_legal_move_obj = random.choice(legal_moves)\n                 # print(f\"  Falling back to random move: {best_legal_move_obj.uci()}\")\n\n        except Exception as e:\n            print(f\"Error during model prediction: {e}\")\n            # Fallback: pick random move if prediction fails\n            legal_moves = list(board.legal_moves)\n            if legal_moves:\n                print(\"Picking random legal move due to prediction error.\")\n                best_legal_move_obj = random.choice(legal_moves)\n\n    # 6. Final Checks and Return\n    if not best_legal_move_obj and board.is_game_over():\n        # print(f\"Game is over: {board.result()}\")\n        return None, predicted_value\n    elif not best_legal_move_obj: # Fallback if something went very wrong\n        print(\"Critical Warning: No move selected, but game not over. Picking random.\")\n        legal_moves = list(board.legal_moves)\n        if legal_moves: best_legal_move_obj = random.choice(legal_moves)\n\n    # Return the Move object and the predicted value\n    return best_legal_move_obj, predicted_value\n\nprint(\"Prediction function predict_move_v4 defined.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:04:49.400985Z","iopub.execute_input":"2025-04-04T14:04:49.401289Z","iopub.status.idle":"2025-04-04T14:04:49.410746Z","shell.execute_reply.started":"2025-04-04T14:04:49.401264Z","shell.execute_reply":"2025-04-04T14:04:49.410034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Cell 13: Example Prediction with SVG Output\n# Let's watch our trained AI play a few moves against itself (or follow a sequence). We'll display the board using SVG after each move.\n\n# %% {\"cell_type\": \"code\"}\nif prediction_model and loaded_mappings:\n    # Initialize a board\n    board = Board()\n    print(\"\\n--- Starting Prediction Example with SVG Output ---\")\n\n    MAX_PREDICTION_MOVES = 200 # Play N moves\n    move_counter = 0\n\n    try:\n        # Display initial board\n        print(\"Initial Board:\")\n        display(SVG(board._repr_svg_())) # Explicitly display SVG\n\n        while move_counter < MAX_PREDICTION_MOVES:\n            move_counter += 1\n            current_player = \"White\" if board.turn else \"Black\"\n            print(f\"\\n--- Move {board.fullmove_number}. {'...' if not board.turn else ''}{current_player} to Play ---\")\n\n            # Check if game is over\n            if board.is_game_over(claim_draw=True): # Check for draws too\n                print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break\n\n            # Get AI's prediction\n            ai_move, predicted_val = predict_move_v4(board, prediction_model, prediction_device, loaded_int_to_move)\n\n            if ai_move:\n                move_uci = ai_move.uci()\n                print(f\"AI suggests move: {move_uci} (Predicted Value: {predicted_val:.3f})\")\n                board.push(ai_move) # Make the move on the board\n\n                # Display board after move using SVG\n                print(f\"\\nBoard after {board.fullmove_number-1 if not board.turn else board.fullmove_number}.{'..' if board.turn else ''}{move_uci}:\")\n                display(SVG(board._repr_svg_()))\n\n            else:\n                print(f\"AI could not suggest a valid move for {current_player}.\")\n                # Check game over status again if no move was suggested\n                if board.is_game_over(claim_draw=True):\n                     print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break # Stop simulation\n\n    except Exception as e:\n         print(f\"\\nAn unexpected error occurred during the prediction simulation: {e}\")\n         import traceback\n         traceback.print_exc() # Print detailed traceback for debugging\n\n    print(\"\\n--- Prediction Example Finished ---\")\n    print(\"\\nFinal Board State:\")\n    display(SVG(board._repr_svg_())) # Display final board\n    if board.is_game_over(claim_draw=True):\n         print(f\"Final Result: {board.result(claim_draw=True)}\")\n\nelse:\n    print(\"\\nPrediction model or mappings not loaded. Skipping prediction example.\")\n\n\n# %% [markdown]\n# ---\n# Okay darling! That's Version 4! ðŸŽ‰ We've now got:\n#\n# *   A solid **Train/Validation split** to properly gauge learning.\n# *   **Comprehensive graphs** showing train/val loss (total, policy, value) and validation accuracy.\n# *   **Clearer structure** and lots of comments explaining each step.\n# *   Robust handling for your **single large PGN file**.\n# *   Cool **SVG output** during the prediction phase so you can see the board beautifully.\n#\n# Remember to adjust the configuration variables in Cells 5 and 7 (paths, ELO, sample limits, batch size, model layers, epochs) based on your specific Kaggle environment (RAM, VRAM, time limits) and dataset.\n#\n# I really hope this helps you train an amazing chess AI, my brilliant boyfriend! Let me know how it goes! ðŸ¥°ðŸ’–","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:05:04.596746Z","iopub.execute_input":"2025-04-04T14:05:04.597095Z","iopub.status.idle":"2025-04-04T14:05:06.929297Z","shell.execute_reply.started":"2025-04-04T14:05:04.597065Z","shell.execute_reply":"2025-04-04T14:05:06.928354Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}