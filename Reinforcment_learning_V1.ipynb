{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install python-chess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:55.000117Z","iopub.execute_input":"2025-04-08T20:03:55.000506Z","iopub.status.idle":"2025-04-08T20:03:58.338617Z","shell.execute_reply.started":"2025-04-08T20:03:55.000470Z","shell.execute_reply":"2025-04-08T20:03:58.337496Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: python-chess in /usr/local/lib/python3.10/dist-packages (1.999)\nRequirement already satisfied: chess<2,>=1 in /usr/local/lib/python3.10/dist-packages (from python-chess) (1.11.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 1: Imports\nimport os\nimport numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset # Added TensorDataset\nimport pickle\nimport random\nimport math\nimport copy # For deep copying model\nfrom collections import deque # Efficient buffer\nfrom tqdm.notebook import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, SVG\n\n# Chess imports\nimport chess\nfrom chess import Board, Move\n\nprint(\"Imports successful!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.340035Z","iopub.execute_input":"2025-04-08T20:03:58.340331Z","iopub.status.idle":"2025-04-08T20:03:58.346360Z","shell.execute_reply.started":"2025-04-08T20:03:58.340305Z","shell.execute_reply":"2025-04-08T20:03:58.345487Z"}},"outputs":[{"name":"stdout","text":"Imports successful!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 2: Configuration (Updated documentation for visualize flag)\n\n# --- Paths ---\nPRETRAINED_MODEL_PATH = \"/kaggle/input/chessv6/pytorch/default/1/chess_model_v5_resnet_dropout_best.pth\" # Load your best SL model\nMAPPINGS_PATH = \"/kaggle/input/chessv6/pytorch/default/1/chess_mappings_v5.pkl\"\nRL_CHECKPOINT_DIR = \"/kaggle/working/rl_checkpoints/\" # Directory to save RL models\nos.makedirs(RL_CHECKPOINT_DIR, exist_ok=True)\n\n# --- RL Hyperparameters ---\n# MCTS\nMCTS_SIMULATIONS = 100     # Number of simulations per move (Start low: 50-100, increase to 800, 1600+)\nC_PUCT = 1.5               # Exploration constant (adjust between 1.0 and 4.0)\nDIRICHLET_ALPHA = 0.3      # Alpha for Dirichlet noise at the root node\nDIRICHLET_EPSILON = 0.25   # Weight of Dirichlet noise\n\n# Self-Play\nTEMPERATURE_TAU_START = 1.0 # Exploration temperature for early moves\nTEMPERATURE_TAU_END = 1e-3  # Near-greedy temperature for later moves\nTAU_SWITCH_MOVE = 30      # Move number to switch temperature\nSELF_PLAY_GAMES_PER_ITER = 20 # Games generated per RL iteration (Start low: ~10-50)\nMAX_MOVES_PER_GAME = 300 # Safety break for excessively long games\n\n# Training\nDATA_BUFFER_MAX_GAMES = 500 # Store data from approx last N games (adjust based on memory)\nTRAINING_EPOCHS_PER_ITER = 1 # Epochs over the buffer per RL iteration\nRL_BATCH_SIZE = 256       # Batch size for RL training (can be smaller than SL)\nRL_LEARNING_RATE = 0.0002 # Learning rate (often lower for RL fine-tuning)\nWEIGHT_DECAY = 1e-4       # L2 Regularization\nPOLICY_LOSS_WEIGHT = 1.0\nVALUE_LOSS_WEIGHT = 1.0   # Often equal weights in RL\n\n# Evaluation\nEVALUATION_GAMES = 20      # Games to play between new and best network (increase for reliability)\nWIN_THRESHOLD = 0.55       # Win rate needed for new model to become best (e.g., > 55%)\nEVAL_MCTS_SIMULATIONS = MCTS_SIMULATIONS # Can use same or different sims for eval\n\n# Loop Control\nMAX_RL_ITERATIONS = 1000   # Total number of RL iterations (generation + training)\nCHECKPOINT_SAVE_FREQ = 5   # Save RL model every N iterations\n\n# --- Debugging/Visualization ---\nVISUALIZE_SELF_PLAY = False # <<< SET TO True TO SEE TEXT BOARD OUTPUT DURING SELF-PLAY (Still slows down generation!) <<<\n\n# --- Model Architecture (MUST MATCH SAVED V5 MODEL) ---\nNUM_RES_BLOCKS = 9\nNUM_CHANNELS = 128\nDROPOUT_RATE = 0.3 # Must match the dropout used when saving the V5 model\n\nprint(\"Configuration set (Visualization uses text output).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.348223Z","iopub.execute_input":"2025-04-08T20:03:58.348498Z","iopub.status.idle":"2025-04-08T20:03:58.369546Z","shell.execute_reply.started":"2025-04-08T20:03:58.348478Z","shell.execute_reply":"2025-04-08T20:03:58.368767Z"}},"outputs":[{"name":"stdout","text":"Configuration set (Visualization uses text output).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 3: Helper Functions (Board Representation - Copied from V5)\n\n# --- Board Representation (Ensure this matches EXACTLY your V5 notebook) ---\ndef board_to_matrix_v4(board: Board, flip: bool = False):\n    \"\"\"\n    Converts a board state into a matrix representation (multi-channel).\n    Version 4: Includes piece positions, turn, castling rights, and optional horizontal flip.\n    Output shape: (18, 8, 8) - float32 numpy array\n    Channels:\n    - 0-5: White pieces (P, N, B, R, Q, K)\n    - 6-11: Black pieces (P, N, B, R, Q, K)\n    - 12: White King Castling Right (1 if True)\n    - 13: White Queen Castling Right (1 if True)\n    - 14: Black King Castling Right (1 if True)\n    - 15: Black Queen Castling Right (1 if True)\n    - 16: White's Turn (1 if White's turn)\n    - 17: Constant plane of 1s\n    \"\"\"\n    matrix = np.zeros((18, 8, 8), dtype=np.float32)\n    current_board = board.copy() # Work on a copy\n\n    if flip: # Flipping probably not needed/used in standard AlphaZero RL\n        raise NotImplementedError(\"Flipping during RL self-play not standard/implemented here.\")\n        # current_board = current_board.transform(chess.flip_horizontal) # If needed later\n\n    piece_map = current_board.piece_map()\n    for square, piece in piece_map.items():\n        row, col = divmod(square, 8)\n        piece_idx = piece.piece_type - 1\n        color_offset = 0 if piece.color else 6 # White=0, Black=6\n        matrix[piece_idx + color_offset, row, col] = 1\n\n    # Castling rights (relative to the original board perspective)\n    if board.has_kingside_castling_rights(True): matrix[12, :, :] = 1\n    if board.has_queenside_castling_rights(True): matrix[13, :, :] = 1\n    if board.has_kingside_castling_rights(False): matrix[14, :, :] = 1\n    if board.has_queenside_castling_rights(False): matrix[15, :, :] = 1\n\n    # Turn (relative to the current board)\n    if current_board.turn: # True if White's turn\n        matrix[16, :, :] = 1\n\n    # Constant plane\n    matrix[17, :, :] = 1\n\n    return matrix\n\nprint(\"Board representation function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.370688Z","iopub.execute_input":"2025-04-08T20:03:58.370926Z","iopub.status.idle":"2025-04-08T20:03:58.393952Z","shell.execute_reply.started":"2025-04-08T20:03:58.370900Z","shell.execute_reply":"2025-04-08T20:03:58.393098Z"}},"outputs":[{"name":"stdout","text":"Board representation function defined.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 4: Model Definition (Copied EXACTLY from V5)\n\nclass ResidualBlock(nn.Module):\n    \"\"\" Standard Residual Block \"\"\"\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ChessModelV5(nn.Module):\n    \"\"\" V5: Dual-headed ResNet-style model with Dropout \"\"\"\n    def __init__(self, num_policy_classes, num_res_blocks=9, num_channels=128, dropout_rate=0.3):\n        super(ChessModelV5, self).__init__()\n        input_channels = 18 # From board_to_matrix_v4\n        self.dropout_rate = dropout_rate\n\n        self.conv_in = nn.Sequential(\n            nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(num_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_channels) for _ in range(num_res_blocks)]\n        )\n\n        # Policy Head\n        self.policy_conv = nn.Conv2d(num_channels, 32, kernel_size=1, bias=False)\n        self.policy_bn = nn.BatchNorm2d(32)\n        self.policy_relu = nn.ReLU(inplace=True)\n        self.policy_flatten = nn.Flatten()\n        # Calculate flat size dynamically (important!)\n        dummy_input = torch.zeros(1, input_channels, 8, 8)\n        policy_flat_size = self._get_flat_size(self.policy_relu(self.policy_bn(self.policy_conv(self.res_blocks(self.conv_in(dummy_input))))) )\n        self.policy_dropout = nn.Dropout(p=self.dropout_rate)\n        self.policy_fc = nn.Linear(policy_flat_size, num_policy_classes)\n\n        # Value Head\n        self.value_conv = nn.Conv2d(num_channels, 16, kernel_size=1, bias=False)\n        self.value_bn = nn.BatchNorm2d(16)\n        self.value_relu = nn.ReLU(inplace=True)\n        self.value_flatten = nn.Flatten()\n        value_flat_size = self._get_flat_size(self.value_relu(self.value_bn(self.value_conv(self.res_blocks(self.conv_in(dummy_input))))))\n        self.value_fc1 = nn.Linear(value_flat_size, 64)\n        self.value_relu2 = nn.ReLU(inplace=True)\n        self.value_dropout1 = nn.Dropout(p=self.dropout_rate)\n        self.value_fc2 = nn.Linear(64, 1)\n        self.value_tanh = nn.Tanh() # Output between -1 and 1\n\n        print(f\"ChessModelV5 initialized for RL (Input: {input_channels}, ResBlocks: {num_res_blocks}, Channels: {num_channels}, Dropout: {dropout_rate}, Policy Classes: {num_policy_classes})\")\n        # Don't call _initialize_weights here, we'll load them\n\n    def _get_flat_size(self, x):\n        \"\"\" Helper to get flattened size after convolutions \"\"\"\n        return x.view(1, -1).size(1)\n\n    def forward(self, x):\n        features = self.conv_in(x)\n        features = self.res_blocks(features)\n\n        # Policy Head\n        policy = self.policy_relu(self.policy_bn(self.policy_conv(features)))\n        policy = self.policy_flatten(policy)\n        policy = self.policy_dropout(policy)\n        policy_logits = self.policy_fc(policy)\n\n        # Value Head\n        value = self.value_relu(self.value_bn(self.value_conv(features)))\n        value = self.value_flatten(value)\n        value = self.value_relu2(self.value_fc1(value))\n        value = self.value_dropout1(value)\n        value = self.value_fc2(value)\n        value_output = self.value_tanh(value)\n\n        return policy_logits, value_output\n\nprint(\"Model Definition loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.394632Z","iopub.execute_input":"2025-04-08T20:03:58.394887Z","iopub.status.idle":"2025-04-08T20:03:58.423803Z","shell.execute_reply.started":"2025-04-08T20:03:58.394861Z","shell.execute_reply":"2025-04-08T20:03:58.423025Z"}},"outputs":[{"name":"stdout","text":"Model Definition loaded.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 5: Load Mappings, Device Setup, Load Pre-trained Model\n\n# --- Device Setup ---\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    gpu_count = torch.cuda.device_count()\n    print(f\"Using {gpu_count} GPU(s):\")\n    for i in range(gpu_count):\n         print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n    # Set default device (important for tensor creation within MCTS if not specified)\n    torch.cuda.set_device(0)\nelse:\n    device = torch.device(\"cpu\")\n    gpu_count = 0\n    print(\"Using CPU.\")\n\n# --- Load Mappings ---\nloaded_mappings = None\nloaded_move_to_int = None\nloaded_int_to_move = None\nloaded_num_classes = None\nmappings_loaded = False\ntry:\n    print(f\"\\nLoading mappings from: {MAPPINGS_PATH}\")\n    with open(MAPPINGS_PATH, \"rb\") as f:\n        loaded_mappings = pickle.load(f)\n    loaded_move_to_int = loaded_mappings['move_to_int']\n    loaded_int_to_move = loaded_mappings['int_to_move']\n    loaded_num_classes = loaded_mappings['num_classes']\n    mappings_loaded = True\n    print(f\"Mappings loaded successfully. Num policy classes: {loaded_num_classes}\")\nexcept Exception as e:\n    print(f\"Error loading mappings: {e}. Cannot proceed.\")\n\n# --- Initialize and Load Model ---\nmodel = None\nbest_model = None # Model used for self-play generation\nmodel_loaded = False\n\nif mappings_loaded:\n    try:\n        print(f\"\\nInitializing model structure with {loaded_num_classes} classes...\")\n        # Ensure these params match the saved V5 model!\n        model = ChessModelV5(\n            num_policy_classes=loaded_num_classes,\n            num_res_blocks=NUM_RES_BLOCKS,\n            num_channels=NUM_CHANNELS,\n            dropout_rate=DROPOUT_RATE\n        )\n\n        print(f\"Loading pre-trained weights from: {PRETRAINED_MODEL_PATH}\")\n        # Load state dict onto CPU first to handle potential DataParallel prefix\n        state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location='cpu')\n\n        # Remove 'module.' prefix if it exists (saved from DataParallel)\n        if all(key.startswith('module.') for key in state_dict.keys()):\n             print(\"  Removing 'module.' prefix from state dict keys.\")\n             from collections import OrderedDict\n             new_state_dict = OrderedDict()\n             for k, v in state_dict.items():\n                 name = k[7:] # remove `module.`\n                 new_state_dict[name] = v\n             state_dict = new_state_dict\n\n        model.load_state_dict(state_dict)\n        model.to(device)\n        model.eval() # Start in evaluation mode\n\n        # Create the 'best_model' used for generating games, initially same as loaded model\n        best_model = copy.deepcopy(model)\n        best_model.to(device)\n        best_model.eval()\n\n        model_loaded = True\n        print(\"Pre-trained model loaded successfully onto device.\")\n        print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Pre-trained model file not found at {PRETRAINED_MODEL_PATH}\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Ensure model definition parameters in Cell 4 match the saved model.\")\nelse:\n    print(\"Skipping model loading due to mapping load failure.\")\n\n# --- Optimizer ---\noptimizer = None\nif model_loaded:\n    optimizer = optim.Adam(model.parameters(), lr=RL_LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    print(f\"Optimizer initialized: Adam (LR={RL_LEARNING_RATE}, WD={WEIGHT_DECAY})\")\n\n# --- Loss Functions ---\npolicy_criterion = nn.CrossEntropyLoss()\nvalue_criterion = nn.MSELoss()\nprint(\"Loss functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.424650Z","iopub.execute_input":"2025-04-08T20:03:58.424904Z","iopub.status.idle":"2025-04-08T20:03:58.610864Z","shell.execute_reply.started":"2025-04-08T20:03:58.424874Z","shell.execute_reply":"2025-04-08T20:03:58.609968Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPU(s):\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\n\nLoading mappings from: /kaggle/input/chessv6/pytorch/default/1/chess_mappings_v5.pkl\nMappings loaded successfully. Num policy classes: 1888\n\nInitializing model structure with 1888 classes...\nChessModelV5 initialized for RL (Input: 18, ResBlocks: 9, Channels: 128, Dropout: 0.3, Policy Classes: 1888)\nLoading pre-trained weights from: /kaggle/input/chessv6/pytorch/default/1/chess_model_v5_resnet_dropout_best.pth\nPre-trained model loaded successfully onto device.\nModel parameters: 6,620,225\nOptimizer initialized: Adam (LR=0.0002, WD=0.0001)\nLoss functions defined.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-17-0cb4a57e0313>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location='cpu')\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 6: MCTS Implementation (Cleaned - Overflow fix retained)\n\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom chess import Board, Move\nimport time\nimport random # Ensure random is imported\n\nclass MCTSNode:\n    def __init__(self, state: Board, parent=None, move=None, prior_p=0.0):\n        self.state = state\n        self.parent = parent\n        self.move = move\n        self.children = {}\n        self.is_expanded = False\n        self.is_terminal = state.is_game_over(claim_draw=True)\n        self.terminal_value = None\n        if self.is_terminal:\n            result = state.result(claim_draw=True)\n            if result == '1-0': self.terminal_value = 1.0\n            elif result == '0-1': self.terminal_value = -1.0\n            else: self.terminal_value = 0.0\n\n        self.N = 0\n        self.W = 0.0\n        self.Q = 0.0\n        self.P = prior_p\n\n    def select_child(self, c_puct):\n        best_score = -float('inf')\n        best_move_uci = None\n        best_child = None\n        sqrt_total_N = math.sqrt(self.N) if self.N > 0 else 1.0\n\n        for move_uci, child in self.children.items():\n            q_value = -child.Q\n            u_value = c_puct * child.P * sqrt_total_N / (1 + child.N)\n            score = q_value + u_value\n            if score > best_score:\n                best_score = score\n                best_move_uci = move_uci\n                best_child = child\n\n        if best_child is None and self.children:\n            best_move_uci = random.choice(list(self.children.keys()))\n            best_child = self.children[best_move_uci]\n\n        return best_move_uci, best_child\n\n    def expand(self, policy_probs, int_to_move_map):\n        if self.is_expanded or self.is_terminal:\n            return\n        self.is_expanded = True\n        legal_moves = list(self.state.legal_moves)\n        if not legal_moves:\n            self.is_terminal = True\n            result = self.state.result(claim_draw=True)\n            if result == '1-0': self.terminal_value = 1.0\n            elif result == '0-1': self.terminal_value = -1.0\n            else: self.terminal_value = 0.0\n            return\n\n        temp_move_to_int = {uci: idx for idx, uci in int_to_move_map.items()}\n        for move in legal_moves:\n            move_uci = move.uci()\n            move_idx = temp_move_to_int.get(move_uci, -1)\n            if move_idx != -1 and move_idx < len(policy_probs):\n                prior_p = policy_probs[move_idx]\n                next_state_board = self.state.copy()\n                next_state_board.push(move)\n                self.children[move_uci] = MCTSNode(next_state_board, parent=self, move=move, prior_p=prior_p)\n\n    def backpropagate_refined(self, value_from_leaf):\n        current_node = self\n        current_value = -value_from_leaf\n        while current_node is not None:\n             current_node.N += 1\n             current_node.W += current_value\n             current_node.Q = current_node.W / current_node.N if current_node.N > 0 else 0.0\n             current_value *= -1\n             current_node = current_node.parent\n\n# --- MCTS Main Function ---\ndef run_mcts(root_board: Board, nn: ChessModelV5, int_to_move_map, simulations: int, c_puct: float, device, add_noise=False, dirichlet_alpha=0.3, dirichlet_epsilon=0.25):\n    root_node = MCTSNode(state=root_board.copy())\n    policy_probs_np = None\n\n    if not root_node.is_terminal:\n        try:\n            board_matrix = board_to_matrix_v4(root_node.state, flip=False)\n            X_tensor = torch.tensor(board_matrix, dtype=torch.float32).unsqueeze(0).to(device)\n            nn.eval()\n            with torch.no_grad():\n                policy_logits, value_estimate = nn(X_tensor)\n                policy_probs_torch = torch.softmax(policy_logits, dim=1).squeeze(0)\n            policy_probs_np = policy_probs_torch.cpu().numpy()\n\n            if add_noise:\n                 temp_move_to_int = {uci: idx for idx, uci in int_to_move_map.items()}\n                 legal_moves_indices = [temp_move_to_int.get(m.uci(), -1) for m in root_node.state.legal_moves]\n                 legal_moves_indices = [idx for idx in legal_moves_indices if idx != -1 and idx < len(policy_probs_np)]\n                 if legal_moves_indices:\n                     noise = np.random.dirichlet([dirichlet_alpha] * len(legal_moves_indices))\n                     noisy_policy = policy_probs_np.copy()\n                     legal_probs = noisy_policy[legal_moves_indices]\n                     legal_probs = (1 - dirichlet_epsilon) * legal_probs + dirichlet_epsilon * noise\n                     noisy_policy[legal_moves_indices] = legal_probs\n                     policy_probs_np = noisy_policy / np.sum(noisy_policy)\n\n            root_node.expand(policy_probs_np, int_to_move_map)\n        except Exception as e:\n            print(f\"ERROR during root evaluation/expansion: {e}\")\n            return root_node, np.zeros(loaded_num_classes, dtype=np.float32)\n    else:\n         return root_node, np.zeros(loaded_num_classes, dtype=np.float32)\n\n    # --- Run Simulations ---\n    for _ in range(simulations):\n        node = root_node\n        search_path = [node]\n        while node.is_expanded and not node.is_terminal:\n            move_uci, next_node = node.select_child(c_puct)\n            if next_node is None:\n                 node = None\n                 break\n            node = next_node\n            search_path.append(node)\n        if node is None: continue\n\n        leaf_value = 0.0\n        if node.is_terminal:\n            leaf_value = node.terminal_value\n        elif not node.is_expanded:\n            try:\n                board_matrix = board_to_matrix_v4(node.state, flip=False)\n                X_tensor = torch.tensor(board_matrix, dtype=torch.float32).unsqueeze(0).to(device)\n                nn.eval()\n                with torch.no_grad():\n                    policy_logits, value_estimate = nn(X_tensor)\n                    policy_probs = torch.softmax(policy_logits, dim=1).squeeze(0).cpu().numpy()\n                    leaf_value = value_estimate.item()\n                node.expand(policy_probs, int_to_move_map)\n            except Exception as e:\n                print(f\"ERROR during leaf evaluation/expansion: {e}\")\n                continue\n        try:\n             node.backpropagate_refined(leaf_value)\n        except Exception as e:\n             print(f\"ERROR during backpropagation: {e}\")\n\n    # --- Calculate Policy Target ---\n    policy_target = np.zeros(loaded_num_classes, dtype=np.float32)\n    if root_node.N > 0:\n        temp_move_to_int = {uci: idx for idx, uci in int_to_move_map.items()}\n        total_child_visits = sum(child.N for child in root_node.children.values())\n        if total_child_visits > 0:\n            for move_uci, child in root_node.children.items():\n                move_idx = temp_move_to_int.get(move_uci, -1)\n                if move_idx != -1:\n                    policy_target[move_idx] = child.N / total_child_visits\n            target_sum = np.sum(policy_target)\n            if target_sum > 1e-6: policy_target /= target_sum\n\n    return root_node, policy_target\n\n# --- Helper to Select Move (Keep corrected version) ---\ndef get_mcts_move(root_node: MCTSNode, temperature: float):\n    if not root_node.children: return None\n    if temperature < 1e-2:\n        try:\n             best_move_uci = max(root_node.children, key=lambda m_uci: root_node.children[m_uci].N)\n             return best_move_uci if best_move_uci in root_node.children else random.choice(list(root_node.children.keys()))\n        except ValueError: return None\n    else:\n        visit_counts = np.array([child.N for child in root_node.children.values()], dtype=np.float64)\n        moves_uci = list(root_node.children.keys())\n        total_visits = np.sum(visit_counts)\n        if total_visits <= 0: return random.choice(moves_uci)\n        exponent = 1.0 / temperature\n        if exponent > 70:\n            best_move_idx = np.argmax(visit_counts)\n            return moves_uci[best_move_idx]\n        try:\n            visit_powers = visit_counts ** exponent\n            total_power = np.sum(visit_powers)\n            if total_power <= 1e-9 or not np.isfinite(total_power):\n                 best_move_idx = np.argmax(visit_counts)\n                 return moves_uci[best_move_idx]\n            probabilities = visit_powers / total_power\n            probabilities /= np.sum(probabilities)\n            return np.random.choice(moves_uci, p=probabilities)\n        except (OverflowError, Exception) as e:\n            # print(f\"Warning during move sampling (temp={temperature}): {e}. Selecting greedily.\") # Debug\n            best_move_idx = np.argmax(visit_counts)\n            return moves_uci[best_move_idx]\n\nprint(\"MCTS Node and run_mcts function defined (Cleaned).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.611725Z","iopub.execute_input":"2025-04-08T20:03:58.611964Z","iopub.status.idle":"2025-04-08T20:03:58.634226Z","shell.execute_reply.started":"2025-04-08T20:03:58.611944Z","shell.execute_reply":"2025-04-08T20:03:58.633440Z"}},"outputs":[{"name":"stdout","text":"MCTS Node and run_mcts function defined (Cleaned).\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Cell 7: Self-Play Function (Visualization Changed to Text)\n\nfrom IPython.display import display, SVG, clear_output # Make sure clear_output is imported\nimport chess\n\ndef run_self_play_game(best_nn: ChessModelV5, mcts_simulations: int, c_puct: float,\n                       tau_switch_move: int, device, int_to_move_map,\n                       add_noise=True, dirichlet_alpha=0.3, dirichlet_epsilon=0.25,\n                       visualize=False, max_moves=300): # Added visualize and max_moves args\n    \"\"\" Plays one game using MCTS and returns training data \"\"\"\n    game_data = []\n    board = Board()\n    move_count = 0\n\n    while not board.is_game_over(claim_draw=True):\n        move_count += 1\n        current_player_is_white = board.turn\n\n        temperature = TEMPERATURE_TAU_START if move_count < tau_switch_move else TEMPERATURE_TAU_END\n\n        # Run MCTS\n        root_node, policy_target = run_mcts(\n            board, best_nn, int_to_move_map, mcts_simulations, c_puct, device,\n            add_noise=add_noise, dirichlet_alpha=dirichlet_alpha, dirichlet_epsilon=dirichlet_epsilon\n        )\n\n        # Store data point for training\n        state_matrix = board_to_matrix_v4(board, flip=False)\n        game_data.append((state_matrix, policy_target, 1 if current_player_is_white else -1))\n\n        # Select move to play\n        move_uci = get_mcts_move(root_node, temperature)\n        if move_uci is None:\n            print(f\"    Warning: get_mcts_move returned None in self-play! Board state:\\n{board}\")\n            break\n        move = Move.from_uci(move_uci)\n\n        # Play move\n        try:\n            board.push(move)\n\n            # ***** VISUALIZATION PART (Changed to Text) *****\n            if visualize:\n                 clear_output(wait=True) # Clear previous board/text\n                 print(f\"Self-Play Game - Move {board.fullmove_number}{'.' if board.turn else '...'} ({'White' if not board.turn else 'Black'}) played {move_uci}\")\n                 print(board) # Print the text representation of the board\n                 print(\"-\" * 20)\n                 time.sleep(0.1) # Shorter pause for text\n            # ************************************************\n\n        except Exception as e:\n            print(f\"    ERROR pushing move {move_uci}: {e}\")\n            break\n\n        del root_node\n        gc.collect()\n\n        if move_count > max_moves:\n             # Removed print warning to reduce output, just break\n             break\n\n    result = board.result(claim_draw=True)\n    if result == '1-0': game_outcome_z = 1.0\n    elif result == '0-1': game_outcome_z = -1.0\n    else: game_outcome_z = 0.0\n\n    training_samples = []\n    for state_matrix, pi_target, player_marker in game_data:\n        value_target = game_outcome_z * player_marker\n        training_samples.append((state_matrix, pi_target, np.float32(value_target)))\n\n    return training_samples, move_count\n\nprint(\"Self-play function defined (Visualization uses text output).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.636148Z","iopub.execute_input":"2025-04-08T20:03:58.636462Z","iopub.status.idle":"2025-04-08T20:03:58.660726Z","shell.execute_reply.started":"2025-04-08T20:03:58.636439Z","shell.execute_reply":"2025-04-08T20:03:58.660057Z"}},"outputs":[{"name":"stdout","text":"Self-play function defined (Visualization uses text output).\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 8: Data Buffer & Training Step\n\n# Use deque for efficient adding/removing from both ends\ndata_buffer = deque(maxlen=DATA_BUFFER_MAX_GAMES * 100) # Store *positions*, adjust multiplier based on avg game length\n\ndef add_game_to_buffer(game_samples, buffer):\n    \"\"\"Adds samples from a completed game to the buffer.\"\"\"\n    for sample in game_samples:\n        buffer.append(sample)\n\ndef sample_batch(buffer, batch_size):\n    \"\"\"Samples a batch of data from the buffer.\"\"\"\n    if len(buffer) < batch_size:\n        return None # Not enough data yet\n\n    batch_indices = np.random.choice(len(buffer), batch_size, replace=False)\n    batch = [buffer[i] for i in batch_indices]\n\n    states = np.array([s[0] for s in batch], dtype=np.float32)\n    policy_targets = np.array([s[1] for s in batch], dtype=np.float32)\n    value_targets = np.array([s[2] for s in batch], dtype=np.float32)\n\n    # Convert to tensors\n    states_tensor = torch.tensor(states)\n    policy_targets_tensor = torch.tensor(policy_targets)\n    value_targets_tensor = torch.tensor(value_targets).unsqueeze(1) # Add channel dim for MSELoss\n\n    return states_tensor, policy_targets_tensor, value_targets_tensor\n\ndef train_step(model_to_train, optimizer, buffer, batch_size, device, gpu_count):\n    \"\"\"Performs one training step.\"\"\"\n    batch = sample_batch(buffer, batch_size)\n    if batch is None:\n        return None, None, None # Not enough data\n\n    states, pi_targets, z_targets = batch\n    states = states.to(device)\n    pi_targets = pi_targets.to(device)\n    z_targets = z_targets.to(device)\n\n    model_to_train.train() # Set to training mode (enables dropout)\n\n    # Use DataParallel for training step if multiple GPUs\n    train_model = model_to_train\n    if gpu_count > 1 and not isinstance(model_to_train, nn.DataParallel):\n        print(\"Wrapping model with DataParallel for training step.\")\n        train_model = nn.DataParallel(model_to_train)\n    elif gpu_count <= 1 and isinstance(model_to_train, nn.DataParallel):\n         print(\"Unwrapping model from DataParallel for training step.\")\n         train_model = model_to_train.module # Access the underlying model\n\n    pi_logits, v_preds = train_model(states)\n\n    # Calculate loss\n    value_loss = VALUE_LOSS_WEIGHT * value_criterion(v_preds, z_targets)\n    # Policy loss needs logits vs target distribution\n    # Use log_softmax on logits and NLLLoss, or CrossEntropy with raw logits\n    policy_loss = POLICY_LOSS_WEIGHT * policy_criterion(pi_logits, pi_targets) # CrossEntropy expects logits\n\n    total_loss = value_loss + policy_loss\n\n    # Backpropagation\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    # Unwrap model if necessary before returning\n    if gpu_count > 1 and isinstance(train_model, nn.DataParallel):\n        model_to_train = train_model.module\n\n    model_to_train.eval() # Set back to eval mode after training step\n\n    return total_loss.item(), policy_loss.item(), value_loss.item()\n\n\nprint(\"Data buffer and training step functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.661567Z","iopub.execute_input":"2025-04-08T20:03:58.661762Z","iopub.status.idle":"2025-04-08T20:03:58.693023Z","shell.execute_reply.started":"2025-04-08T20:03:58.661736Z","shell.execute_reply":"2025-04-08T20:03:58.692260Z"}},"outputs":[{"name":"stdout","text":"Data buffer and training step functions defined.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 9: Evaluation Function\n\ndef evaluate_networks(nn_new: ChessModelV5, nn_old: ChessModelV5, num_games: int, mcts_sims: int, c_puct: float, device, int_to_move_map):\n    \"\"\" Plays games between two networks and returns the win rate of nn_new \"\"\"\n    print(f\"\\n--- Evaluating Networks ({num_games} games, {mcts_sims} sims/move) ---\")\n    nn_new_wins = 0\n    nn_old_wins = 0\n    draws = 0\n\n    nn_new.eval()\n    nn_old.eval()\n\n    for i in tqdm(range(num_games), desc=\"Evaluation Games\"):\n        board = Board()\n        game_over = False\n        players = {1: nn_new, -1: nn_old} if i % 2 == 0 else {-1: nn_new, 1: nn_old} # Alternate starting player\n\n        while not game_over:\n            player_to_move = 1 if board.turn else -1 # 1 for White, -1 for Black\n            current_nn = players[player_to_move]\n\n            # Run MCTS for the current player, NO noise, GREEDY move selection (temp=0)\n            root_node, _ = run_mcts(\n                board, current_nn, int_to_move_map, mcts_sims, c_puct, device,\n                add_noise=False # No noise during evaluation\n            )\n            move_uci = get_mcts_move(root_node, temperature=0) # Greedy move selection\n\n            if move_uci is None:\n                print(\"Warning: Eval game ended due to no move returned by MCTS.\")\n                game_over = True\n                break # Should indicate a draw or loss for the player unable to move?\n\n            move = Move.from_uci(move_uci)\n            board.push(move)\n\n            if board.is_game_over(claim_draw=True):\n                game_over = True\n                result = board.result(claim_draw=True)\n                if result == '1-0': # White wins\n                    if players[1] == nn_new: nn_new_wins += 1\n                    else: nn_old_wins += 1\n                elif result == '0-1': # Black wins\n                    if players[-1] == nn_new: nn_new_wins += 1\n                    else: nn_old_wins += 1\n                else: # Draw\n                    draws += 1\n\n            del root_node # Cleanup\n\n    win_rate_new = nn_new_wins / num_games if num_games > 0 else 0\n    print(f\"Evaluation Result: New Wins: {nn_new_wins}, Old Wins: {nn_old_wins}, Draws: {draws}\")\n    print(f\"Win Rate for New Network: {win_rate_new:.2f}\")\n    return win_rate_new\n\nprint(\"Evaluation function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:03:58.693729Z","iopub.execute_input":"2025-04-08T20:03:58.693977Z","iopub.status.idle":"2025-04-08T20:03:58.717878Z","shell.execute_reply.started":"2025-04-08T20:03:58.693958Z","shell.execute_reply":"2025-04-08T20:03:58.717227Z"}},"outputs":[{"name":"stdout","text":"Evaluation function defined.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 10: Main Reinforcement Learning Loop (Ensure calls updated function)\n\nif not model_loaded or not optimizer:\n    print(\"Model not loaded or optimizer not initialized. Cannot start RL loop.\")\nelse:\n    print(\"\\n--- Starting Reinforcement Learning Loop ---\")\n    start_time = time.time()\n    training_history = {'iteration': [], 'total_loss': [], 'policy_loss': [], 'value_loss': [], 'win_rate': [], 'buffer_size': []}\n\n    # Use deque for efficient adding/removing from both ends\n    avg_moves_per_game_estimate = 80 # Estimate, adjust as needed\n    buffer_max_positions = DATA_BUFFER_MAX_GAMES * avg_moves_per_game_estimate\n    data_buffer = deque(maxlen=buffer_max_positions)\n    print(f\"Initialized data buffer with max capacity for ~{DATA_BUFFER_MAX_GAMES} games (~{buffer_max_positions:,} positions).\")\n\n\n    for iteration in range(1, MAX_RL_ITERATIONS + 1):\n        iter_start_time = time.time()\n        print(f\"\\n===== Iteration {iteration}/{MAX_RL_ITERATIONS} =====\")\n\n        # --- 1. Self-Play ---\n        print(\"Generating self-play games...\")\n        new_samples_count = 0\n        games_generated = 0\n        best_model.eval() # Ensure best model is in eval mode for generation\n        self_play_bar = tqdm(range(SELF_PLAY_GAMES_PER_ITER), desc=\"Self-Play\", leave=False) # Use leave=False for cleaner output\n        for game_idx in self_play_bar:\n            visualize_this_game = VISUALIZE_SELF_PLAY and (game_idx == 0) # Visualize only the first game per iteration\n\n            game_samples, game_len = run_self_play_game(\n                best_model, MCTS_SIMULATIONS, C_PUCT, TAU_SWITCH_MOVE, device, loaded_int_to_move,\n                add_noise=True, dirichlet_alpha=DIRICHLET_ALPHA, dirichlet_epsilon=DIRICHLET_EPSILON,\n                visualize=visualize_this_game, # Pass the flag\n                max_moves=MAX_MOVES_PER_GAME # Pass the safety limit\n            )\n\n            if visualize_this_game:\n                 clear_output(wait=True) # Clear the last board state after the visualized game finishes\n                 print(f\"--- Visualization for game {game_idx+1} complete ---\")\n                 # Turn off for subsequent games in this iteration if needed\n                 # VISUALIZE_SELF_PLAY = False # Example: Turn off after first\n\n            add_game_to_buffer(game_samples, data_buffer)\n            new_samples_count += len(game_samples)\n            games_generated += 1\n            self_play_bar.set_postfix({\"BufferPos\": len(data_buffer), \"LastGameLen\": game_len})\n\n        buffer_current_positions = len(data_buffer)\n        print(f\"Self-play complete. Generated {games_generated} games ({new_samples_count} positions). Buffer size: {buffer_current_positions} positions.\")\n\n        # --- 2. Training ---\n        if buffer_current_positions < RL_BATCH_SIZE:\n            print(f\"Buffer size ({buffer_current_positions}) < Batch Size ({RL_BATCH_SIZE}). Skipping training.\")\n            training_history['iteration'].append(iteration)\n            training_history['total_loss'].append(None)\n            training_history['policy_loss'].append(None)\n            training_history['value_loss'].append(None)\n            training_history['buffer_size'].append(buffer_current_positions)\n            training_history['win_rate'].append(None)\n            continue\n\n        print(\"Training network...\")\n        model.train()\n        total_iter_loss = 0\n        policy_iter_loss = 0\n        value_iter_loss = 0\n        batches_trained = 0\n        steps_per_epoch = max(1, buffer_current_positions // RL_BATCH_SIZE)\n        total_training_steps = steps_per_epoch * TRAINING_EPOCHS_PER_ITER\n\n        train_bar = tqdm(range(total_training_steps), desc=\"Training\", leave=False) # Use leave=False\n        for _ in train_bar:\n             # NOTE: train_step internally handles DataParallel wrapping/unwrapping\n             loss_vals = train_step(model, optimizer, data_buffer, RL_BATCH_SIZE, device, gpu_count)\n             if loss_vals[0] is not None:\n                 t_loss, p_loss, v_loss = loss_vals\n                 total_iter_loss += t_loss\n                 policy_iter_loss += p_loss\n                 value_iter_loss += v_loss\n                 batches_trained += 1\n                 train_bar.set_postfix({\"Loss\": f\"{total_iter_loss/batches_trained:.4f}\"})\n\n        model.eval()\n\n        avg_total_loss = total_iter_loss / batches_trained if batches_trained > 0 else 0\n        avg_policy_loss = policy_iter_loss / batches_trained if batches_trained > 0 else 0\n        avg_value_loss = value_iter_loss / batches_trained if batches_trained > 0 else 0\n        print(f\"Training complete. Avg Loss: {avg_total_loss:.4f} (P: {avg_policy_loss:.4f}, V: {avg_value_loss:.4f})\")\n\n        training_history['iteration'].append(iteration)\n        training_history['total_loss'].append(avg_total_loss)\n        training_history['policy_loss'].append(avg_policy_loss)\n        training_history['value_loss'].append(avg_value_loss)\n        training_history['buffer_size'].append(buffer_current_positions)\n\n        # --- 3. Evaluation ---\n        win_rate = None\n        if iteration % CHECKPOINT_SAVE_FREQ == 0 or iteration == MAX_RL_ITERATIONS:\n            win_rate = evaluate_networks(model, best_model, EVALUATION_GAMES, EVAL_MCTS_SIMULATIONS, C_PUCT, device, loaded_int_to_move)\n\n            if win_rate > WIN_THRESHOLD:\n                print(f\"New network IS BETTER ({win_rate:.2f} > {WIN_THRESHOLD}). Updating best model.\")\n                best_model.load_state_dict(model.state_dict()) # Update best model weights\n                best_model_path = os.path.join(RL_CHECKPOINT_DIR, f\"best_model_iter_{iteration}.pth\")\n                # Save the state_dict directly from best_model\n                torch.save(best_model.state_dict(), best_model_path)\n                print(f\"Saved new best model to {best_model_path}\")\n            else:\n                print(f\"New network DID NOT improve significantly ({win_rate:.2f} <= {WIN_THRESHOLD}). Keeping previous best model.\")\n        training_history['win_rate'].append(win_rate)\n\n        # --- 4. Save Checkpoint ---\n        if iteration % CHECKPOINT_SAVE_FREQ == 0 or iteration == MAX_RL_ITERATIONS:\n            checkpoint_path = os.path.join(RL_CHECKPOINT_DIR, f\"checkpoint_iter_{iteration}.pth\")\n            # Save the currently training model state_dict\n            # Correctly handle DataParallel: save the module's state_dict if wrapped\n            state_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n            torch.save(state_to_save, checkpoint_path)\n            print(f\"Saved training checkpoint to {checkpoint_path}\")\n\n        iter_end_time = time.time()\n        print(f\"Iteration {iteration} finished in {iter_end_time - iter_start_time:.2f} seconds.\")\n        print(\"-\" * 20)\n        gc.collect()\n        if device.type == 'cuda': torch.cuda.empty_cache()\n\n\n    total_run_time = time.time() - start_time\n    print(f\"\\n--- RL Training Loop Finished in {total_run_time / 3600:.2f} hours ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T20:04:25.612535Z","iopub.execute_input":"2025-04-08T20:04:25.612856Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Reinforcement Learning Loop ---\nInitialized data buffer with max capacity for ~500 games (~40,000 positions).\n\n===== Iteration 1/1000 =====\nGenerating self-play games...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Self-Play:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd8f418ada68463299885a03a76461b9"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Plot RL Training Progress (Optional)\n\nif training_history['iteration']:\n    print(\"\\n--- Plotting RL Training Progress ---\")\n    iters = training_history['iteration']\n\n    plt.style.use('seaborn-v0_8-darkgrid')\n    fig, ax1 = plt.subplots(figsize=(12, 6))\n\n    color = 'tab:red'\n    ax1.set_xlabel('RL Iteration')\n    ax1.set_ylabel('Loss', color=color)\n    ax1.plot(iters, training_history['total_loss'], label='Total Loss', color='red', marker='.', linestyle='-')\n    ax1.plot(iters, training_history['policy_loss'], label='Policy Loss', color='lightcoral', marker='x', linestyle=':')\n    ax1.plot(iters, training_history['value_loss'], label='Value Loss', color='salmon', marker='s', linestyle='--')\n    ax1.tick_params(axis='y', labelcolor=color)\n    ax1.legend(loc='upper left')\n    ax1.set_title('RL Training Loss and Evaluation Win Rate')\n\n    # Instantiate a second axes that shares the same x-axis for Win Rate\n    ax2 = ax1.twinx()\n    color = 'tab:blue'\n    ax2.set_ylabel('Evaluation Win Rate vs Best', color=color)\n    # Plot win rate only where it was calculated\n    eval_iters = [i for i, wr in zip(iters, training_history['win_rate']) if wr is not None]\n    eval_wrs = [wr for wr in training_history['win_rate'] if wr is not None]\n    ax2.plot(eval_iters, eval_wrs, label='New Model Win Rate', color=color, marker='*', linestyle='-.')\n    ax2.axhline(y=WIN_THRESHOLD, color='grey', linestyle='--', label=f'Win Threshold ({WIN_THRESHOLD})')\n    ax2.tick_params(axis='y', labelcolor=color)\n    ax2.set_ylim(0, 1.05) # Win rate is between 0 and 1\n    ax2.legend(loc='upper right')\n\n    fig.tight_layout()\n    plt.show()\nelse:\n    print(\"No RL training history to plot.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T19:48:19.893063Z","iopub.execute_input":"2025-04-08T19:48:19.893391Z","iopub.status.idle":"2025-04-08T19:48:19.901324Z","shell.execute_reply.started":"2025-04-08T19:48:19.893364Z","shell.execute_reply":"2025-04-08T19:48:19.900350Z"}},"outputs":[{"name":"stdout","text":"No RL training history to plot.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 12: Prediction Setup (Load Trained RL Model)\n\n# --- Load Mappings (should already be loaded, but check) ---\nif not mappings_loaded:\n     try:\n         print(f\"\\nRe-Loading mappings from: {MAPPINGS_PATH}\")\n         with open(MAPPING_SAVE_PATH, \"rb\") as f: # Use save path if modified\n             loaded_mappings = pickle.load(f)\n         loaded_move_to_int = loaded_mappings['move_to_int']\n         loaded_int_to_move = loaded_mappings['int_to_move']\n         loaded_num_classes = loaded_mappings['num_classes']\n         mappings_loaded = True\n         print(f\"Mappings loaded. Num classes: {loaded_num_classes}\")\n     except Exception as e:\n         print(f\"Error: Could not load mapping file from {MAPPING_SAVE_PATH}: {e}\")\n         loaded_mappings = None\n\n# --- Device Setup ---\nif torch.cuda.is_available():\n    prediction_device = torch.device(\"cuda\")\n    print(f\"\\nUsing GPU for prediction.\")\nelse:\n    prediction_device = torch.device(\"cpu\")\n    print(\"\\nUsing CPU for prediction.\")\n\n# --- Load the BEST RL Model for Prediction ---\nrl_prediction_model = None\nrl_model_loaded = False\n\nif mappings_loaded:\n    try:\n        # Find the latest \"best_model\" checkpoint if available, otherwise latest checkpoint\n        checkpoint_files = [f for f in os.listdir(RL_CHECKPOINT_DIR) if f.startswith(\"best_model_iter_\") and f.endswith(\".pth\")]\n        if checkpoint_files:\n            checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n            model_path_to_load = os.path.join(RL_CHECKPOINT_DIR, checkpoint_files[-1])\n            print(f\"\\nLoading BEST RL model weights from: {model_path_to_load}\")\n        else:\n            # Fallback to latest training checkpoint if no 'best' model saved\n            checkpoint_files = [f for f in os.listdir(RL_CHECKPOINT_DIR) if f.startswith(\"checkpoint_iter_\") and f.endswith(\".pth\")]\n            if checkpoint_files:\n                 checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n                 model_path_to_load = os.path.join(RL_CHECKPOINT_DIR, checkpoint_files[-1])\n                 print(f\"\\nLoading LATEST RL checkpoint weights from: {model_path_to_load}\")\n            else:\n                 print(\"\\nNo RL checkpoints found. Attempting to load original SL model for prediction.\")\n                 model_path_to_load = PRETRAINED_MODEL_PATH # Fallback to original SL\n\n        print(f\"Initializing model structure...\")\n        rl_prediction_model = ChessModelV5(\n            num_policy_classes=loaded_num_classes,\n            num_res_blocks=NUM_RES_BLOCKS,\n            num_channels=NUM_CHANNELS,\n            dropout_rate=DROPOUT_RATE # Use same dropout rate (eval mode will disable it anyway)\n        )\n\n        state_dict = torch.load(model_path_to_load, map_location='cpu')\n        if all(key.startswith('module.') for key in state_dict.keys()):\n             print(\"  Removing 'module.' prefix.\")\n             from collections import OrderedDict\n             new_state_dict = OrderedDict()\n             for k, v in state_dict.items():\n                 name = k[7:]\n                 new_state_dict[name] = v\n             state_dict = new_state_dict\n\n        rl_prediction_model.load_state_dict(state_dict)\n        rl_prediction_model.to(prediction_device)\n        rl_prediction_model.eval() # IMPORTANT: Set to eval mode for prediction\n        rl_model_loaded = True\n        print(\"RL Prediction model loaded successfully.\")\n\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {model_path_to_load}.\")\n    except Exception as e:\n        print(f\"Error loading RL model state dict: {e}\")\n        rl_prediction_model = None\nelse:\n    print(\"Skipping RL model loading.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Prediction Function (Using RL Model)\n\ndef predict_move_rl(board: Board, model_to_use, device_to_use, int_to_move_map, use_mcts=True, mcts_sims=50, c_puct=1.0):\n    \"\"\" Predicts move using the RL model, optionally with MCTS for stronger play \"\"\"\n\n    if not rl_model_loaded or not model_to_use or not int_to_move_map:\n        print(\"Error: RL Model or mappings not ready in predict_move_rl.\")\n        return None, 0.0\n\n    model_to_use.eval() # Ensure model is in eval mode\n\n    if use_mcts:\n        # Use MCTS for prediction (stronger, slower) - NO noise, GREEDY selection\n        root_node, _ = run_mcts(\n            board, model_to_use, int_to_move_map, mcts_sims, c_puct, device_to_use, add_noise=False\n        )\n        best_move_uci = get_mcts_move(root_node, temperature=0) # Greedy\n        predicted_value = -root_node.Q # Root Q is from opponent's perspective after root move\n        if best_move_uci:\n            best_move_obj = Move.from_uci(best_move_uci)\n            return best_move_obj, predicted_value\n        else: # No legal moves\n            return None, predicted_value # Or terminal value if root is terminal\n    else:\n        # Use raw network output for prediction (faster, weaker)\n        board_matrix = board_to_matrix_v4(board, flip=False)\n        X_tensor = torch.tensor(board_matrix, dtype=torch.float32).unsqueeze(0).to(device_to_use)\n        best_legal_move_obj = None\n        predicted_value = 0.0\n\n        with torch.no_grad():\n            try:\n                policy_logits, value_output = model_to_use(X_tensor)\n                predicted_value = value_output.item()\n                policy_probs = torch.softmax(policy_logits.squeeze(0), dim=0).cpu().numpy()\n\n                legal_moves = list(board.legal_moves)\n                if not legal_moves: return None, predicted_value\n\n                best_prob = -1.0\n                for move in legal_moves:\n                    move_uci = move.uci()\n                    move_idx = -1\n                    # Find index (can be slow, better to use move_to_int if available & consistent)\n                    for idx, uci in int_to_move_map.items():\n                        if uci == move_uci:\n                            move_idx = idx\n                            break\n                    if move_idx != -1:\n                        prob = policy_probs[move_idx]\n                        if prob > best_prob:\n                            best_prob = prob\n                            best_legal_move_obj = move\n\n                if not best_legal_move_obj: # If all legal moves had 0 prob (unlikely)\n                    best_legal_move_obj = random.choice(legal_moves)\n\n            except Exception as e:\n                print(f\"Error during raw NN prediction: {e}\")\n                legal_moves = list(board.legal_moves)\n                if legal_moves: best_legal_move_obj = random.choice(legal_moves)\n\n        return best_legal_move_obj, predicted_value\n\n\nprint(\"RL Prediction function defined.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Prediction Example (with SVG)\n\nif rl_model_loaded:\n    board = Board()\n    print(\"\\n--- Starting RL Prediction Example with SVG Output ---\")\n    MAX_PREDICTION_MOVES = 50 # Play N moves\n    USE_MCTS_IN_EXAMPLE = True # Use MCTS for stronger moves in the example?\n    EXAMPLE_MCTS_SIMS = MCTS_SIMULATIONS # Use same sims as self-play, or adjust\n\n    move_counter = 0\n\n    try:\n        print(\"Initial Board:\")\n        display(SVG(board._repr_svg_()))\n\n        while move_counter < MAX_PREDICTION_MOVES:\n            move_counter += 1\n            current_player = \"White\" if board.turn else \"Black\"\n            print(f\"\\n--- Move {board.fullmove_number}. {'...' if not board.turn else ''}{current_player} to Play ---\")\n\n            if board.is_game_over(claim_draw=True):\n                print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break\n\n            # Use the RL prediction function\n            ai_move, predicted_val = predict_move_rl(\n                board, rl_prediction_model, prediction_device, loaded_int_to_move,\n                use_mcts=USE_MCTS_IN_EXAMPLE, mcts_sims=EXAMPLE_MCTS_SIMS, c_puct=C_PUCT\n            )\n\n            if ai_move:\n                move_uci = ai_move.uci()\n                print(f\"RL AI {'(MCTS)' if USE_MCTS_IN_EXAMPLE else '(Raw NN)'} suggests: {move_uci} (Predicted Value: {predicted_val:.3f})\")\n                board.push(ai_move)\n                print(f\"\\nBoard after {board.fullmove_number-1 if not board.turn else board.fullmove_number}.{'..' if board.turn else ''}{move_uci}:\")\n                display(SVG(board._repr_svg_()))\n            else:\n                print(f\"RL AI could not suggest a valid move for {current_player}.\")\n                if board.is_game_over(claim_draw=True):\n                     print(f\"Game Over! Result: {board.result(claim_draw=True)}\")\n                break\n\n    except Exception as e:\n         print(f\"\\nAn unexpected error occurred during the prediction simulation: {e}\")\n         import traceback\n         traceback.print_exc()\n\n    print(\"\\n--- Prediction Example Finished ---\")\n    print(\"\\nFinal Board State:\")\n    display(SVG(board._repr_svg_()))\n    if board.is_game_over(claim_draw=True):\n         print(f\"Final Result: {board.result(claim_draw=True)}\")\n\nelse:\n    print(\"\\nRL prediction model or mappings not loaded. Skipping prediction example.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}